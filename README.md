# conflu
---

**Full Project Plan: Self-Hosted Confluence Clone (Revision 10 - Meticulously Self-Contained)**

---

**Project Goal:**
Build an exceptionally robust, secure, scalable, and self-hostable Confluence clone that:
1.  Imports Confluence page content with high fidelity, addressing scalability considerations for large HTML files (e.g., via SAX/incremental parsing investigation for extreme cases) and attachments (with clear configuration guidelines), and ensuring accurate page hierarchy reconstruction (with illustrative HTML-to-ProseMirror JSON mapping examples provided within this plan).
2.  Supports Mermaid diagrams and draw.io integration with consistent ProseMirror schema definitions (including schema versioning tracked on content to ensure integrity during updates), embedded storage for diagram data within page content, mobile-friendly touch interactions for editing these diagrams, and performance considerations (e.g., exploring Web Workers for computationally intensive Mermaid rendering, and lazy loading strategies for large draw.io diagrams in view mode).
3.  Offers Markdown export functionality from the application's native ProseMirror JSON content and allows Markdown file import to create new pages. Includes a "raw Markdown" editing mode featuring on-demand conversion to and from ProseMirror JSON, avoiding complex live synchronization.
4.  Features an advanced Tiptap-based rich text editor utilizing a well-defined and extensible ProseMirror schema. Page version comparison capabilities will ideally use ProseMirror-native diffing tools (e.g., `prosemirror-changeset`) for semantic accuracy, with a fallback to HTML-based diffing if necessary for the initial MVP.
5.  Includes secure code snippet rendering with client-side syntax highlighting (e.g., `highlight.js` sanitized by `DOMPurify`), with code block content being part of the overall page versioning. Client-side generated UUIDs will be embedded in code block nodes for precise Large Language Model (LLM) referencing.
6.  Uses local, CPU-friendly LLMs, primarily interacting with GGUF quantized models (with documented guidance on specified quantization levels, e.g., Q4_K_M) via the `llama-cpp-python` library. Implements enhanced safety measures including robust input sanitization for prompt injection defense, output validation (including programmatic validation of LLM-generated Mermaid syntax via an internal API), circuit breakers for LLM service calls to improve resilience, API rate limiting for LLM endpoints, streaming HTTP responses for text generation to enhance user experience, and semantic caching of LLM responses in Redis. Considers runtime LLM model management (hot-swapping) as a **[Post-MVP Consideration]**.
7.  Provides a user-friendly, performant wiki experience featuring object-level permissions using `django-guardian`, soft deletes for data safety, PostgreSQL Full-Text Search with fuzzy matching capabilities (via the `pg_trgm` extension) and optimized indexing, a mobile-responsive design (built with Tailwind CSS, adhering to a basic internal style guide, with responsive behavior examples like sidebar collapse defined) aiming for high Lighthouse performance targets, and secure "zero-trust" attachment downloads. Includes an asynchronous virus scanning pipeline for uploaded attachments (e.g., using ClamAV, managed by Celery).
8.  Employs `django-storages` for flexible attachment handling (supporting local filesystem and S3-compatible object storage, including pre-signed URLs for direct client-to-S3 uploads), Redis with AOF persistence for caching and as a Celery message broker, comprehensive error handling with a defined list of standardized error codes (defined as Python enums) and links to error documentation URLs, and Sentry (or a similar tool like self-hosted GlitchTip) for production-grade error tracking integrated from Phase 1.
9.  Is developed with detailed API contracts specified using OpenAPI 3.0 (auto-generated by `drf-spectacular`, covering all endpoints including a list of key REST paths/methods for initial scaffolding and specific details for LLM APIs, and defining pagination standards like offset-based for MVP), a robust testing strategy (including disaster recovery testing guidance, load testing parameters and targets, specific tooling like PyTest/React Testing Library/Playwright, example E2E scenarios, and automated accessibility/performance scans via `axe-core`/Lighthouse), a clear backup and restore procedure including Redis AOF data, secure JWT rotation with token blacklisting, and Content Security Policy (CSP) with nonces for enhanced frontend security. User management includes self-service registration with email verification and password reset functionality via email, potentially leveraging `django-allauth`. Includes a suggested Django application structure and an illustrative Docker Compose configuration for deployment. Explicitly defers real-time collaboration and advanced Multi-Factor Authentication (MFA such as WebAuthn) to **[Post-MVP Consideration]**. Includes plans for content previews (link unfurling via OEmbed/Iframely) and guidance on achieving near-zero-downtime upgrades (e.g., Blue-Green deployment concepts) as **[Post-MVP Consideration]**.

---

**I. Core Architecture & Data Models**

**A. Overall Architecture:**
*   **Backend:** Python (latest stable Django Long-Term Support (LTS) version, e.g., 4.2.x or a newer LTS if available at project start), Django REST Framework (DRF) for building RESTful APIs. A WSGI (Web Server Gateway Interface) server like Gunicorn or uWSGI will be used for production deployments, typically behind a reverse proxy like Nginx.
    *   **Suggested Django App Structure:** The Django project will be organized into logical applications (Python packages within the Django project) to promote modularity, separation of concerns, and maintainability:
        *   `core`: This app will house project-wide utilities, base abstract models (if any), custom middleware, global settings helpers, and other core functionalities not specific to a single feature domain.
        *   `users`: This app will handle user-related concerns. It may contain a custom User model (if extending Django's default `django.contrib.auth.models.User` is necessary, though often not required if `django-allauth` is used), user profile models, and views/logic related to authentication and account management (especially if not fully handled by a third-party library like `django-allauth`).
        *   `workspaces` (or `spaces`): This app will contain the models (e.g., `Space` model), API views, services, and business logic related to "spaces" or "workspaces" which are collections of pages.
        *   `pages`: This app will be responsible for models (`Page`, `PageVersion`, `Tag`), API views, services, and logic related to individual pages, their versioning, and tagging.
        *   `attachments`: This app will manage models (`Attachment`), API views, file upload handling logic (including pre-signed URL generation for S3), and integration with the virus scanning pipeline for attachments.
        *   `importer`: This app will contain all logic related to the Confluence import feature, including HTML parsing, content transformation, and the Celery tasks for managing the asynchronous import process.
        *   `llm_integrations`: This app will house services, API views, and Celery tasks (if any long-running LLM tasks are defined) for interacting with Large Language Models, including prompt management and communication with `llama-cpp-python` or Ollama.
        *   `api`: This app might serve as the root for Django REST Framework router configuration, and could contain shared API utility views, base classes for viewsets, or custom DRF components like renderers or parsers if needed across multiple API domains.
        This suggested structure is a guideline and can be adapted or further modularized as the project's complexity grows and specific needs arise during development.
*   **Frontend:** React (v18+) with TypeScript. Vite will be used as the frontend build tool and development server, chosen for its fast Hot Module Replacement (HMR) capabilities during development and its use of Rollup for optimized production builds. Tiptap (v2), which is based on ProseMirror, will be the core rich text editor component.
*   **Database:** PostgreSQL (latest stable version, e.g., 14+, 15+, or 16+ at project start). The `pg_trgm` PostgreSQL extension must be enabled in the database to support trigram similarity matching, which is crucial for implementing fuzzy text search capabilities.
*   **Cache & Message Broker:** Redis (latest stable version, e.g., 6+ or 7+). Redis will be configured with AOF (Append Only File) persistence enabled for improved data durability over RDB snapshots alone. Its primary uses will be:
    *   As Django's caching backend (for caching API responses, LLM results, pre-computed search queries, etc.).
    *   As the message broker for Celery to manage asynchronous background tasks.
    *   Potentially for storing Django sessions if database-backed sessions are not preferred for performance reasons.
*   **Background Task Queue:** **Celery** will be used as the distributed task queue system, with **Redis** serving as its message broker. Celery will handle asynchronous, potentially long-running background tasks such as the Confluence import process and the virus scanning of uploaded attachments.
*   **Attachment Storage:** The `django-storages` library will be integrated to provide flexible backend options for storing uploaded files:
    *   **Default for Development/Simple Setups:** Local filesystem storage. Django's `MEDIA_ROOT` setting will define the base directory where files are stored on the server.
    *   **Production Recommendation:** S3-compatible object storage (e.g., AWS S3, MinIO, Google Cloud Storage, Azure Blob Storage) for better scalability, durability, and offloading file serving from the application server.
*   **Virus Scanning:** An antivirus engine, such as **ClamAV**, will be integrated for asynchronous scanning of uploaded attachments. ClamAV would typically be configured to run in a separate Docker container, and the Django application (via a Celery task) would communicate with it using its daemon socket (`clamd`) or by invoking its command-line scanner (`clamscan`/`clamdscan`).
*   **Content Storage Strategy:**
    *   **Primary Editable Content (ProseMirror JSON):** All rich page content (including text, headings, lists, tables, embedded diagrams, code snippets, and other custom elements) will be stored as ProseMirror JSON objects. This data will reside in a PostgreSQL `JSONB` field within the `Page` model (specifically, `Page.raw_content`). This Abstract Syntax Tree (AST) representation serves as the canonical, single source of truth for all editable page content. PostgreSQL's TOAST (The Oversized Attribute Storage Technique) mechanism will automatically handle the efficient storage and compression of large JSONB objects.
    *   **Schema Versioning for Content:** To ensure long-term content integrity and manage evolutions of the editor's capabilities, a `schema_version` integer field will be included in both the `Page` model (for the current live content) and the `PageVersion` model (for historical versions of content). This field will track the version of the `appSchema` (the application's defined ProseMirror schema, see Section III.A) that was used to create or last migrate the `raw_content` JSON. A clear, documented migration path, including potential data transformation scripts (e.g., Django management commands) and editor handling logic (e.g., auto-migrating content on load/save if an older schema version is detected), must be defined and implemented for any future breaking changes to the `appSchema`. This is a critical aspect for maintaining backward compatibility and preventing content corruption as the editor evolves.
    *   **Diagrams (Mermaid, draw.io):** The source text (for Mermaid syntax) or data (for draw.io XML) for diagrams will be embedded directly within custom Tiptap nodes as attributes within the page's ProseMirror JSON structure (`Page.raw_content`). This keeps diagram data versioned along with the rest of the page content.
        *   *Editor Implementation Note:* The Tiptap editor will implement client-side validation or provide user guidance to suggest a reasonable size limit (e.g., 500KB to 1MB) for individual diagram node data. This is to prevent excessive bloat of the `Page.raw_content` JSONB field which could impact editor performance or page load times, even though PostgreSQL itself can handle very large JSON objects.
    *   **Attachments:** Files uploaded by users (images, documents, executables, etc.) will be managed by `django-storages`. Metadata about these attachments (filename, MIME type, size, storage path, virus scan status) will be stored in the `Attachment` database model. The actual file binaries will reside in the configured storage backend (local filesystem or an S3 bucket).

**B. Database Schema Details (Django ORM Models with Constraints & Indexes):**

1.  **`User` (django.contrib.auth.models.User):**
    *   Utilizes Django's standard User model. Fields like `id`, `username` (unique, indexed), `password` (hashed), `email` (unique, indexed), `first_name`, `last_name`, `is_active`, `is_staff`, `is_superuser`, `date_joined`, `last_login` are provided by Django. Standard Django-applied database indexes will be used.

2.  **`Space`:**
    *   `id` (AutoField, Primary Key)
    *   `key` (CharField, max_length=50, unique=True, db_index=True)
    *   `name` (CharField, max_length=255, db_index=True)
    *   `description` (TextField, blank=True, null=True)
    *   `owner` (ForeignKey to `User`, on_delete=models.SET_NULL, null=True, blank=True, related_name='owned_spaces')
    *   `is_deleted` (BooleanField, default=False, db_index=True)
    *   `deleted_at` (DateTimeField, null=True, blank=True)
    *   `created_at` (DateTimeField, auto_now_add=True)
    *   `updated_at` (DateTimeField, auto_now=True)
    *   *Custom Manager: Will filter out `is_deleted=True` records by default.*

3.  **`Page`:**
    *   `id` (AutoField, Primary Key)
    *   `space` (ForeignKey to `Space`, on_delete=models.CASCADE, related_name='pages')
    *   `title` (CharField, max_length=255, db_index=True)
    *   `raw_content` (JSONField, default=dict, help_text="ProseMirror JSON representation of page content.")
    *   `schema_version` (IntegerField, default=1, help_text="Version of appSchema used for raw_content.")
    *   `parent_page` (ForeignKey to 'self', on_delete=models.SET_NULL, null=True, blank=True, related_name='child_pages')
    *   `author` (ForeignKey to `User`, on_delete=models.SET_NULL, null=True, blank=True, related_name='authored_pages')
    *   `version` (IntegerField, default=1, help_text="Current active version number.")
    *   `is_deleted` (BooleanField, default=False, db_index=True)
    *   `deleted_at` (DateTimeField, null=True, blank=True)
    *   `created_at` (DateTimeField, auto_now_add=True)
    *   `updated_at` (DateTimeField, auto_now=True)
    *   *Custom Manager: Will filter out `is_deleted=True` records. A `SearchVectorField` will be added for FTS, with an auto-generated GIN index.*

4.  **`PageVersion`:**
    *   `id` (AutoField, Primary Key)
    *   `page` (ForeignKey to `Page`, on_delete=models.CASCADE, related_name='versions')
    *   `version_number` (IntegerField)
    *   `raw_content` (JSONField, default=dict)
    *   `schema_version` (IntegerField, default=1)
    *   `author` (ForeignKey to `User`, on_delete=models.SET_NULL, null=True, blank=True, related_name='authored_versions')
    *   `commit_message` (CharField, max_length=255, blank=True, null=True)
    *   `created_at` (DateTimeField, auto_now_add=True)

5.  **`Attachment`:**
    *   `id` (AutoField, Primary Key)
    *   `page` (ForeignKey to `Page`, on_delete=models.CASCADE, related_name='attachments')
    *   `uploader` (ForeignKey to `User`, on_delete=models.SET_NULL, null=True, blank=True, related_name='uploaded_attachments')
    *   `file_name` (CharField, max_length=255)
    *   `file` (StorageFileField from `django-storages`, upload_to='attachments/%Y/%m/%d/')
    *   `mime_type` (CharField, max_length=100)
    *   `size_bytes` (BigIntegerField)
    *   `scan_status` (CharField, max_length=20, choices=[('pending', 'Pending Scan'), ('clean', 'Scan Clean'), ('infected', 'Scan Infected'), ('error', 'Scan Error'), ('skipped', 'Scan Skipped')], default='pending', db_index=True)
    *   `scanned_at` (DateTimeField, null=True, blank=True)
    *   `created_at` (DateTimeField, auto_now_add=True)

6.  **`FallbackMacro`:**
    *   `id` (AutoField, Primary Key)
    *   `page_version` (ForeignKey to `PageVersion`, on_delete=models.CASCADE, related_name='fallback_macros')
    *   `macro_name` (CharField, max_length=100)
    *   `raw_macro_content` (TextField)
    *   `import_notes` (TextField, blank=True, null=True)
    *   `placeholder_id_in_content` (UUIDField, null=True, blank=True, unique=True)

7.  **`Tag` / `Label`:**
    *   `id` (AutoField, Primary Key)
    *   `name` (CharField, max_length=50, unique=True, db_index=True)
    *   `pages` (ManyToManyField to `Page`, related_name='tags', blank=True)

*Database Index Review: Post-MVP, after the application has been under some load (even simulated load in a staging environment), a thorough review of frequently executed query patterns will be conducted. Tools like `django-debug-toolbar` (during development) and PostgreSQL's `EXPLAIN ANALYZE` command will be used to identify any slow or inefficient queries. Based on this analysis, additional custom database indexes may be created on specific fields or combinations of fields to optimize performance beyond Django's default index creation and the explicit `db_index=True` fields already specified. This is an ongoing optimization process.*

**C. Authentication & Authorization:**
*   **Authentication:**
    *   The application will use Django's standard, robust authentication system (`django.contrib.auth`) for managing user accounts, password hashing (using strong, modern algorithms like Argon2 or PBKDF2 as configured by Django), and session management if web-based sessions are used alongside API tokens.
    *   API authentication will be handled via JSON Web Tokens (JWT) using the `djangorestframework-simplejwt` library.
    *   **JWT Security Configuration:** `djangorestframework-simplejwt` will be configured with security best practices:
        *   `ROTATE_REFRESH_TOKENS = True`: When a refresh token is used to obtain a new access token, a new refresh token is also issued, and the old refresh token is invalidated (or added to a blacklist).
        *   `BLACKLIST_AFTER_ROTATION = True`: The old refresh token (after rotation) is added to a blacklist to prevent its reuse. This requires a blacklist mechanism, which can be implemented using a separate Django app like `django-rest-framework-simplejwt-blacklist` or a custom solution if sliding tokens with blacklist functionality are used.
        *   Access tokens will have short expiry times (e.g., 15 minutes, configurable via `ACCESS_TOKEN_LIFETIME`).
        *   Refresh tokens will have longer expiry times (e.g., 7 days, configurable via `REFRESH_TOKEN_LIFETIME`).
    *   **User Registration & Account Management:**
        *   Self-service user registration will be supported, including a mechanism for email verification (e.g., sending a confirmation link to the user's provided email address).
        *   Password reset functionality ("forgot password") will be implemented, typically involving sending an email with a unique, time-limited reset token/link to the user.
        *   Django's built-in authentication views (`django.contrib.auth.views`) and email utilities (`django.core.mail`) will be leveraged for these features.
        *   Consideration will be given to integrating a comprehensive third-party library like **`django-allauth`**. `django-allauth` provides robust handling for local account registration, email verification, password reset, social authentication (though social auth itself is a **[Post-MVP Consideration]**), and other account management features, potentially reducing the amount of custom code needed for these common tasks. If `django-allauth` is used, it will replace or augment some of the custom view logic for authentication and account management.
*   **Authorization (Permissions):**
    *   Role-Based Access Control (RBAC) will be implemented using the **`django-guardian`** library. This library provides fine-grained, object-level permissions, allowing precise control over who can perform what actions on specific instances of models (like particular `Space` objects or individual `Page` objects).
    *   **Defined Roles:** Initial roles will be defined within the application (e.g., programmatically or via Django fixtures), such as:
        *   `SpaceAdmin`: Grants full control over a specific `Space` object. Users with this role for a space can manage its settings, members (if member management is implemented), and permissions within that space (e.g., assign other users as editors or viewers of that space).
        *   `SpaceEditor`: Grants permission to create new pages and edit existing pages within a specific `Space`.
        *   `SpaceViewer`: Grants permission to only view pages and other content within a specific `Space`.
    *   Permissions based on these roles will be assigned to `User` objects or Django `Group` objects for specific `Space` instances. `Page` objects will typically inherit their permissions from their parent `Space`, but `django-guardian` allows for overriding these permissions at the individual `Page` level if more granular, page-specific control is required (though space-level inheritance will be the primary model for simplicity in the MVP).
    *   Django's `is_staff` user flag will grant access to the Django administrative interface (e.g., `/admin/`) only; it will not automatically bypass application-level permissions enforced by `django-guardian`. Superusers (`is_superuser=True`) will have all permissions implicitly across the application.
*   **[Post-MVP Consideration] Multi-Factor Authentication (MFA):** While not part of the MVP, support for Multi-Factor Authentication is recognized as a high-priority post-MVP security enhancement to protect user accounts further. Future iterations would aim to include support for:
    *   TOTP (Time-based One-Time Password) applications (e.g., Google Authenticator, Authy, FreeOTP).
    *   WebAuthn (FIDO2) for phishing-resistant authentication using hardware security keys (like YubiKeys) and platform authenticators (like Windows Hello, Touch ID/Face ID).

---

**II. Frontend Architecture & Design**

**A. Core Technologies:**
*   **React (v18+):** The latest stable version of React will be used as the JavaScript library for building the user interface as a Single Page Application (SPA).
*   **TypeScript:** TypeScript will be used for all frontend code. This provides static typing, which helps catch errors early in development, improves code quality and maintainability, and enhances the developer experience with better autocompletion and refactoring capabilities.
*   **Build Tool: Vite:** Vite will be used as the frontend build tool and development server. It is chosen for its:
    *   Extremely fast Hot Module Replacement (HMR) during development, leading to a much smoother and quicker development feedback loop.
    *   Optimized production builds using Rollup under the hood, which includes features like tree-shaking and code-splitting.
*   **CSS Framework & Styling:**
    *   **Tailwind CSS** will be the primary CSS framework used for styling the application. Its utility-first approach allows for rapid UI development and easy customization while maintaining consistency.
    *   For the MVP, components will be custom-styled using Tailwind utilities directly. No pre-built Tailwind UI component theme (like DaisyUI or Flowbite) will be used initially; this is to maintain a unique design identity and avoid being locked into a specific theme's opinions too early. However, this could be revisited post-MVP if rapid development of many standard components is needed and a theme aligns with the desired aesthetics.
    *   A **basic style guide** covering core design tokens (typography scale, color palette with accessibility considerations for contrast, spacing units) and the general appearance of common UI elements (buttons, inputs, modals) will be developed iteratively starting in Phase 1 and documented for frontend developers to ensure visual consistency.

**B. State Management:**
*   **Zustand:** This lightweight, flexible, and performant library will be selected as the primary **global state management** solution for the React application. It will be used to manage application-wide state that needs to be accessible from many components or persists across different views. Examples include:
    *   Authentication state: Current authenticated user's information (ID, username, roles/permissions), authentication tokens, and login status.
    *   Global UI preferences: User-selectable theme (e.g., light mode/dark mode), sidebar visibility state (open/closed), or other persistent UI settings.
    *   Potentially some lightly cached, non-critical fetched data that is used across multiple components and doesn't warrant a more complex server state management library for MVP (e.g., list of user's spaces).
*   **React Context API:** For more **localized state** that needs to be shared within specific component subtrees (e.g., state within a complex multi-step form, context for a specific feature module like the editor's current mode or a particular diagram editor's settings), React's built-in Context API will be used. This avoids introducing the overhead of a global store for every piece of shared state and keeps state management closer to the components that directly need it, promoting better component encapsulation.
*   **Server State Management (Consideration):** While Zustand can handle some fetched data, for more complex server state management (caching, optimistic updates, background refetching, pagination for API data), a dedicated library like **React Query (TanStack Query) or SWR** might be considered for integration post-MVP or if data fetching patterns become very complex during MVP development. For MVP, direct API calls with local component state or Zustand for simple caching will be the initial approach.

**C. Routing:**
*   **React Router (`react-router-dom` v6+):** This library will be used to handle all client-side navigation, URL structure management, and rendering of different views or pages within the SPA based on the current URL.
*   **Key Routing Features to Implement:**
    *   **Route Declaration:** Clear declaration of routes mapping URL paths (e.g., `/spaces/{spaceKey}/pages/{pageId}`) to their corresponding React view components.
    *   **Nested Routes:** Utilizing nested routes for hierarchical UI structures (e.g., settings panels within a user settings page, or page versions view as a child of a page view).
    *   **Programmatic Navigation:** Using React Router's hooks (e.g., `useNavigate`) for navigating programmatically (e.g., redirecting a user to their dashboard after successful login, or navigating to a newly created page after form submission).
    *   **URL Parameter Handling:** Extracting and using dynamic segments from the URL (e.g., `spaceKey`, `pageId`) as parameters within components.
    *   **Protected Routes:** Implementing wrapper components or logic to protect routes that require authentication. Unauthenticated users attempting to access these routes will be redirected to a login page, potentially with the original intended URL stored for redirection after login.
    *   **"Not Found" (404) Route:** A catch-all route for handling invalid URLs, displaying a user-friendly "Page Not Found" message.
    *   **Scroll Restoration:** Ensuring that browser scroll position is handled correctly during navigation (e.g., scrolling to the top on new page loads, or restoring scroll position when using browser back/forward buttons).
*   **Example Routes (Conceptual Structure - this will be fully defined):**
    *   `/auth/login` (renders `LoginPage.tsx`)
    *   `/auth/register` (renders `RegisterPage.tsx`)
    *   `/auth/forgot-password` (renders `ForgotPasswordPage.tsx`)
    *   `/auth/reset-password/:token` (renders `ResetPasswordPage.tsx`)
    *   `/dashboard` (renders `UserDashboardView.tsx` - user's main landing page after login)
    *   `/spaces/{spaceKey}` (renders `SpaceView.tsx` - displays overview of a specific space, lists its pages)
    *   `/spaces/{spaceKey}/settings` (renders `SpaceSettingsView.tsx` - for space administrators)
    *   `/spaces/{spaceKey}/pages/{pageId}` (renders `PageView.tsx` - renders a single page for viewing)
    *   `/spaces/{spaceKey}/pages/{pageId}/edit` (renders `EditorView.tsx` - opens the Tiptap editor to edit an existing page)
    *   `/spaces/{spaceKey}/pages/{pageId}/versions` (renders `PageVersionsView.tsx` - lists historical versions of a page)
    *   `/spaces/{spaceKey}/pages/new?parent={parentPageId}` (renders `EditorView.tsx` - opens Tiptap editor to create a new page, optionally under a given parent page)
    *   `/search?q={queryString}&space={spaceKey}&tags={tagList}` (renders `SearchResultsView.tsx` - displays search results based on query parameters)
    *   `/settings/profile` (renders `UserSettingsView.tsx` - panel for user profile information)
    *   `/settings/account` (renders `UserSettingsView.tsx` - panel for account settings like password change, email change)
    *   `/admin/users` (renders `AdminUsersView.tsx` - if a custom frontend administration UI for user management beyond the Django admin panel is built).

**D. Component Hierarchy (Illustrative Sketch - High-Level Overview):**
*   `App.tsx`: The root component of the React application. It initializes global providers (Zustand store, Theme provider, React Router `BrowserRouter`, Sentry Error Boundary for catching React rendering errors).
    *   `AuthRoutes.tsx`: A route grouping component that defines routes accessible only to unauthenticated users (e.g., login, registration pages).
        *   `LoginPage.tsx`, `RegisterPage.tsx`, `ForgotPasswordPage.tsx`
    *   `ProtectedRoutes.tsx`: A route grouping component that defines routes requiring user authentication. It will typically check the authentication state from the Zustand store and redirect to the login page if the user is not authenticated.
        *   `AppLayout.tsx`: The main layout structure for the authenticated application experience. This component usually persists across different views, providing consistent header, sidebar, etc.
            *   `AppHeader.tsx`: The top navigation bar containing application branding/logo, a global search input field, user menu (linking to profile, settings, and providing a logout option), and potentially a notification bell icon or area.
            *   `AppSidebar.tsx`: A contextual sidebar that adapts its content based on the current view or selected space. For example, when viewing a space, it might display the hierarchical page tree for that space, links to space settings, and quick actions like "New Page." It will be collapsible on desktop and off-canvas on mobile.
            *   `MainContentArea.tsx`: The central area of the `AppLayout` where the content for the current route is rendered. This will typically use React Router's `<Outlet />` component to render the matched child route component.
                *   `UserDashboardView.tsx`: User's main landing page, perhaps showing recent activity or favorite pages.
                *   `SpaceDashboardView.tsx`: Displays an overview of a specific space, lists its root pages, shows recent activity, etc.
                *   `PageView.tsx`: Responsible for fetching (or receiving via a route loader) and displaying the rendered content of a single page. It will utilize a sub-component like `RenderedPageContent.tsx`.
                    *   `RenderedPageContent.tsx`: Takes the ProseMirror JSON object for a page and renders it into HTML for display, correctly handling all standard and custom Tiptap nodes. This might involve invoking React Node Views for interactive custom nodes if Tiptap's default rendering capabilities are insufficient for the desired UI/UX within those nodes.
                *   `EditorView.tsx`: Hosts the Tiptap editor interface for creating new pages or editing existing ones. It will manage the editor's state (loading content, handling changes), save actions (communicating with the backend API), and any editor-specific UI elements not part of the Tiptap core toolbar (e.g., a custom save button with status indicators, page title input).
                    *   `TiptapEditor.tsx`: A wrapper component that initializes and configures the Tiptap editor instance. This includes setting up all necessary extensions (starter kit, custom nodes/marks based on `appSchema`), the editor schema (`appSchema` itself), and the editor's menu bar or bubble menu UI.
                        *   *(If using React Node Views for Tiptap for enhanced interactivity within custom nodes)*:
                            *   `MermaidNodeView.tsx` (React Component): Custom Tiptap Node View responsible for rendering the editable state (e.g., syntax input area) and live preview of Mermaid diagrams directly within the ProseMirror document structure.
                            *   `DrawioNodeView.tsx` (React Component): Custom Tiptap Node View for interacting with draw.io diagrams (e.g., displaying a diagram placeholder/preview and providing a button to launch the draw.io editor modal).
                            *   `CodeBlockNodeView.tsx` (React Component): Custom Tiptap Node View for code blocks, potentially offering additional UI within the node itself, such as a "Copy to Clipboard" button or a display of the selected language.
                *   `SearchResultsView.tsx`: Displays a list of search results with titles, context snippets, and links to the respective pages. Will include UI for pagination and potentially sorting/filtering of results.
                *   `UserSettingsView.tsx`: Contains various panels or tabs for users to manage their profile information (name, avatar), account settings (password change, email change), API key management (if this feature is implemented later), notification preferences, etc.
                *   `SpaceSettingsView.tsx`: For Space Admins, provides UI to manage space details, members (if implemented), and permissions within that space.
*   **Shared UI Components Library (`src/components/ui/`):**
    *   A dedicated directory (`src/components/ui/`) will house a collection of reusable, accessible, and consistently styled UI primitive components. These components will form the building blocks of the application's user interface.
    *   Examples include: `Button.tsx`, `Modal.tsx` (with proper focus trapping and accessibility attributes), `Input.tsx` (with associated `<label>` elements), `Textarea.tsx`, `Select.tsx`, `DropdownMenu.tsx`, `Tooltip.tsx`, `Spinner.tsx` (loading indicator), `Alert.tsx` (for notifications and warnings), `Table.tsx` (with sorting and pagination capabilities), `Pagination.tsx`, `Breadcrumbs.tsx`, `Tabs.tsx`, `Accordion.tsx`.
    *   These components will be built with accessibility best practices in mind from the start (e.g., correct ARIA attributes, keyboard navigation support, focus management).
    *   Styling will be done using Tailwind CSS utility classes. Consideration will be given to using a headless UI library like **Radix UI** or **Headless UI by Tailwind Labs** as a foundation. These libraries provide unstyled, accessible, and behaviorally complete component primitives, which can then be easily styled with Tailwind CSS, accelerating development while ensuring high quality.

**E. Props/Interfaces (Illustrative TypeScript Examples for Clarity):**
*   TypeScript interfaces will be meticulously defined for all significant data structures exchanged with the API (API request payloads and response DTOs - Data Transfer Objects) and for the props (properties) of all major React components. This practice ensures type safety, improves code readability, facilitates autocompletion in IDEs, and serves as a form of documentation for component contracts.
    ```typescript
    // Example: Located in a common types definition file, e.g., 'src/types/apiModels.ts'
    // This file would contain interfaces mirroring the backend API responses.

    export interface ApiUserInfo {
      id: number;
      username: string;
      email: string;
      first_name?: string;
      last_name?: string;
    }

    export interface ApiSpaceDetail {
      key: string;
      name:string;
      description?: string;
      owner_username?: string; // Username of the space owner
      created_at: string; // ISO 8601 date-time string
      updated_at: string; // ISO 8601 date-time string
      // Add other relevant space details, e.g., permissions for current user
    }

    export interface ApiPageSummary { // For lists of pages
      id: number;
      title: string;
      space_key: string; // Key of the space this page belongs to
      updated_at: string; // ISO 8601 date-time string
      author_username?: string; // Username of the last modifier or original author
      version: number; // Current version number
      // Potentially a short excerpt or abstract
    }

    export interface ApiPageDetail extends ApiPageSummary { // For viewing a single page
      raw_content: Record<string, any>; // Represents the ProseMirror JSON object structure
      schema_version: number; // Version of the ProseMirror schema used for raw_content
      tags: Array<{ id: number; name: string }>; // List of tags associated with the page
      parent_page_id?: number; // ID of the parent page, if any
      created_at: string; // ISO 8601 date-time string
      // Permissions for the current user on this page (e.g., can_edit, can_delete)
    }

    export interface ApiPageVersionInfo {
        version_number: number;
        author_username?: string;
        created_at: string;
        commit_message?: string;
    }


    // Example props for a component displaying a list of pages within a space
    // e.g., src/features/spaces/components/PageListDisplay.tsx
    export interface PageListDisplayProps {
      pages: ApiPageSummary[]; // Array of page summary objects fetched from the API
      spaceKey: string; // The key of the current space, for context or further actions
      isLoading: boolean; // To show a loading indicator while pages are being fetched
      onPageSelect: (pageId: number) => void; // Callback function when a page is selected/clicked
      // Add other props like pagination controls, sorting options, etc.
    }

    // Example props for the main Tiptap editor wrapper component
    // e.g., src/features/editor/components/TiptapCoreEditor.tsx
    export interface TiptapCoreEditorProps {
      pageId?: number; // ID of the page being edited (if editing an existing page)
      spaceKey: string; // Key of the space the page belongs to
      initialTitle?: string;
      initialContentJson?: Record<string, any>; // Initial ProseMirror JSON for editing an existing page
      onSave: (payload: { title: string; contentJson: Record<string, any>; commitMessage?: string; parentPageId?: number }) => Promise<ApiPageDetail | null>; // Callback on save, returns saved page data or null on error
      isSaving: boolean; // To show a loading state on the save button or other UI elements
      placeholderText?: string; // Placeholder text to display in an empty editor
      autoFocus?: boolean; // Whether the editor should autofocus on mount
    }
    ```
*   These interfaces (and many others for API payloads, component props, and internal data structures) will be defined comprehensively as development progresses and as the API specification is finalized. They will reside in appropriate `types` directories or alongside the modules/components they relate to.

**F. Mobile Responsive Design:**
*   The application's user interface will be designed and implemented to be **mobile-responsive** from the outset. This means ensuring a good user experience (UX) on a wide range of devices, including desktops, laptops, tablets, and smartphones.
*   **Content Viewing Priority:** Core functionality such as navigating spaces, viewing page content (including rendered diagrams and code blocks), and using the search feature will be fully functional and optimized for readability and usability on smaller screens. Text will reflow correctly, images will scale appropriately, and interactive elements will have sufficient tap targets.
*   **Editing Experience on Mobile:** While the rich text editing experience (Tiptap editor) is inherently more complex and benefits from larger screen real estate for optimal usability with all its features, efforts will be made to ensure that basic editing tasks can be performed reasonably well on mobile devices. This might involve:
    *   Responsive editor toolbars that adapt their layout or collapse items into menus on smaller screens.
    *   Touch-friendly controls and ensuring that interactive elements within the editor (like handles for resizing tables or images, if implemented) are usable with touch input.
    *   Potentially a more focused or simplified editing mode for very small screens, with access to less frequently used or more complex formatting options perhaps hidden behind sub-menus or accessible via slash commands if implemented.
*   **Responsive Design Techniques & Tailwind CSS:**
    *   Responsive design will be achieved using standard CSS techniques like media queries to apply different styles based on screen width, orientation, and other device characteristics.
    *   Flexible layout systems like CSS Flexbox and CSS Grid will be used to create fluid page structures that adapt to available space.
    *   Responsive typography (e.g., using `vw` units for font sizes, or adjusting font sizes and line heights via media queries) will ensure text remains readable on all screen sizes.
    *   Images will be made responsive (e.g., `max-width: 100%; height: auto;`) to prevent them from breaking layouts.
    *   **Tailwind CSS's responsive modifiers** (e.g., `sm:`, `md:`, `lg:`, `xl:`) will be extensively used throughout the styling process. This allows developers to easily apply different utility classes (for layout, typography, spacing, visibility, etc.) at specific predefined breakpoints, making the implementation of responsive designs very efficient.
*   **Example Responsive Behavior (Sidebar):** The `AppSidebar` (which might contain the page tree for a space or other navigation links) will be designed to be collapsible on desktop views to allow users to maximize the content area. On mobile viewports (e.g., screen widths less than 768px, a common breakpoint for tablets in portrait mode), the sidebar will default to a hidden (off-canvas) state and will be accessible via a "hamburger" toggle button typically located in the `AppHeader`. When opened on mobile, it might overlay part of the content or push content aside, depending on the chosen UX pattern.

**G. Accessibility (WCAG Compliance):**
*   The development process will actively strive to achieve compliance with the **Web Content Accessibility Guidelines (WCAG) 2.1 Level AA**. This is a critical requirement to ensure the application is usable by people with a wide range of disabilities, including visual, auditory, motor, and cognitive impairments.
*   **Key Accessibility Considerations and Practices to be Implemented:**
    *   **Semantic HTML:** Using appropriate HTML5 elements (`<nav>`, `<main>`, `<article>`, `<aside>`, `<button>`, `<table>`, `<thead>`, `<tbody>`, `<th>` with `scope` attributes, etc.) to accurately convey the structure and meaning of content. This is fundamental for assistive technologies like screen readers.
    *   **Keyboard Navigability:** Ensuring that all interactive elements (links, buttons, form controls, custom components like dropdowns, modals, tabs) are fully focusable and operable using only the keyboard, in a logical and predictable tab order. Focus indicators (outlines) must be clearly visible and have sufficient contrast.
    *   **Color Contrast:** Maintaining sufficient color contrast ratios between text and its background (minimum 4.5:1 for normal-sized text and 3:1 for large text, as per WCAG AA) and for important graphical elements and UI components. Color contrast checking tools will be used during design and development.
    *   **ARIA (Accessible Rich Internet Applications) Attributes:** Correctly using ARIA attributes to provide additional semantic information and improve the accessibility of custom widgets (e.g., `role="dialog"` for modals, `aria-expanded` for accordions/collapsible sections, `aria-labelledby` and `aria-describedby` for complex form controls), dynamic content updates, and complex UI interactions where native HTML semantics are insufficient. Use ARIA sparingly and correctly, prioritizing native HTML semantics where possible.
    *   **Text Alternatives for Non-Text Content:** Providing appropriate, concise, and descriptive `alt` text for all informative images. For purely decorative images that do not convey information, an empty `alt=""` attribute will be used so that screen readers can ignore them.
    *   **Forms:** Ensuring all form inputs have clearly associated, visible labels (using `<label for="...">` or `aria-labelledby`). Error messages for form validation must be clearly associated with their respective inputs (e.g., using `aria-describedby` or placing them visually adjacent) and be programmatically determinable by assistive technologies.
    *   **Headings:** Using HTML headings (H1-H6) to structure page content hierarchically and logically. There should typically be only one H1 per page, and heading levels should not be skipped (e.g., H2 followed by H4 without an H3 is generally bad practice).
    *   **Consistent Navigation:** Maintaining consistent navigation mechanisms (e.g., main menu, breadcrumbs) across all pages of the application.
    *   **Resizable Text:** Ensuring that text can be resized by the user (e.g., using browser zoom functions) up to 200% without loss of content or functionality, and without requiring horizontal scrolling for single columns of text.
    *   **"Skip to Main Content" Links:** Implementing a "skip link" that is visible on focus at the beginning of the page to allow keyboard users to bypass repetitive navigation blocks and jump directly to the main content area.
*   **Accessibility Testing Strategy:** Accessibility will be an ongoing consideration throughout the design and development lifecycle, not just an afterthought. Testing will involve a combination of:
    *   **Automated Tools:** Integrating automated accessibility scanning tools (e.g., **`axe-core`** via libraries like `axe-playwright` for E2E tests or `jest-axe` for component tests) into the development pipeline (e.g., as part of Continuous Integration builds). These tools can catch many common WCAG violations.
    *   **Manual Testing:** Performing regular manual testing by developers and QAs, including:
        *   Keyboard-only navigation checks for all interactive elements and workflows.
        *   Screen reader testing using common screen readers (e.g., NVDA on Windows, VoiceOver on macOS/iOS, TalkBack on Android) for core user workflows to ensure content is understandable and operable.
        *   Browser zoom testing to check for text resizing issues.
        *   Manual color contrast checks using browser developer tools or dedicated contrast checkers.
    *   **User Feedback:** If possible, involving users with disabilities in usability testing sessions.

**H. Custom Tiptap Node View Interaction (Mobile - Touch Event Handling):**
*   For custom Tiptap Node Views that represent interactive elements directly within the editor canvas (e.g., a placeholder for a Mermaid diagram that, when tapped, should open an editor modal, or a Draw.io diagram placeholder that needs to be selectable for editing), event handling will be specifically designed to provide a good and responsive user experience on touch-enabled mobile devices.
*   This includes adding **`touchend` event listeners** to the root DOM element of such Node Views. The `touchend` event is often more responsive and reliable than `click` for touch interactions, especially if there are potential delays or interference from other touch-related browser behaviors.
*   These `touchend` event listeners will be configured with **`passive: true`** in their `addEventListener` options if they do not call `event.preventDefault()`. This informs the browser that the listener will not block scrolling, which can improve scroll performance on touch devices. If `preventDefault()` is needed (e.g., to prevent a default browser action on tap), then `passive: true` should not be used for that specific listener.
*   The UI for these interactions that are triggered by touch (e.g., modals for editing diagram syntax, context menus) will also be designed to be mobile-responsive and touch-friendly, with adequate tap target sizes.

**I. Performance (Vite Chunking Strategy for Frontend):**
*   The Vite build tool (which utilizes Rollup for its production builds) will be configured to optimize the frontend application's performance through effective **code splitting** and **lazy loading**. This aims to reduce the initial JavaScript bundle size, leading to faster perceived load times, especially on first visit or on slower network connections.
*   **Route-based Code Splitting:** Major route components (e.g., `EditorView.tsx`, `SpaceDashboardView.tsx`, `UserSettingsView.tsx`, and other components tied to specific top-level routes) will be lazy-loaded. This will be implemented using React's `React.lazy()` function in conjunction with the `<React.Suspense />` component (which allows specifying a fallback UI, like a spinner, while the lazy-loaded component's code is being downloaded and parsed). This ensures that the JavaScript code for a particular route is only fetched when the user navigates to it.
*   **Component-based Code Splitting (for Large/Non-Critical Components):** For particularly large or complex components that are not immediately critical for the initial rendering of a view (e.g., a very feature-rich charting library used only in an administrative dashboard, a complex data visualization component, or the Draw.io editor iframe loader logic), dynamic `import()` statements can be used for finer-grained lazy loading. This allows these components to be loaded on demand, perhaps when a user interacts with a specific UI element that requires them.
*   **Vendor Chunking:** Vite/Rollup will be configured to create separate chunks for large third-party libraries (vendor code, e.g., React, React Router, Tiptap, Mermaid.js). This allows these relatively stable vendor chunks to be cached effectively by the browser. If only the application code changes, users will not need to re-download these large vendor libraries.
*   **Tree Shaking:** Vite's use of Rollup ensures effective tree shaking, which automatically eliminates unused code (dead code) from the final production bundles, further reducing their size.
*   **Asset Optimization:** Vite also handles optimization of static assets like images and CSS.
*   The overall goal of these strategies is to keep the initial JavaScript payload downloaded by the browser as small as possible for fast Time to Interactive (TTI) and improved Core Web Vitals (LCP, INP/FID, CLS). Performance will be monitored using tools like Lighthouse.

---

**III. Editor Implementation (Tiptap & ProseMirror)**

**A. ProseMirror Schema Definition (`appSchema`):**
*   The `appSchema` is the formal definition of the document structure allowed within the Tiptap editor. It dictates all valid nodes (structural elements like paragraphs, headings, custom diagram nodes) and marks (inline styling like bold, italic, links). This schema is critical for the editor's behavior, content validation, and serialization/deserialization of the `raw_content` JSON. It must be comprehensive and consistently applied on both client and server (for validation).
    ```javascript
    // Typically located in a shared frontend module, e.g., 'src/editor/prosemirror/schema.ts'
    import { Schema, NodeSpec, MarkSpec, DOMOutputSpec, ParseRule } from "prosemirror-model";

    // Helper for common block node group
    const blockGroup = "block";
    const inlineGroup = "inline";

    // --- Node Definitions ---

    const docNode: NodeSpec = {
        content: `${blockGroup}+` // The document must contain one or more block-level nodes
    };

    const paragraphNode: NodeSpec = {
        content: `${inlineGroup}*`, // Can contain zero or more inline nodes
        group: blockGroup,
        toDOM: (): DOMOutputSpec => ["p", 0], // 0 means "render this node's content here"
        parseDOM: [{ tag: "p" }] // Parse <p> tags from HTML into this node
    };

    const headingNode: NodeSpec = {
        attrs: { level: { default: 1 } }, // Attribute for heading level (1-6)
        content: `${inlineGroup}*`, // Headings can contain inline content
        group: blockGroup,
        defining: true, // A heading "defines" its content for selection purposes
        toDOM: node => [`h${node.attrs.level}`, 0] as DOMOutputSpec, // Render as H1, H2, etc.
        parseDOM: [ // Rules for parsing H1-H6 tags from HTML
            { tag: "h1", attrs: { level: 1 } }, { tag: "h2", attrs: { level: 2 } },
            { tag: "h3", attrs: { level: 3 } }, { tag: "h4", attrs: { level: 4 } },
            { tag: "h5", attrs: { level: 5 } }, { tag: "h6", attrs: { level: 6 } },
        ],
    };

    const textNode: NodeSpec = { // Basic text node, fundamental unit of inline content
        group: inlineGroup
    };

    const hardBreakNode: NodeSpec = { // For explicit line breaks (<br>)
        inline: true, group: inlineGroup, selectable: false, // Not selectable on its own
        toDOM: (): DOMOutputSpec => ["br"],
        parseDOM: [{ tag: "br" }]
    };

    const horizontalRuleNode: NodeSpec = {
        group: blockGroup,
        toDOM: (): DOMOutputSpec => ["hr"],
        parseDOM: [{ tag: "hr" }]
    };

    const imageNode: NodeSpec = {
        inline: false, // Typically block, but can be inline depending on schema design needs
        group: blockGroup, // Or inlineGroup if inline images are desired
        attrs: {
            src: { default: "" },
            alt: { default: null },
            title: { default: null },
            // width: { default: null }, // Consider if fixed widths are stored or if CSS handles sizing
            // height: { default: null },
        },
        draggable: true,
        toDOM: node => ["img", { ...node.attrs, class: "ProseMirror-image" }] as DOMOutputSpec,
        parseDOM: [{
            tag: "img[src]",
            getAttrs: domNode => {
                const dom = domNode as HTMLImageElement;
                return {
                    src: dom.getAttribute("src"),
                    alt: dom.getAttribute("alt"),
                    title: dom.getAttribute("title"),
                    // width: dom.getAttribute("width"),
                    // height: dom.getAttribute("height"),
                };
            },
        }] as ParseRule[],
    };


    const codeBlockNode: NodeSpec = {
        content: "text*", // Allows only plain text content (no nested nodes or marks)
        marks: "", // No marks (like bold, italic) allowed within code_block's text content
        group: blockGroup,
        code: true, // Signifies to ProseMirror that this is a code block (affects behavior e.g. tab key)
        defining: true, // Content inside is semantically "defined" by this node (affects selection)
        attrs: {
            language: { default: null }, // Stores the selected programming language (e.g., "javascript", "python")
            uuid: { default: null },     // Client-side generated unique ID for LLM referencing and other unique identification
        },
        toDOM: node => {
            // Class for highlight.js or other syntax highlighters
            const langClass = node.attrs.language ? `language-${node.attrs.language}` : 'language-plaintext';
            // data-uuid is for easy querying/identification if needed by editor extensions
            return ["pre", { "data-language": node.attrs.language, "data-uuid": node.attrs.uuid, class: langClass }, ["code", 0]] as DOMOutputSpec;
        },
        parseDOM: [{
            tag: "pre", // Parses <pre> tags (often containing a <code> tag)
            preserveWhitespace: "full", // Crucial for maintaining indentation and spacing in code blocks
            getAttrs: domNode => {
                const dom = domNode as HTMLElement;
                // Attempt to get language from a data-attribute or a common class pattern (e.g., class="language-python")
                const codeElement = dom.querySelector('code');
                const langFromClass = codeElement?.className.match(/language-(\S+)/)?.[1];
                return {
                    language: dom.getAttribute("data-language") || langFromClass || null, // Default to null if not found
                    uuid: dom.getAttribute("data-uuid") || null, // Get UUID if present from data-attribute
                };
            },
        }] as ParseRule[],
    };

    const mermaidDiagramNode: NodeSpec = {
        group: blockGroup,
        atom: true, // Treat as a single, indivisible unit in the editor; selection handles the whole node
        attrs: { syntax: { default: "" } }, // Stores the raw Mermaid diagram syntax as a string
        inline: false, // This is a block-level node
        draggable: true, // Allows the node to be dragged and dropped within the editor
        toDOM: node => ["div", {
            "data-type": "mermaid-diagram", // Custom data attribute for identification
            class: "mermaid-container resizable-diagram-node ProseMirror-widget", // Classes for styling and potential JS interaction
        }, node.attrs.syntax] as DOMOutputSpec, // Embed syntax directly as text content of the div for simpler parsing
        parseDOM: [{
            tag: "div[data-type='mermaid-diagram']", // Parse custom div structure based on data-type
            getAttrs: dom => ({ syntax: (dom as HTMLElement).textContent || "" }), // Extract syntax from the text content
        }] as ParseRule[],
    };

    const drawioDiagramNode: NodeSpec = {
        group: blockGroup,
        atom: true,
        attrs: { xml: { default: "" } }, // Stores the Draw.io diagram XML data as a string
        inline: false,
        draggable: true,
        toDOM: node => ["div", {
            "data-type": "drawio-diagram",
            class: "drawio-container resizable-diagram-node ProseMirror-widget",
        }, node.attrs.xml] as DOMOutputSpec, // Embed XML as text content for simpler parsing
        parseDOM: [{
            tag: "div[data-type='drawio-diagram']",
            getAttrs: dom => ({ xml: (dom as HTMLElement).textContent || "" }), // Node view logic will use this XML to initialize the Draw.io editor
        }] as ParseRule[],
    };

    const fallbackMacroPlaceholderNode: NodeSpec = {
        group: blockGroup,
        atom: true,
        attrs: {
            macroName: { default: "" }, // Stores the name of the original Confluence macro
            fallbackMacroId: { default: null } // Stores the ID of the `FallbackMacro` record in the database
        },
        draggable: true,
        toDOM: node => ["div", {
            "data-type": "fallback-macro-placeholder",
            class: "fallback-macro-display ProseMirror-widget", // Class for styling
            "data-macro-name": node.attrs.macroName,
            "data-fallback-id": String(node.attrs.fallbackMacroId) // Store ID as string for attribute consistency
        }, `[Unsupported Confluence Macro: ${node.attrs.macroName || 'Unknown'}. Click for details. ID: ${node.attrs.fallbackMacroId || 'N/A'}]`] as DOMOutputSpec,
        parseDOM: [{
            tag: "div[data-type='fallback-macro-placeholder']",
            getAttrs: dom => ({
                macroName: (dom as HTMLElement).getAttribute("data-macro-name") || "",
                fallbackMacroId: parseInt((dom as HTMLElement).getAttribute("data-fallback-id") || "0") || null,
            })
        }] as ParseRule[],
    };

    // TODO during Phase 2: Define other standard nodes:
    // bullet_list, ordered_list, list_item (these have specific content constraints and parsing rules)
    // table, table_row, table_cell, table_header (these form a complex nested structure with attributes for colspan/rowspan)

    // --- Mark Definitions ---
    // Marks are used for inline styling (e.g., bold, italic, links).

    const linkMark: MarkSpec = {
        attrs: {
            href: { default: "" },
            title: { default: null },
            target: { default: "_blank" } // Default to opening links in a new tab for external links
        },
        inclusive: false, // A link mark should not automatically span across nodes if the selection is collapsed at its boundary
        toDOM: node => ["a", { href: node.attrs.href, title: node.attrs.title, rel: "noopener noreferrer nofollow", target: node.attrs.target }, 0] as DOMOutputSpec, // 0 renders content
        parseDOM: [{
            tag: "a[href]", // Parse <a> tags that have an href attribute
            getAttrs: domNode => {
                const dom = domNode as HTMLLinkElement;
                return { href: dom.getAttribute("href"), title: dom.getAttribute("title"), target: dom.getAttribute("target") };
            },
        }] as ParseRule[],
    };

    const boldMark: MarkSpec = {
        toDOM: (): DOMOutputSpec => ["strong", 0], // Render as <strong> (semantically preferred over <b>)
        parseDOM: [{ tag: "strong" }, { tag: "b" }] // Parse both <strong> and <b> tags from HTML
    };

    const italicMark: MarkSpec = {
        toDOM: (): DOMOutputSpec => ["em", 0], // Render as <em> (semantically preferred over <i>)
        parseDOM: [{ tag: "em" }, { tag: "i" }] // Parse both <em> and <i> tags
    };

    const codeMark: MarkSpec = { // For inline code snippets (e.g., `variableName`)
        toDOM: (): DOMOutputSpec => ["code", { class: "inline-code-mark" }, 0], // Add a class for specific styling
        parseDOM: [{ tag: "code" }] // Parse <code> tags (distinguish from <pre><code> for code blocks)
    };

    const strikeMark: MarkSpec = { // For strikethrough text
        toDOM: (): DOMOutputSpec => ["s", 0], // Render as <s> (strikethrough)
        parseDOM: [{ tag: "s" }, { tag: "del" }, { tag: "strike" }] // Parse various common strikethrough tags
    };

    // TODO during Phase 2: Define other marks like underline.

    // --- Schema Assembly ---
    // The final schema object that combines all defined nodes and marks.
    export const appSchema = new Schema({
      nodes: {
        doc: docNode,
        paragraph: paragraphNode,
        heading: headingNode,
        text: textNode,
        hard_break: hardBreakNode,
        horizontal_rule: horizontalRuleNode, // Added
        image: imageNode, // Added
        code_block: codeBlockNode,
        mermaid_diagram: mermaidDiagramNode,
        drawio_diagram: drawioDiagramNode,
        fallback_macro_placeholder: fallbackMacroPlaceholderNode,
        // TODO: Add list and table nodes (bullet_list, ordered_list, list_item, table, table_row, table_cell, table_header)
      },
      marks: {
        link: linkMark,
        bold: boldMark,
        italic: italicMark,
        code: codeMark, // Inline code
        strike: strikeMark,
        // TODO: Add underlineMark
      }
    });
    ```
*   This `appSchema` will be progressively built out with all necessary standard nodes (image, bullet_list, ordered_list, list_item, table nodes, horizontal_rule) and marks (underline) during Phase 2 of development.
*   **Server-Side Validation of `raw_content`:** As mentioned in the `Page` model definition (Section I.B.3), the `raw_content` (ProseMirror JSON) submitted to the backend API when creating or updating pages will be validated against this `appSchema` (or key structural aspects of it) before being saved to the database. This validation can be performed by:
    1.  Attempting to parse the incoming JSON into a ProseMirror document using the `appSchema` on the server. This would ideally be done using ProseMirror libraries directly, which might require a Node.js microservice for this validation step if suitable Python libraries are unavailable.
    2.  Alternatively, a JSON Schema derived from the `appSchema` could be used for validation with Python-based JSON Schema validators.
    3.  Simpler structural checks (e.g., ensuring it's a valid JSON object with a `type: "doc"` root and `content` array) can be a first step if full schema validation is too complex initially.
    This server-side validation is crucial for data integrity and preventing malformed content from being stored.

**B. Tiptap Editor Configuration:**
*   The Tiptap editor instance in the React frontend will be initialized with the fully defined `appSchema` (from Section III.A).
*   **Core Extensions:** A comprehensive set of Tiptap's official extensions will be used to provide standard rich text editing features:
    *   `@tiptap/starter-kit`: This package conveniently bundles many common extensions, including Document, Paragraph, Text, Bold, Italic, Strike, Heading, Blockquote, ListItem, OrderedList, BulletList, HardBreak, HorizontalRule, and History (for undo/redo).
    *   Specific extensions will be added individually as needed:
        *   `@tiptap/extension-link`: For creating and editing hyperlinks.
        *   `@tiptap/extension-table`, `@tiptap/extension-table-row`, `@tiptap/extension-table-cell`, `@tiptap/extension-table-header`: For full table support.
        *   `@tiptap/extension-task-list`, `@tiptap/extension-task-item`: For creating interactive checklist items.
        *   `@tiptap/extension-image`: For inserting and managing images (potentially with attributes for src, alt, title, and alignment).
        *   `@tiptap/extension-placeholder`: To show placeholder text in an empty editor or empty nodes.
        *   `@tiptap/extension-focus`: For managing focus states and styling focused nodes.
        *   `@tiptap/extension-character-count`: To display character or word count if desired.
*   **Custom Tiptap Extensions and/or React Node Views** will be created for the custom nodes defined in `appSchema` to provide specialized editing experiences:
    *   **Code Blocks (`code_block` node):**
        *   A custom Tiptap extension (or modification of the official one) will manage the `language` and `uuid` attributes.
        *   The `uuid` attribute will be automatically generated (client-side using a library like `uuid`) when a new code block node is created. This UUID is crucial for LLM features like "Explain this Code."
        *   A React Node View for code blocks will provide UI elements such as:
            *   A language selection dropdown menu (which updates the `language` attribute of the node).
            *   Potentially a "Copy to Clipboard" button.
            *   Display of the selected language.
        *   Syntax highlighting within the editor's Node View will be provided by integrating `highlight.js` (or a similar library like Prism.js or CodeMirror's highlighting capabilities if CodeMirror is used for the text input part of the code block). The HTML output generated by `highlight.js` for highlighting will be sanitized using `DOMPurify` (as detailed in Section VI.C) before being rendered if it produces complex HTML structures that could pose an XSS risk.
    *   **Mermaid Diagrams (`mermaid_diagram` node):**
        *   A React Node View will be implemented for this node.
        *   It will provide an input area (this could be an inline editable region within the node view itself, or a separate modal window that opens when the user interacts with the node, e.g., clicks an "Edit Syntax" button) for users to type or paste Mermaid diagram syntax. This syntax is stored in the node's `syntax` attribute.
        *   A debounced live preview of the diagram, rendered using the client-side `Mermaid.js` library, will be displayed within this Node View as the user types or modifies the syntax. The debounce mechanism (e.g., updating preview 500ms after last keystroke) prevents excessive re-renders.
        *   **Performance (Web Workers for Mermaid Preview):** For very large or computationally complex Mermaid diagrams where client-side `Mermaid.js` rendering in the main UI thread causes noticeable performance issues (editor lag, unresponsiveness), the rendering task for the preview will be offloaded to a Web Worker. This keeps the main thread free. Cancellation logic for ongoing render tasks in the worker will be implemented if the diagram syntax changes again before the current render completes.
    *   **Draw.io Diagrams (`drawio_diagram` node):**
        *   The React Node View for this node in the Tiptap editor will typically display a placeholder image or a static SVG preview of the diagram (if a preview can be easily generated or stored).
        *   Interacting with this node (e.g., clicking an "Edit Diagram" button provided within the Node View UI) will launch the full `draw.io` editor. This editor will be embedded using a library like `diagrams.net-embed` (or by constructing the URL for `embed.diagrams.net`) within a sandboxed `<iframe>`. This iframe will usually be presented inside a full-screen or large modal window to provide an optimal editing experience.
        *   The current diagram's XML data (retrieved from the Tiptap node's `xml` attribute) will be passed to the `draw.io` iframe editor when it is initialized.
        *   Upon closing the `draw.io` editor (e.g., when the user clicks "Save" or "Apply" within the draw.io UI), the updated diagram XML data will be retrieved from the iframe (e.g., via `window.postMessage` API communication or a callback mechanism provided by the embedding library) and saved back to the Tiptap node's `xml` attribute, triggering an editor update.
        *   **Performance (Lazy Loading for Draw.io in Page View Mode):** For draw.io diagrams when viewing a rendered page (i.e., not in edit mode), if rendering many or very large/complex diagrams (which are often represented as intricate SVGs derived from the XML) significantly impacts the initial page load performance, a lazy-loading strategy will be implemented. Diagrams that are initially below the visible fold of the page will only be fully rendered when they are about to scroll into view (e.g., using `IntersectionObserver` API). Alternatively, they might initially render as lightweight static image previews (if feasible to generate/store such previews), with an option for the user to click to load the full, interactive, and scalable diagram.
    *   **Fallback Macro Placeholders (`fallback_macro_placeholder` node):**
        *   This will be implemented as a non-editable (atomic) Tiptap node.
        *   Its React Node View will clearly display information about the original Confluence macro that could not be converted during import (e.g., its name, retrieved from the `macroName` attribute).
        *   The Node View might offer an option (e.g., a tooltip that appears on hover, or a small clickable "details" icon) to view the raw HTML/text content of the original Confluence macro (which would involve fetching this raw content from the backend using the `fallbackMacroId` attribute stored in the node, or embedding it directly if small enough and safe).
*   **Editor Toolbar/Menu System:** A user-friendly and configurable editor interface will be provided to allow users to easily apply formatting and insert rich content:
    *   **Fixed Top Toolbar:** A primary toolbar, similar in concept to those found in traditional word processors or other rich text editors, will likely be present at the top of the Tiptap editor instance. This toolbar will contain buttons and dropdowns for common formatting options (headings, bold, italic, lists, blockquotes), node insertion commands (e.g., insert table, code block, Mermaid diagram, Draw.io diagram, image, horizontal rule), link creation/editing, and potentially text alignment.
    *   **Contextual Bubble Menu (Optional but Recommended):** A floating "bubble menu" may also be implemented. This menu appears contextually when the user selects text within the editor, offering quick access to relevant inline formatting options (e.g., bold, italic, link, inline code).
    *   The toolbar and/or bubble menu will also provide UI elements for interacting with the custom nodes (e.g., an "Edit Mermaid Syntax" button when a Mermaid node is selected, an "Edit Draw.io Diagram" button for a Draw.io node, or a language selector for a code block).
    *   The design of the toolbar/menu system will prioritize ease of use, discoverability of features, and a clean, uncluttered appearance.

**C. Markdown Mode (On-Demand Conversion):**
*   A toggle button or a mode switcher within the editor UI will allow users to switch between the default WYSIWYG (What You See Is What You Get) Tiptap editor experience and a "Raw Markdown Mode."
*   **WYSIWYG to Markdown Conversion (Viewing/Editing Markdown):**
    *   When the user activates Raw Markdown Mode, the current ProseMirror document (which is internally a JSON object representing the rich text content) from the Tiptap editor will be serialized into a standard Markdown string (preferably GitHub Flavored Markdown - GFM).
    *   This conversion will be handled by:
        *   Tiptap's official `@tiptap/extension-markdown`'s `getMarkdown()` method, if its output is sufficiently comprehensive and configurable to accurately represent all elements in the `appSchema`.
        *   Alternatively, the `prosemirror-markdown` library can be used. This would require creating a custom `MarkdownSerializer` instance specifically configured to map all nodes (paragraphs, headings, lists, code blocks, custom nodes if they have a sensible Markdown representation) and marks (bold, italic, links, inline code) in the `appSchema` to their GFM equivalents. For custom nodes like diagrams, a placeholder syntax might be used in Markdown (e.g., ````mermaid\n...\n````, or a custom tag like `[drawio:diagram_data_goes_here]`).
    *   The resulting Markdown string will then be displayed in a dedicated text editor component. For a better user experience with Markdown, this component could be CodeMirror or Monaco Editor (configured for Markdown syntax highlighting and other editing aids), or a simpler `<textarea>` if a lighter solution is preferred for MVP.
*   **Markdown to WYSIWYG Conversion (Applying Markdown Edits):**
    *   When the user has finished editing the content in Raw Markdown Mode and wishes to apply these changes back to the WYSIWYG view (e.g., by clicking an "Apply Markdown Changes" or "Switch to Rich Text Editor" button):
        *   The edited Markdown string from the text editor component will be parsed back into a ProseMirror document object.
        *   This parsing will be handled by Tiptap's `setContent()` method (if it's configured with `@tiptap/extension-markdown` and can accept Markdown input directly, parsing it based on the active extensions) or by using the `prosemirror-markdown` library with a custom `MarkdownParser` configured for the `appSchema`. This parser would need rules to convert GFM constructs back into the corresponding ProseMirror nodes and marks.
        *   The resulting ProseMirror document will then replace the entire current content of the Tiptap WYSIWYG editor.
*   **No Live Synchronization:** This on-demand conversion approach is chosen explicitly to avoid the significant complexities, potential performance overhead, and risks of data loss or corruption that can arise from attempting to implement live, bi-directional synchronization between a WYSIWYG view and a raw Markdown view. The user makes a conscious decision to switch between editing modes, and conversions happen at the point of switching.

---

**IV. API Contract Specifications (OpenAPI 3.0)**

*   All backend API endpoints will be formally and meticulously documented using the **OpenAPI 3.0 specification**. This specification serves as the definitive contract between the frontend and backend, ensuring clarity, consistency, and facilitating easier integration and testing.
*   **Key aspects covered by the OpenAPI specification for every endpoint:**
    *   **Paths and HTTP Methods:** Clear definition of all resource URLs (e.g., `/api/pages/`, `/api/pages/{id}/`) and the allowed HTTP methods (GET, POST, PUT, DELETE, PATCH) for each resource.
    *   **Request and Response Schemas:** Detailed JSON Schema definitions for all request bodies and response payloads. This includes specifying the precise expected structure for ProseMirror JSON when sending or receiving `raw_content` fields (this might be a general `type: object` with a description pointing to the ProseMirror schema documentation, or a more detailed, representative JSON schema snippet if feasible within OpenAPI's constraints).
    *   **Parameters:** Comprehensive definition of:
        *   Path parameters (e.g., `{id}` in `/api/pages/{id}/`), including their data type (integer, string, uuid) and format.
        *   Query parameters (e.g., `?space_key=DEV&tag=release&limit=25&offset=0` for filtering and paginating lists), including their data type, format, and whether they are required or optional.
        *   Request headers that are expected or relevant (e.g., `Authorization` for JWT, `Content-Type`, `Accept`).
    *   **Status Codes:** Clear documentation of all expected HTTP status codes for successful operations (e.g., 200 OK, 201 Created, 204 No Content) and for various error conditions (e.g., 400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found, 422 Unprocessable Entity for validation errors, 429 Too Many Requests for rate limiting, 500 Internal Server Error). The schema for standardized error responses (see Section VII.A) will also be defined and referenced.
    *   **Authentication Methods:** Specification of the authentication method required for each endpoint (typically JWT Bearer token in the `Authorization` header). This will be defined via OpenAPI `securitySchemes` and applied to operations using the `security` keyword.
    *   **Tags:** Grouping of related endpoints using tags (e.g., "Pages", "Spaces", "Authentication", "LLM Services", "Attachments", "Search", "ImportExport") for better organization and navigability in the generated API documentation.
    *   **Summaries and Descriptions:** Concise summaries for each operation (e.g., "Create a new page") and more detailed descriptions explaining its purpose, behavior, any preconditions, or specific business rules. Parameters and schema properties will also have descriptions.
    *   **Examples:** Illustrative examples for request bodies and response payloads to aid understanding, integration by frontend developers, and automated testing.
*   **Tooling for Generation and Documentation:**
    *   The **`drf-spectacular`** Django package will be used for API documentation. This tool can auto-generate the OpenAPI 3.0 schema directly from Django REST Framework serializers, viewsets, view function docstrings, and Python type hints. This approach helps ensure that the API specification remains closely synchronized with the actual backend implementation, reducing the risk of outdated or incorrect documentation.
    *   The generated OpenAPI schema (typically as `openapi.yaml` or `openapi.json`) will be made available via a dedicated, unauthenticated API endpoint (e.g., `/api/schema/openapi.yaml`).
    *   This schema can then be consumed by various tools:
        *   **Interactive API Documentation:** Tools like **Swagger UI** or **Redoc** can be easily integrated into the Django project (e.g., served as views by `drf-spectacular`) or hosted as separate static sites. These tools parse the OpenAPI schema and provide interactive, human-readable documentation where developers can explore endpoints, view schemas, and even try out API calls directly from their browser (if configured).
        *   **Client Code Generation:** OpenAPI Generator or similar tools can use the schema to generate client SDKs in various programming languages (including TypeScript for the frontend), simplifying API integration.
        *   **Contract Testing:** The schema can be used as a basis for contract testing to ensure that the frontend and backend adhere to the defined API specifications.
*   **Pagination Standards:**
    *   API endpoints that return lists of resources (e.g., `/api/pages/`, `/api/spaces/`, `/api/pages/{id}/versions/`) will use **offset-based pagination by default for the MVP**. This typically involves `limit` (number of items per page) and `offset` (starting index of items) query parameters (e.g., `?limit=25&offset=50`). Django REST Framework's `LimitOffsetPagination` class will be the standard. API responses for paginated lists will include:
        *   `count`: Total number of items available.
        *   `next`: URL for the next page of results (or `null` if on the last page).
        *   `previous`: URL for the previous page of results (or `null` if on the first page).
        *   `results`: An array of the actual resource items for the current page.
    *   **[Post-MVP Consideration] Cursor-based pagination** may be considered as a post-MVP enhancement for specific high-volume, frequently updated feeds (e.g., a global activity stream, if such a feature is implemented) where offset pagination can suffer from performance issues (due to `COUNT(*)` and `OFFSET` clauses on large tables) or item duplication/skipping when new items are added while paginating.
*   **OpenAPI Snippets for All Endpoints (including LLM APIs):** The OpenAPI specification generated by `drf-spectacular` will comprehensively cover *all* defined API endpoints, including those dedicated to LLM interactions (e.g., text generation, code explanation, diagram code generation). This will detail their specific request/response schemas, including how streaming responses are handled for text generation, the structure of prompt and context inputs, and any model selection parameters.
*   **List of Key API Endpoints (Illustrative for Initial Scaffolding - Full details in OpenAPI Spec):**
    *   **Authentication & User Management:**
        *   `POST /api/auth/register/` (User self-registration)
        *   `POST /api/auth/login/` (Obtain JWT access/refresh tokens)
        *   `POST /api/auth/token/refresh/` (Refresh access token)
        *   `POST /api/auth/logout/` (If using token blacklisting for refresh tokens)
        *   `GET /api/auth/user/` (Get current authenticated user's details)
        *   `POST /api/auth/password/reset/` (Request password reset email)
        *   `POST /api/auth/password/reset/confirm/` (Confirm password reset with token)
        *   `POST /api/auth/email/verify/` (Request email verification)
        *   `POST /api/auth/email/verify/confirm/` (Confirm email with token)
    *   **Spaces:**
        *   `GET /api/spaces/` (List spaces)
        *   `POST /api/spaces/` (Create space)
        *   `GET /api/spaces/{space_key}/` (Retrieve space)
        *   `PUT /api/spaces/{space_key}/` (Update space)
        *   `DELETE /api/spaces/{space_key}/` (Soft-delete space)
    *   **Pages & Versions:**
        *   `GET /api/spaces/{space_key}/pages/` (List pages in a space)
        *   `POST /api/pages/` (Create page)
        *   `GET /api/pages/{page_id}/` (Retrieve page)
        *   `PUT /api/pages/{page_id}/` (Update page, creates new version)
        *   `DELETE /api/pages/{page_id}/` (Soft-delete page)
        *   `GET /api/pages/{page_id}/versions/` (List page versions)
        *   `GET /api/pages/{page_id}/versions/{version_number}/` (Retrieve specific page version)
        *   `POST /api/pages/{page_id}/revert/{version_number}/` (Revert page to a version)
    *   **Attachments:**
        *   `GET /api/pages/{page_id}/attachments/` (List attachments for a page)
        *   `POST /api/attachments/presigned-upload-url/` (For S3: Get URL for client direct upload)
        *   `POST /api/attachments/complete-upload/` (For S3: Notify backend of successful client upload)
        *   `POST /api/attachments/` (For local storage: Upload file through server, triggers scan)
        *   `GET /api/attachments/{attachment_id}/download/` (Download attachment with secure headers)
        *   `DELETE /api/attachments/{attachment_id}/` (Delete attachment)
    *   **Import/Export & Utilities:**
        *   `POST /api/io/import/confluence/` (Start Confluence import task)
        *   `GET /api/io/import/confluence/status/{task_id}/` (Get import task status)
        *   `GET /api/io/pages/{page_id}/export/markdown/` (Export page as Markdown)
        *   `POST /api/io/pages/import/markdown/` (Import Markdown file as new page)
        *   `POST /api/diagrams/validate/mermaid/` (Validate Mermaid syntax)
    *   **LLM Services:**
        *   `POST /api/llm/generate-text/` (Streaming response)
        *   `POST /api/llm/explain-code/`
        *   `POST /api/llm/generate-diagram-code/`
    *   **Search & Tags:**
        *   `GET /api/search/` (Full-text search with filters)
        *   `GET /api/tags/` (List all tags)
        *   `POST /api/tags/` (Create tag - admin/privileged)
        *   `GET /api/pages/{page_id}/tags/` (List tags for a page)
        *   `POST /api/pages/{page_id}/tags/` (Add tag to page)
        *   `DELETE /api/pages/{page_id}/tags/{tag_id_or_name}/` (Remove tag from page)
    *   **Permissions (Conceptual - actual paths may vary with `django-guardian` or custom views):**
        *   `GET /api/spaces/{space_key}/permissions/` (List permissions for a space)
        *   `POST /api/spaces/{space_key}/permissions/` (Set/update permissions for a space)
    *   **System:**
        *   `GET /api/health/` (Application health check)
        *   `GET /api/schema/openapi.yaml` (Serves the OpenAPI specification document)

---

**V. Key Features & Requirements (Consolidated & Refined)**

**A. Confluence Import (Target: HTML Export from Confluence):**
*   **Input Format:** The importer will process Confluence Space HTML Exports, which are typically provided as zip files containing multiple HTML pages, associated attachments (images, documents, etc.), and often an XML or JSON metadata file (e.g., `entities.xml` or similar). This metadata file is crucial as it usually details page hierarchy (parent-child relationships via page IDs), original page IDs, author information, and page creation/modification dates.
*   **Process Steps (Asynchronous Task via Celery):** The entire import process for a space will be handled as an asynchronous background task managed by Celery to prevent blocking API requests and to handle potentially long-running operations.
    1.  **Archive Extraction:** Upon API request, the uploaded Confluence export zip file is saved, and a Celery task is initiated. The task first extracts the contents of the zip file to a temporary processing directory.
    2.  **Metadata Parsing:** The Celery task identifies and parses Confluence's metadata file from the extracted contents. This step is critical for extracting:
        *   Page IDs and their corresponding parent page IDs (to reconstruct the page hierarchy).
        *   Original authors and creation/modification dates (to populate these fields in the local database as accurately as possible).
        *   Relationships between pages and their attachments.
    3.  **HTML File Parsing Strategy:** For each HTML file representing a Confluence page:
        *   **Large HTML File Handling:** For very large individual HTML files (e.g., exceeding a configurable threshold like 50-100MB, though this is rare for single pages), where loading the entire DOM into memory with `BeautifulSoup4` could lead to memory exhaustion, the importer will investigate using a **SAX (Simple API for XML) parser (e.g., Python's built-in `xml.sax` if the HTML is well-formed enough to be treated as XML, which is often not the case for arbitrary HTML) or, more practically, an incremental HTML parser (like Python's built-in `html.parser` with custom event handlers).** This would allow processing these exceptionally large files in chunks or streams, reducing peak memory usage. For typical HTML files, `BeautifulSoup4` will be the primary parsing tool due to its ease of use and robust handling of malformed HTML. This incremental parsing is an advanced optimization to be implemented primarily if significant performance or memory issues are encountered with real-world large export files.
    4.  **Content Conversion (HTML -> ProseMirror JSON):**
        *   **Primary Strategy (Direct Conversion):** The core conversion approach will be direct HTML to ProseMirror JSON. This involves:
            *   Using `BeautifulSoup4` (or the chosen incremental parser for very large files) to parse the HTML string of a page into a traversable Document Object Model (DOM)-like structure.
            *   Developing custom Python mapping logic. This logic will iterate through the parsed HTML structure and systematically transform recognized Confluence elements (such as headings H1-H6, paragraphs `<p>`, lists `<ul>/<ol>/<li>`, tables `<table>/<tr>/<td>/<th>`, links `<a>`, images `<img>`, common formatting tags like `<strong>`, `<em>`, `<code>`, and common macros) into their corresponding nodes and marks as defined in the application's `appSchema` (see Section III.A) for ProseMirror JSON. This mapping needs to be robust to handle variations in Confluence's HTML output.
        *   **Fallback Strategy for Complex/Non-Standard HTML:** For HTML elements or structures that are exceptionally complex, non-standard (e.g., due to heavy use of obscure or custom Confluence user macros, or deeply nested unconventional layouts), or poorly formed in some Confluence exports, and which prove difficult to map directly and reliably to ProseMirror JSON, an intermediate conversion step may be employed as a fallback for those specific problematic sections:
            1.  HTML -> Markdown: Use a library like `Turndown` (or a Python equivalent) to convert the problematic HTML snippet into Markdown.
            2.  Markdown -> ProseMirror JSON: Use `Remark` (if a Python bridge or direct Python port is available and suitable for AST conversion) or `prosemirror-markdown` (again, potentially via a bridge or by processing the Markdown on the client-side during a final import review step, though server-side is preferred for batch processing) to parse this Markdown into a ProseMirror document structure compatible with `appSchema`. This approach accepts that there might be some minor fidelity loss for these specific complex elements but aims to ensure that their textual content is not entirely lost.
    5.  **Page Hierarchy Reconstruction:** Using the parent-child relationships extracted from the Confluence metadata (mapping `pageId` to `parentId`), accurately recreate the original page hierarchy within the application's database. This involves creating `Page` model instances and correctly setting their `parent_page` foreign key relationships. This step often requires processing pages in an order that respects dependencies or using multiple passes.
    6.  **Confluence Macro Conversion Logic:**
        *   **Supported Macros:** Implement specific Python converter functions for common and essential Confluence macros:
            *   `{code}` macro (and its variants like `{noformat}`): Convert to a Tiptap `code_block` node. The converter will attempt to detect and preserve the programming language specified in the macro's parameters (e.g., `language=java`) or from CSS classes present in the rendered HTML output of the macro.
            *   `[draw.io]` macro (or Confluence's native embedded draw.io diagrams): Extract the embedded draw.io diagram data, which is usually stored as XML. Create a Tiptap `drawio_diagram` node within the ProseMirror JSON, storing this extracted diagram XML data in its `xml` attribute.
            *   `[Mermaid]` macro (or Confluence's native embedded Mermaid diagrams): Extract the Mermaid syntax text from the macro body or its rendered output. Create a Tiptap `mermaid_diagram` node within the ProseMirror JSON, storing the extracted Mermaid syntax in its `syntax` attribute.
        *   **Unsupported Macros:** For Confluence macros that are not explicitly supported by dedicated converters (this includes many built-in macros like `{panel}`, `{info}`, `{note}`, `{tip}`, `{warning}`, various chart macros, JIRA integration macros, and any custom user macros), a `FallbackMacro` record will be created in the database. This record will store the macro's original name (if discernible) and its raw HTML or text content as it appeared in the Confluence export. In the imported ProseMirror JSON content, a `fallback_macro_placeholder` node will be inserted at the macro's original position. This placeholder will reference the `FallbackMacro` database record and will render on the frontend as a distinct block clearly indicating an unsupported macro, potentially offering an option for users to view the original raw content for manual migration or reference.
    7.  **Attachment Handling during Import:**
        *   Identify and extract attachment files (images, documents, etc.) referenced in the Confluence export and linked to specific pages.
        *   For each attachment, create an `Attachment` record in the application's database, storing metadata such as the original filename, detected MIME type (if possible from export data or file extension, otherwise a generic type), and file size.
        *   Store the actual attachment file binaries using the configured `django-storages` backend (e.g., copying to the local `MEDIA_ROOT` or uploading to an S3 bucket). For large batch imports, server-side upload to S3 (if S3 is configured as the backend) might be more efficient than generating individual pre-signed URLs for each imported file.
        *   Ensure these imported attachments are correctly linked or embedded within the ProseMirror JSON content of their respective pages (e.g., creating `image` nodes in ProseMirror for embedded images, or creating links to other downloadable file types).
        *   Imported attachments will also be subject to the asynchronous virus scanning pipeline (see Section V.C.5).
    8.  **User Mapping (Best Effort during Import):** During the import process, the system will attempt to map Confluence users (who are identified as authors or last modifiers of pages/versions in the export metadata) to existing users in the application's database. This mapping will typically be based on matching email addresses or usernames if this information is consistently available and formatted in the Confluence export data. If no corresponding application user is found for a Confluence user, imported content can be attributed to a designated "Default Import User" account (which should be created as part of the application setup) or, alternatively, to the user account that initiated the import process.
*   **Illustrative Confluence HTML to ProseMirror JSON Mapping Example:**
    *   **Confluence HTML Snippet (Simplified):**
        `<h1>My Page Title</h1><p>Some <strong>bold</strong> text and a <a href="http://example.com" title="Example Link">link</a>.</p><ul><li>Item 1</li></ul><pre class="brush: java; gutter: false">public class HelloWorld {}</pre>`
    *   **Target ProseMirror JSON Snippet (Simplified, actual schema is richer and more structured):**
        ```json
        {
          "type": "doc",
          "content": [
            { "type": "heading", "attrs": { "level": 1 }, "content": [{ "type": "text", "text": "My Page Title" }] },
            { "type": "paragraph", "content": [
              { "type": "text", "text": "Some " },
              { "type": "text", "marks": [{ "type": "bold" }], "text": "bold" },
              { "type": "text", "text": " text and a " },
              { "type": "text", "marks": [{ "type": "link", "attrs": { "href": "http://example.com", "title": "Example Link", "target": "_blank" } }], "text": "link" },
              { "type": "text", "text": "." }
            ]},
            { "type": "bullet_list", "content": [
              { "type": "list_item", "content": [
                { "type": "paragraph", "content": [{ "type": "text", "text": "Item 1" }] }
              ]}
            ]},
            { "type": "code_block", "attrs": { "language": "java", "uuid": "client-generated-uuid-123" }, "content": [{ "type": "text", "text": "public class HelloWorld {}" }] }
          ]
        }
        ```
    This example provides a concrete target for the import conversion logic for some basic elements. More complex mappings for tables, nested lists, various Confluence macros, and inline formatting will need to be defined to align with the full `appSchema`.
*   **Scalability Note for `CC_IMPORT_MAX_PAGES_PER_SPACE` (Configuration Setting):** The documentation for this configuration setting (which provides a guideline for the maximum number of pages per space the importer is robustly tested against, e.g., 5000) will clearly state that this is a general guideline. The actual resource consumption (CPU, memory, disk I/O, network bandwidth) and processing time for an import depend heavily on factors beyond just page count. These include the average number and size of attachments per page, the complexity and length of page content (e.g., pages with many large embedded diagrams or tables), and the overall size of the Confluence export archive. Administrators performing very large imports (e.g., tens of thousands of pages or hundreds of gigabytes of attachments) should monitor system resources (application server, database, Celery workers) closely during the import process. The import process itself may include internal checks or detailed logging to flag disproportionately large attachment loads or unusually complex pages that might significantly slow down processing or consume excessive resources.
*   **Fidelity Definition & Goal in Confluence Import:** "Fidelity" in the context of Confluence import for this project means:
    *   **Structural Integrity:** Accurate preservation and mapping of fundamental document structure (headings H1-H6, paragraphs, unordered lists `<ul>`, ordered lists `<ol>`, list items `<li>`, tables `<table>` with rows `<tr>` and cells `<td>`/`<th>`, blockquotes `<blockquote>`, horizontal rules `<hr>`).
    *   **Content Preservation:** Correct retention of all textual content within these structures, including preservation of whitespace where significant (e.g., in `<pre>` blocks or code macros). Basic inline formatting (bold `<strong>`/`<b>`, italic `<em>`/`<i>`, underline `<u>` (if supported), strikethrough `<s>`/`<del>`, inline code `<code>`, hyperlinks `<a>`) should be accurately translated to corresponding ProseMirror marks.
    *   **Macro Conversion:** Successful conversion of the specified common Confluence macros (`{code}`, `[draw.io]`, `[Mermaid]`) to their functional equivalents as defined Tiptap nodes within the application, preserving their core data (code language and content, diagram XML or syntax). For unsupported macros, they should be gracefully handled by creating `FallbackMacro` database records and corresponding `fallback_macro_placeholder` nodes in the content, ensuring original macro content is preserved for user review or future processing.
    *   **Attachment Integrity:** All attachments referenced in the Confluence export and linked to specific pages must be correctly extracted, stored by the application, and then accurately linked or embedded within the corresponding imported page content. For common image types (JPEG, PNG, GIF), image dimensions and visual appearance should be preserved as closely as possible when rendered. Other file types should be correctly linked for download.
    *   **Hierarchy Accuracy:** Accurate reconstruction of the original page hierarchy (parent-child relationships between all pages in the imported space) as defined in the Confluence export metadata.
    *   The **goal is to achieve 90-95% fidelity** for these defined aspects on typical, reasonably well-structured Confluence exports from the targeted supported Confluence versions. Pixel-perfect replication of Confluence's exact rendering or styling is not a goal; functional and structural fidelity is key.
*   **Compatibility & Iterative Improvement of Importer:** Initial development and testing of the Confluence importer will focus on exports from recent and common versions of Confluence Cloud and Confluence Server/Data Center (e.g., the last 2-3 major versions generally available at the time of development). A compatibility matrix outlining tested Confluence versions and any known limitations or specific issues with certain export formats or Confluence features will be developed and maintained as part of the project documentation, and updated post-launch based on user feedback. The Confluence importer is recognized as a complex component due to the wide variability in Confluence content and export formats (especially with third-party macros). Therefore, it will be an area of ongoing iterative improvement. A mechanism for users to (optionally and securely) submit problematic (but anonymized, if possible) export snippets that fail to import correctly will be considered to help improve the importer over time.

**B. LLM AI-Assisted Authoring:**
1.  **Integration Strategy & LLM Interaction:**
    *   Backend API endpoints (developed in Django) will orchestrate all interactions with local Large Language Models, ensuring that LLM processing logic is centralized and managed by the application server.
    *   **Primary Interaction Method (`llama-cpp-python`):** The preferred and primary method for interacting with LLMs will be direct integration with GGUF (Georgi Gerganov Universal Format) quantized models. These models are optimized for running efficiently on CPUs and can also leverage GPU acceleration if available. The `llama-cpp-python` library, which provides Python bindings for the underlying `llama.cpp` C/C++ library, will be used for this. This method offers:
        *   Fine-grained control over model loading (from local file paths specified by `CC_LLM_MODEL_PATH` or via a model alias mapping defined in `CC_LLM_MODEL_MAPPING`).
        *   Detailed control over context management (number of tokens, sliding window if supported).
        *   Precise adjustment of inference parameters (e.g., temperature, top_p, top_k, repetition penalty).
        *   Support for configurable GPU layer offloading (`CC_LLM_N_GPU_LAYERS` setting in Django) if a compatible GPU (typically NVIDIA with CUDA, or AMD with ROCm, depending on the `llama.cpp` build) is available on the server.
    *   **Alternative Interaction (Ollama HTTP API):** As a fallback or for users who already have an Ollama server instance running locally or on their network and prefer to manage their LLMs through it, the application will also support interacting with LLMs via Ollama's HTTP API. This mode of operation will be configurable via the `CC_LLM_PROVIDER="ollama_http"` setting, and the `CC_OLLAMA_API_BASE_URL` setting will specify the Ollama server's endpoint. The application will then make standard HTTP requests to Ollama for LLM inference.
    *   **LangChain (Python Library):** The LangChain library will be extensively used on the backend as an orchestration framework for building LLM-powered features. Its key roles will include:
        *   **Prompt Templating:** Creating, managing, and formatting structured prompts for various AI tasks (e.g., text summarization, code explanation, diagram generation from natural language). This allows for consistent and optimized prompting.
        *   **Context Management:** Implementing strategies for managing the context window limitations of different LLMs. This might involve techniques like "stuffing" relevant page content or selected text into the prompt, summarizing long contexts before passing them to the main LLM, or managing conversational history if chat-like AI features are implemented.
        *   **LLM Interaction Abstraction:** Providing a consistent interface layer (LangChain's LLM wrappers) for making calls to the chosen LLM interaction method (`llama-cpp-python` or Ollama API), simplifying the application code that triggers LLM tasks.
        *   **Output Parsing:** Processing and validating responses received from LLMs. This includes extracting specific pieces of information from the LLM's textual output, converting generated text into required formats (e.g., ensuring generated Mermaid syntax is clean and free of extraneous explanations), and potentially using LangChain's `OutputFixingParser` or similar tools to attempt to automatically correct minor formatting issues in LLM outputs if they are expected to adhere to a specific structure (like JSON).
        *   **Chains/Agents (Simple Orchestration):** Potentially using simple LangChain "chains" (LCEL - LangChain Expression Language) to sequence multiple LLM calls or to combine LLM outputs with other application logic or tools (e.g., calling the internal Mermaid syntax validator after an LLM generates Mermaid code as part of a chain). More complex agentic behavior with autonomous tool use is out of scope for the MVP.
    *   **Caching LLM Responses (Semantic Keys & Redis):** To improve performance, reduce redundant LLM computations (which can be resource-intensive on CPU), and potentially manage costs if external LLM APIs were ever used, responses from LLMs for idempotent, non-streaming queries (e.g., "Explain this Code" for an identical code snippet, results of validating an identical piece of Mermaid syntax) will be cached in Redis. Cache keys will be generated based on a hash of semantically significant components of the request, rather than just the raw input string. This might include a hash of:
        *   The identifier for the specific prompt template being used.
        *   Key context elements passed to the LLM (e.g., a hash of the code snippet content being explained).
        *   The alias or identifier of the LLM model being used (e.g., `codellama-7b-instruct`).
        *   Key inference parameters that affect the output (e.g., temperature).
        This approach aims to improve cache hits for requests that are semantically similar in purpose, even if the exact wording of a user's free-form input part of the prompt might vary slightly.
    *   **Circuit Breaker for LLM Calls:** To enhance the resilience of AI features and prevent the application from being bogged down by an unresponsive or consistently failing LLM service, a circuit breaker pattern will be implemented for all calls to LLM services (this applies to both direct `llama-cpp-python` interactions and Ollama HTTP API calls).
        *   A library like `pybreaker` (or a similar Python circuit breaker implementation) will be used.
        *   Configuration: The circuit breaker will be configured with parameters such as:
            *   `failure_threshold`: Number of consecutive failures (e.g., 3-5) within a defined time window before the circuit "opens."
            *   `recovery_timeout`: Duration (e.g., 60 seconds) for which the circuit remains open, during which calls to the LLM service are immediately failed without attempting connection.
            *   After the recovery timeout, the circuit transitions to a "half-open" state, allowing a limited number of test calls. If these succeed, the circuit closes and normal operation resumes. If they fail, the circuit re-opens for another recovery timeout.
        *   This pattern improves user experience by providing faster feedback when an LLM feature is temporarily unavailable and protects the application server from resource exhaustion due to repeated, failing LLM calls.
    *   **GGUF Quantization Levels Guidance & Documentation:** The application's documentation will provide clear guidance to self-hosting users on selecting appropriate GGUF quantization levels for commonly used LLM models. This will include:
        *   Recommendations for specific quantization levels (e.g., **Q4_K_M or Q5_K_M for a good balance of inference speed, model quality, and RAM usage on CPUs; Q8_0 for higher quality if CPU resources are ample and slight performance decrease is acceptable; smaller Q2/Q3 variants for extremely resource-constrained environments like older Raspberry Pi models**, though with a noticeable drop in quality).
        *   Guidance on their typical resource usage (estimated RAM footprint, disk space for model files).
        *   Information on the performance trade-offs (speed vs. accuracy/coherence) associated with different quantization levels.
    *   **[Post-MVP Consideration] LLM Model Hot-Swapping:** For the MVP, the specific LLM models used by the application (e.g., the default model for text generation, the model for code explanation) will be configured at application startup via environment variables (e.g., `CC_LLM_MODEL_PATH` for `llama-cpp-python`, `CC_LLM_DEFAULT_MODEL_ALIAS` which maps to a model name for Ollama or a specific file for `llama-cpp-python`, potentially through a `CC_LLM_MODEL_MAPPING` configuration). Changing the active models will typically require an application restart.
        Future post-MVP enhancements could explore implementing a **runtime model registry**. This would allow administrators (via an admin UI or API) to:
        *   Add new LLM models (e.g., point to new GGUF files or register new Ollama model names).
        *   Remove existing models.
        *   Switch the active model for specific AI features without requiring a full application downtime.
        This would involve more complex dynamic loading and unloading of models if using `llama-cpp-python` directly (which can be resource-intensive) or dynamically updating configurations for an external LLM server like Ollama. It would also necessitate implementing robust health checks for newly registered or activated models before they are put into service.
2.  **Input/Output Security & Validation for LLMs:**
    *   **Input Sanitization (Defense against Prompt Injection):** All user-provided free-text inputs that are incorporated into LLM prompts (e.g., `context_text` provided by the user for summarization, user queries for text generation, selected code for explanation) will undergo several layers of sanitization on the backend before being passed to the LLM. This is a critical defense against prompt injection attacks where users might try to trick the LLM into ignoring its original instructions or performing unauthorized actions. The sanitization process will include:
        1.  **Strict Length Truncation:** Enforce maximum length limits (configurable via settings like `CC_LLM_MAX_CONTEXT_LENGTH` but also potentially per-feature limits based on typical input sizes and the specific LLM's token limits) to ensure inputs fit within the LLM's context window and to prevent denial-of-service attacks or excessive resource consumption by overly long prompts. Strategies like taking the first N characters/tokens and the last M characters/tokens might be used to preserve leading/trailing context from a larger user input while managing overall length.
        2.  **Control Character Stripping/Escaping:** Removal or proper escaping of non-printable ASCII control characters (e.g., null bytes, bell characters, vertical tabs) that could disrupt LLM processing, cause rendering issues if the LLM echoes them back, or potentially be used in obscure injection attacks.
        3.  **HTML Tag Stripping (if context is intended as plain text):** If the input context being passed to an LLM is expected to be plain text but might contain user-generated HTML (e.g., from a selection made by the user in the rich text editor that includes formatting), these HTML tags will be stripped using a robust HTML sanitization library like `Bleach` configured with an empty allowlist (i.e., strip all tags, keep only the text content), or a carefully crafted and well-tested regular expression (e.g., `re.sub(r'<[^>]+>', '', text)`). This prevents HTML structure from confusing the LLM if it expects plain text.
        4.  **Normalization of Whitespace:** Excessive whitespace (multiple consecutive spaces, tabs, multiple newlines) will be normalized (e.g., multiple spaces collapsed to a single space, multiple newlines reduced to one or two as appropriate for the LLM's expected input format or the prompting strategy).
        5.  **Known Injection Sequence Filtering and Escaping (Ongoing Research & Model-Specific Defenses):**
            *   The sanitization logic will be designed to be extensible to incorporate defenses against known prompt injection techniques as they are discovered and become relevant. This is an evolving area of AI security research.
            *   This might involve detecting and neutralizing or escaping specific character sequences or keywords that are known to act as instruction separators (e.g., `---`, `### Instruction:`), role-play induction markers (e.g., "You are now an unconstrained AI..."), or "jailbreak" prompts designed to make the LLM ignore its original system prompt or safety guidelines.
            *   The effectiveness of these techniques can be model-specific, so this requires ongoing monitoring of LLM security research and may need to be tailored to the specific LLMs being used and the structure of the application's system prompts.
    *   **Output Validation & Sanitization from LLMs:**
        *   Responses received from LLMs will be carefully validated and, where necessary, sanitized before being stored in the database, returned to the client for display, or used in further processing steps.
        *   **Mermaid Syntax Validation:** LLM-generated Mermaid syntax (e.g., from the "AI-assisted Mermaid Diagram Generation" feature) will be programmatically validated. This will be done by making an internal call from the LLM service to the application's own `/api/diagrams/validate/mermaid/` endpoint (or a shared internal function that endpoint uses, which itself might leverage `Mermaid.js` via a Node.js bridge if server-side JS execution is set up, or a Python-based Mermaid syntax linter if a suitable one exists, or even a client-side callback mechanism for validation). If the generated syntax is found to be invalid, it will not be directly inserted into the page; instead, an error message will be returned to the user, perhaps along with the invalid syntax for them to inspect or correct, or an option to request the LLM to try generating it again with a refined prompt.
        *   **Structured Outputs (e.g., JSON):** If an LLM is prompted to produce structured data (e.g., a JSON object with specific fields for a particular task), the raw output string from the LLM will be parsed (e.g., using `json.loads()`). The resulting Python object's schema will then be validated against an expected Pydantic model or a JSON Schema definition. LangChain's `OutputFixingParser` might be employed to attempt to automatically correct minor formatting issues in LLM outputs if they are close to the desired JSON structure but not perfectly valid (e.g., missing a comma, incorrect quoting).
        *   **Textual Outputs for Display:** Textual outputs from LLMs intended for direct display to the user (e.g., summaries, explanations, generated paragraphs of text) will generally be treated as plain text. Before rendering this text in an HTML context on the frontend, any characters that have special meaning in HTML (such as `<`, `>`, `&`, `"`, `'`) will be properly HTML-escaped (e.g., using a utility function similar to Python's `html.escape()` or relying on React's default JSX escaping behavior for text content) to prevent XSS vulnerabilities if the LLM output accidentally contains HTML-like structures or malicious script tags.
        *   **Controlled HTML Snippet Rendering (Sandboxed iFrames):** In very specific, controlled scenarios where an LLM might be guided by highly constrained prompts and post-processing to generate extremely simple and predictable HTML snippets (e.g., for creating custom formatted information boxes with only `<p>`, `<strong>`, `<em>` tags, or simple unordered lists, where the structure is predefined by application logic), this HTML output will be rendered within a sandboxed `<iframe>` on the frontend using the `srcdoc` attribute.
            *   The `sandbox` attribute of the iframe will be configured as restrictively as possible (e.g., `sandbox="allow-same-origin"` if the content needs no scripts or forms, or potentially `sandbox="allow-scripts allow-same-origin"` if minimal, trusted, and self-contained scripting within the iframe is absolutely necessary, but never `allow-top-navigation` or `allow-popups-to-escape-sandbox` unless essential and the risks are fully understood and mitigated).
            *   The HTML content provided to the `srcdoc` attribute must either be:
                *   Generated by trusted backend logic that tightly constrains the LLM's output to a very small, known-safe subset of HTML tags and attributes, possibly by using an LLM to fill in only textual parts of a predefined HTML template.
                *   Or, if based on more free-form LLM output (which is generally discouraged for direct HTML rendering), be heavily and rigorously sanitized using `DOMPurify` with an extremely strict allowlist (e.g., only allowing a handful of safe presentational tags like `<p>`, `<strong>`, `<em>`, `<ul>`, `<ol>`, `<li>` and no JavaScript-related attributes or dangerous tags like `<script>`, `<iframe>`, `<object>`).
            *   Rendering arbitrary LLM-generated HTML/JavaScript directly in the main page context is strictly out of scope due to significant and unacceptable security risks.
3.  **Key AI Features (with Streaming for Text Generation):**
    *   **Text Generation (with Streaming Responses):** This category includes features where the LLM generates prose content based on user prompts and potentially existing page content as context. Examples include:
        *   "Write an introduction for this page based on the following key points..."
        *   "Summarize this selected section of text..."
        *   "Suggest three alternative phrasings for this sentence: 'The quick brown fox...'"
        *   "Expand on this bullet point from the current page: 'Improve user onboarding experience.'"
        *   "Draft a paragraph explaining the benefits of X based on this context: '...'"
        The backend API endpoint (`POST /api/llm/generate-text/`) responsible for these text generation tasks will support **streaming HTTP responses**. This means that as the LLM generates text token by token (or in small chunks of tokens), these tokens will be immediately sent to the frontend client over an HTTP stream (e.g., using Django's `StreamingHttpResponse` with a generator function that yields tokens from `llama-cpp-python`'s streaming output, or potentially using Django Channels for more complex bi-directional real-time interactions, though `StreamingHttpResponse` is generally simpler and sufficient for this unidirectional streaming use case). The frontend React application will consume these streams and display the text to the user progressively in the UI (e.g., in the editor or a designated output area), providing a much better and more interactive user experience than making the user wait for the entire (potentially long) response to be generated before seeing any output.
    *   **"Explain this Code":** This feature allows users to get an AI-generated explanation of a code snippet.
        *   The user selects a code snippet within a page (this `code_block` node in ProseMirror will be identifiable by its client-side generated `uuid` attribute).
        *   This action triggers an API call (`POST /api/llm/explain-code/`) to the backend, sending the code content itself, its programming language (from the `language` attribute of the `code_block` node), the `uuid` of the code block, and potentially some surrounding page context (e.g., the page title or section heading) to help the LLM understand the code's purpose.
        *   An LLM specifically suited for code understanding (e.g., CodeLlama) then processes this input and returns a natural language explanation of the code's functionality, logic, potential issues, or suggestions for improvement.
        *   This response is typically not streamed, as it's a complete, self-contained explanation. The frontend displays this explanation to the user (e.g., in a modal, a sidebar panel, or an annotation near the code block).
    *   **AI-assisted Mermaid Diagram Generation:** This feature helps users create diagrams using natural language.
        *   Users can provide a textual description of a diagram they want to create (e.g., "Create a sequence diagram that shows the user login process, including steps for entering username/password, 2FA verification if enabled, and handling successful login or authentication failure scenarios.").
        *   This description is sent to the backend API (`POST /api/llm/generate-diagram-code/`).
        *   An LLM (e.g., Mistral, Llama variants, or a model fine-tuned for code/syntax generation) attempts to generate the corresponding Mermaid syntax for the described diagram.
        *   The generated Mermaid syntax is then validated on the backend (as per V.B.2).
        *   If valid, the syntax is returned to the frontend, where it can be inserted into a `mermaid_diagram` node in the Tiptap editor for further refinement by the user or direct rendering. If invalid, an error message is returned.
4.  **Rate Limiting for LLM Endpoints:** All API endpoints that trigger LLM interactions (`/api/llm/...`) will be subject to rate limiting using the `django-ratelimit` library. This is crucial to:
    *   Prevent abuse of potentially resource-intensive (CPU, GPU, memory) AI features by individual users or automated scripts.
    *   Manage operational costs if, in the future, pay-per-use external LLM APIs were integrated (though the current plan focuses on local LLMs).
    *   Ensure fair usage of shared LLM resources on a self-hosted instance, especially if multiple users are concurrently using AI features.
    *   The rates (e.g., number of requests per user per hour or per day for different AI features) will be configurable via Django settings / environment variables (e.g., `CC_LLM_GENERATE_TEXT_RATE_LIMIT = "100/hour"`).
5.  **Hardware Requirements Documentation for LLMs:** Clear, practical, and realistic documentation will be provided for self-hosting users regarding the minimum and recommended hardware specifications for running the LLM features effectively, especially when relying on CPU-only inference for GGUF models. This documentation will include:
    *   **CPU:** Minimum number of cores and recommended clock speed (e.g., a modern quad-core CPU with decent single-thread performance as a baseline for smaller models; more cores/faster speeds for larger models or higher throughput).
    *   **RAM:** Guidance on required system RAM, which is critical for loading LLM models into memory. This will be provided based on model size (e.g., a 7B parameter GGUF model might require 8GB+ of free RAM for the model itself plus system overhead; a 13B model might require 16GB+).
    *   **Disk Space:** Information on the disk space needed for storing downloaded GGUF model files (which can range from a few gigabytes for smaller models like 3B/7B to tens of gigabytes per model for larger ones like 30B/70B).
    *   **GPU Offloading (Optional but Recommended for Performance):** If users have a compatible GPU (typically NVIDIA GPUs with CUDA support for standard `llama.cpp` builds, or potentially AMD GPUs with ROCm support depending on the `llama.cpp` compilation), the documentation will guide them on:
        *   How to configure `llama-cpp-python` for GPU layer offloading using the `CC_LLM_N_GPU_LAYERS` setting.
        *   Typical VRAM (GPU memory) requirements for offloading a significant number of layers for different model sizes and quantization levels (e.g., "a 7B Q4_K_M model might offload many layers effectively with 4-6GB VRAM, while full offload might require 8GB+ VRAM"). This helps users match models to their available GPU hardware.
    *   Guidance will also be provided for running on popular low-power devices like newer Raspberry Pi models (e.g., Raspberry Pi 4 with 8GB RAM, Raspberry Pi 5), with clear caveats about expected performance limitations, especially for larger models or more complex tasks. This helps set realistic expectations for users with constrained hardware.

**C. Wiki Core Features:**
1.  **Page and Space Management (with Soft Deletes):**
    *   Full CRUD (Create, Read, Update, Delete) operations for pages and spaces will be available through both the backend API and the frontend user interface, governed by the permissions system.
    *   **Soft Deletes (to be fully implemented in Phase 3):** This feature provides a crucial safety net against accidental data loss for important content.
        *   When a user with appropriate permissions "deletes" a `Page` or `Space` object through the UI or API, the object will not be immediately and permanently removed from the database.
        *   Instead, its `is_deleted` boolean flag in the database model will be set to `True`, and the `deleted_at` timestamp field will be populated with the current date and time.
        *   Standard application queries that list pages in a space, display search results, or populate navigation trees will automatically exclude records where `is_deleted=True` by default. This will be handled by custom default managers on the Django models.
        *   A dedicated UI section (e.g., an "Admin Panel" accessible to superusers, or a "Trash" / "Recycle Bin" feature within space settings accessible to Space Admins) will allow users with the necessary privileges to:
            *   View a list of soft-deleted items (pages and/or spaces) within their scope of authority.
            *   Permanently delete selected soft-deleted items from the database (a "hard delete" operation, which will be clearly marked as irreversible).
            *   Restore selected soft-deleted items, which would involve setting their `is_deleted` flag back to `False` and clearing the `deleted_at` timestamp. Restored items would then reappear in normal listings and search results.
2.  **Page Version History & Diffing:**
    *   Every time a page's `raw_content` (ProseMirror JSON) is updated and saved, a new `PageVersion` record will be created in the database. This record will store a snapshot of the `raw_content` at that point in time, along with metadata such as the user who made the change (author of the version), the timestamp of the change, the new version number, and an optional commit message provided by the user describing their changes.
    *   The user interface will provide features to:
        *   View a chronological list of all historical versions for a given page. This list will typically display the version number, the author of that version, the creation timestamp, and any commit message.
        *   Allow users to select any past version from this list and view its fully rendered content as it appeared at that point in time.
        *   Enable users to revert the page's current content to that of a selected previous historical version. This "revert" action will itself create a new current `PageVersion` reflecting the state of the chosen older version, rather than overwriting the existing current version or deleting intermediate versions. This ensures that the full, unbroken history of the page is always preserved.
    *   **Diffing Between Versions:** A visual diffing capability will be implemented to help users clearly understand what changed between two selected page versions (e.g., the current version and the immediately preceding one, or any two arbitrary historical versions).
        *   **Preferred Method (Semantic ProseMirror Diff):** The implementation will prioritize using a ProseMirror-native diffing library (e.g., **`prosemirror-changeset`** or a similar stable and well-maintained tool if available for React/JavaScript). Such libraries can compute a semantic diff of the two ProseMirror JSON document structures, identifying not just text changes but also structural changes (e.g., a paragraph split into two, a list item moved, a table cell merged) and attribute changes (e.g., a heading level changed, a link URL modified). This semantic diff can then be rendered into a user-friendly visual comparison that accurately highlights these changes (e.g., using different background colors or side-by-side views with change markers).
        *   **Fallback Method (Rendered HTML Diff):** If implementing a robust ProseMirror-native semantic diff proves too complex for the MVP timeline, or if a suitable library is not readily available or mature enough, the fallback strategy will be to:
            1.  On the client-side, render the HTML output of both selected page versions (by converting their respective ProseMirror JSON `raw_content` to HTML using Tiptap/ProseMirror's rendering logic).
            2.  Use a client-side library like `diff-match-patch` (or a React wrapper around it) to compute and display the differences between these two rendered HTML strings. While less semantically precise than a ProseMirror JSON diff (as it operates on the presentation layer), this still provides a useful visual comparison for identifying textual changes and some structural modifications.
3.  **Hierarchical Page Organization:**
    *   The system will fully support parent-child relationships between `Page` objects within a `Space`. This allows users to organize content into a tree-like, hierarchical structure (e.g., a main project overview page with several sub-pages for different project phases, which in turn might have sub-pages for specific tasks or documentation).
    *   A key UI element for navigating this hierarchy will be a **navigation panel**, typically implemented as a collapsible sidebar within the application. This panel will display the page tree for the currently active `Space`. The tree view will:
        *   Show root-level pages in the space.
        *   Allow users to expand parent pages to see their direct child pages.
        *   Support multiple levels of nesting.
        *   Allow users to click on any page in the tree to navigate directly to view that page.
        *   Potentially indicate the currently viewed page within the tree.
        *   May offer context menu options on tree nodes (e.g., "Create child page," "Rename," "Move" - though move is more complex and might be post-MVP).
4.  **Search Functionality (PostgreSQL FTS with Fuzzy Matching & Snippets):**
    *   The application will provide robust full-text search capabilities, allowing users to search across page titles and the textual content extracted from the `raw_content` (ProseMirror JSON) of all pages they have permission to access.
    *   **Backend Implementation (Leveraging PostgreSQL's Advanced Features):**
        *   A `SearchVectorField` will be added to the `Page` Django model. This field will store pre-processed GIN (Generalized Inverted Index) compatible text vectors generated by PostgreSQL from the page's title and a textual representation of its `raw_content` (ProseMirror JSON needs to be converted to plain text for indexing). Django signals (e.g., `post_save`) will trigger updates to this vector field whenever a page is created or its content is modified.
        *   User search queries from the frontend will be converted into PostgreSQL `SearchQuery` objects on the backend, potentially supporting boolean operators (AND, OR, NOT) and phrase searching.
        *   Search results will be ranked by relevance using PostgreSQL's `SearchRank` function, which scores documents based on query term frequency and proximity.
        *   The **`pg_trgm` PostgreSQL extension** must be enabled in the database. This extension provides functions for determining the similarity of text based on trigram matching. It will be used to implement **fuzzy search capabilities**, allowing the search to find relevant pages even if the user's query contains minor misspellings or variations of words (e.g., finding "documentation" when a user searches for "documnetation" or "documenting"). This can be achieved using functions like `similarity()` or operators like `pg_trgm`'s `%` operator in conjunction with FTS.
        *   Search result snippets (short excerpts of page content showing the search terms in context) with highlighted matching terms will be generated on the server-side using PostgreSQL's `ts_headline` function (accessible via Django ORM's `SearchHeadline` annotation). This provides users with immediate context for why a page matched their query.
    *   **Frontend Interface for Search:**
        *   A global search bar will likely be present in the `AppHeader` for users to quickly initiate searches from anywhere in the application.
        *   A dedicated search results page (`/search?q=...`) will display a paginated list of matching pages. Each result item will typically show:
            *   The page title (as a link to the page).
            *   A context snippet with the user's search terms highlighted.
            *   Relevant metadata such as the page's author, last updated date, and the space it belongs to.
        *   Users will be able to filter search results, for example, by a specific `Space` they have access to, or by one or more `Tags`.
    *   **Caching Search Results:** Frequently executed search queries and their corresponding result sets (especially for queries that don't change often or for popular search terms) may be cached using Redis to improve performance and reduce database load for common searches, particularly in larger wikis. Cache invalidation strategies will need to be carefully considered when pages are updated or deleted to ensure search results remain fresh.
5.  **Markdown File Import/Export:**
    *   **Import `.md` File to Create New Page:**
        *   Users will be able to upload a Markdown (`.md`) file through the application's UI (e.g., via a "Import Markdown" button within a space or page creation flow).
        *   The backend API endpoint receiving this file will:
            1.  Read the Markdown content from the uploaded file.
            2.  Parse the Markdown content into an Abstract Syntax Tree (AST) or directly into ProseMirror JSON. This can be done using a robust Markdown parser like `Remark` (if a Python bridge or a suitable Python port is used to get a ProseMirror-compatible AST) or `python-markdown` (which typically outputs HTML, requiring a subsequent HTML-to-ProseMirror conversion step, similar to the Confluence import fallback).
            3.  Convert the parsed Markdown (or its AST/HTML representation) into the application's standard ProseMirror JSON format, ensuring it conforms to the `appSchema`.
            4.  Create a new `Page` object in the database with this ProseMirror JSON as its `raw_content`. The user will be able to select the `Space` (and optionally a `parent_page`) where this new page should be created. The filename of the uploaded Markdown file might be used as a default title for the new page.
    *   **Export Existing Page Content as `.md` File:**
        *   Users will be able to select an existing page within the application and choose an option to export its content as a Markdown file.
        *   The backend API endpoint for this action will:
            1.  Retrieve the page's current `raw_content` (which is in ProseMirror JSON format).
            2.  Serialize this ProseMirror JSON document into a Markdown string. This will be done using a `MarkdownSerializer` configured for the `appSchema` (e.g., from the `prosemirror-markdown` library, potentially via a server-side JavaScript execution environment if no direct Python equivalent is mature enough, or by sending the JSON to the client for Tiptap's `getMarkdown()` method if client-side export is acceptable, though backend is generally preferred for consistency).
            3.  Offer this generated Markdown string for download as a `.md` file to the user, with a filename typically derived from the page title.
6.  **Tagging/Labeling System:**
    *   Users will be able to add multiple tags (also known as labels) to `Page` objects. This provides a flexible way to categorize content, improve discoverability, and enable faceted search.
    *   **Tag Management:**
        *   Tags will likely be managed as global entities within the application (i.e., a tag named "release-notes" is the same tag object regardless of which space or page it's applied to). However, their application (association) is per-page. Consideration for space-specific tag vocabularies could be a post-MVP feature if there's a strong need for it.
        *   The UI will allow users with appropriate permissions to:
            *   Add existing tags to a page (e.g., via an autocomplete input field when editing page metadata).
            *   Create new tags if they don't already exist (permissions for global tag creation might be restricted to certain user roles like administrators or space administrators, or it could be open to all editors).
            *   Remove tags from a page.
    *   **Discovery via Tags:**
        *   The UI will allow users to browse or list all pages associated with a specific tag.
        *   Tags will be usable as filters in the main search results page (e.g., "search for 'keyword' in pages tagged 'urgent' and 'project-alpha'").
    *   **API Support:** The backend API will provide endpoints for:
        *   CRUD operations for `Tag` objects themselves (e.g., list all tags, create a new tag).
        *   Managing the many-to-many relationship between `Page` and `Tag` objects (e.g., add a tag to a page, remove a tag from a page, list tags for a page).
7.  **Zero-Trust Attachment Downloads (Security Measure):**
    *   When a user requests to download an attachment file through the application (e.g., by clicking a download link for a PDF or a ZIP file attached to a page), the backend API endpoint responsible for serving the file will set specific HTTP headers. These headers are designed to instruct the browser to treat the file strictly as a download and to mitigate security risks associated with browsers potentially misinterpreting or attempting to render user-uploaded files that might have incorrect or maliciously crafted MIME types.
    *   The key headers to be set are:
        *   **`Content-Type: application/octet-stream`**: This generic MIME type tells the browser that the content is an arbitrary binary data stream. This usually prompts the browser to initiate a "Save As" dialog for the user, rather than attempting to render the file inline (e.g., display a PDF in a browser plugin, or execute HTML/JavaScript if the file was misidentified as such).
        *   **`Content-Disposition: attachment; filename="sanitized_user_filename.ext"`**: This header explicitly tells the browser that the content is an attachment that should be downloaded, rather than displayed inline. It also suggests a filename for the download. The `sanitized_user_filename.ext` part will be derived from the original filename provided by the uploader but will be sanitized by the backend to remove any potentially harmful characters (e.g., path traversal sequences like `../`, shell metacharacters, or characters that could cause issues with filesystems or HTTP headers) before being included in the header.
        *   **`X-Content-Type-Options: nosniff`**: This header (primarily respected by Internet Explorer and Chrome, but good practice for all) prevents the browser from trying to MIME-sniff the content type away from the one declared by the server (`application/octet-stream` in this case). MIME sniffing can be a security risk if a malicious file (e.g., an HTML file containing JavaScript) is uploaded with a benign-looking filename and MIME type, and the browser then "corrects" the MIME type and executes the malicious content.
8.  **Asynchronous Attachment Virus Scanning (Using Celery & ClamAV):**
    *   To prevent the distribution of malware (viruses, trojans, etc.) through uploaded attachments, an asynchronous virus scanning pipeline will be implemented. This ensures that scanning does not block the user's upload request.
    1.  **Upload & Initial State:** When a user uploads an attachment file, it is initially saved to the configured storage backend (`django-storages` - either local filesystem or S3/MinIO). The corresponding `Attachment` database record is created with its `scan_status` field set to `'pending'`.
    2.  **Asynchronous Task Trigger (Celery):** After the file is successfully saved, a **Celery background task** is enqueued. This task is responsible for performing the virus scan. The task payload will include information needed to locate the attachment file (e.g., its primary key or storage path).
    3.  **Virus Scanning Process (ClamAV):** The Celery worker picks up the task and interfaces with an antivirus engine. **ClamAV** is a common open-source choice for this.
        *   ClamAV would typically be configured to run as a daemon (`clamd`) in a separate Docker container (for isolation and easy updates of virus definitions) or installed locally on the server where Celery workers run.
        *   The Celery task communicates with `clamd` (e.g., via its TCP/IP socket or a UNIX domain socket, using a Python library like `python-clamd`) to submit the file for scanning. Alternatively, the task could invoke the `clamdscan` or `clamscan` command-line utilities as a subprocess, passing the file path.
    4.  **Status Update:** Based on the scan result returned by ClamAV:
        *   If the file is clean, the `Attachment.scan_status` field is updated to `'clean'`.
        *   If the file is found to be infected, the `scan_status` is updated to `'infected'`. Additional actions might be taken, such as moving the infected file to a quarantine area or logging a security alert.
        *   If the scan itself fails due to an error (e.g., ClamAV daemon unavailable, file too large for ClamAV to handle), the `scan_status` is updated to `'error'`.
        *   The `scanned_at` timestamp field in the `Attachment` model is also updated upon completion of the scan attempt.
    5.  **Configuration Option for Skipping Scans:** For self-hosting environments where setting up and maintaining ClamAV (or another AV solution) might be difficult or undesirable, a configuration option (e.g., `CC_ENABLE_VIRUS_SCANNING=False` in the `.env` file) will allow administrators to disable the virus scanning feature. If disabled, new attachments might have their `scan_status` set to `'skipped'`.
    6.  **Access Control Based on Scan Status:**
        *   Attachments with `scan_status='infected'` will be effectively quarantined: users (including the uploader) will be prevented from downloading or accessing them through the application. The UI might show an "Infected File - Cannot Download" warning.
        *   Attachments with `scan_status='pending'` or `'error'` might either be blocked from download (stricter security) or allowed with a clear warning to the user that the file has not been verified as safe (more permissive, depends on desired security posture). For MVP, blocking pending/error might be safer.
        *   Only attachments with `scan_status='clean'` (or `'skipped'` if virus scanning is disabled via configuration) will be freely downloadable by users with appropriate permissions to the page.
        *   Administrators should be notified (e.g., via email notifications sent by a Celery task, or through a dedicated section in an admin dashboard) of any detected infected files.
9.  **[Post-MVP Consideration] Content Previews (Link Unfurling):**
    *   As a post-MVP enhancement to improve the user experience when users paste external links into the Tiptap editor, functionality for "link unfurling" (also known as generating rich link previews) could be implemented.
    *   **Mechanism:** When a user pastes a URL into the editor, an event would trigger a request (likely to a backend API endpoint to avoid client-side CORS (Cross-Origin Resource Sharing) issues and to manage any necessary API keys for metadata services).
    *   This backend service would then fetch metadata from the provided URL. This metadata typically includes:
        *   The page title (from the `<title>` tag).
        *   A short description (e.g., from the `meta name="description"` tag or Open Graph `og:description`).
        *   A preview image or thumbnail (e.g., from Open Graph `og:image` or Twitter Card `twitter:image`).
        *   The site name (e.g., from `og:site_name`).
    *   The backend service would need to handle:
        *   Making HTTP requests to the external URL (with appropriate timeouts and error handling).
        *   Parsing the HTML content of the fetched page.
        *   Extracting the relevant metadata tags (Open Graph, Twitter Cards, standard meta tags).
        *   Potentially fetching, resizing, and caching preview images to optimize performance and avoid hotlinking.
    *   **Tools/Libraries:** This could be implemented using Python libraries for HTTP requests (`requests` or `httpx`) and HTML parsing (`BeautifulSoup4`). Alternatively, for more comprehensive support across a wide variety of websites and for handling OEmbed/o gewnscht (Open Embedded) providers, the backend might integrate with a dedicated link unfurling service or library like **Iframely** (which can be self-hosted or used as a cloud service) or a simpler OEmbed consumer library.
    *   **Editor Integration:** Once the metadata is fetched, the Tiptap editor would replace the plain text link with a visually rich preview card displaying the title, description, and thumbnail image. This card would still function as a hyperlink to the original URL.
    *   **Security Considerations:**
        *   **Server-Side Request Forgery (SSRF):** The backend service making requests to external URLs must be hardened against SSRF attacks (e.g., by validating URLs, restricting protocols, using a proxy with an allowlist of TLDs, running the fetcher in an isolated environment).
        *   **XSS from Fetched Metadata:** Titles, descriptions, and other text fetched from external sites must be properly sanitized before being rendered in the preview card to prevent XSS if the external site contains malicious content in its meta tags.

**D. [Out of Scope for MVP] Real-Time Collaboration:**
*   Features enabling multiple users to edit the same page simultaneously in real-time (similar to the collaborative editing experience in Google Docs or modern versions of Confluence) are **explicitly out of scope for the MVP and initial post-MVP releases.**
*   Implementing real-time collaboration reliably is a highly complex architectural challenge that typically requires a fundamentally different approach to content management and synchronization than what is planned for the single-user editing model of the MVP. Key components and complexities include:
    *   **Conflict Resolution Algorithms:** Mechanisms to merge concurrent changes made by multiple users without data loss or corruption. Common approaches include:
        *   **Operational Transformation (OT):** A traditional algorithm for collaborative editing where operations (changes) are transformed against concurrent operations to maintain consistency. ProseMirror itself has some foundational support for collaborative editing based on OT.
        *   **Conflict-free Replicated Data Types (CRDTs):** A more modern approach where data structures are designed to be inherently mergeable without conflicts. Libraries like Yjs (CRDT-based) or Automerge (CRDT-based) are popular choices for building collaborative applications.
    *   **Real-Time Backend Component:** A dedicated backend service, typically using WebSockets (e.g., implemented with Django Channels, or a specialized Node.js server using libraries like `socket.io` or `ws`), to broadcast changes made by one client to all other connected clients editing the same document in real-time. This service would also handle an authoritative version of the document or coordinate the merging of changes.
    *   **Presence Indication:** UI elements to show which other users are currently viewing or actively editing the same page (e.g., displaying their avatars or cursors).
    *   **Editor State Management:** Significant changes to the Tiptap editor's state management would be needed to integrate with an OT or CRDT library and to handle incoming changes from other users while preserving local edits.
*   If real-time collaboration is identified as a critical feature in the future, it would need to be planned as a separate, major project, likely involving a significant re-architecture of parts of the application. The current MVP focuses on providing a robust single-user editing experience and a solid foundation for content management.

---

**VI. Security Implementation Details**

**A. Content Security Policy (CSP) with Nonces:**
*   A robust Content Security Policy (CSP) will be implemented via HTTP headers. This is a critical defense-in-depth measure to mitigate Cross-Site Scripting (XSS) attacks and other content injection vulnerabilities by restricting the sources from which content (scripts, styles, images, fonts, frames, etc.) can be loaded and executed by the browser.
*   **Nonce-Based Approach for Inline Resources:** To allow necessary inline scripts (e.g., for React hydration, initial state injection from server to client) and inline styles (if any are unavoidable) without resorting to the less secure `'unsafe-inline'` keyword (which broadly permits *all* inline scripts/styles, including injected ones), a **nonce-based CSP strategy** will be employed.
    1.  **Nonce Generation (Backend):** For each HTTP request that renders an HTML page (typically the main `index.html` for the React SPA), the backend Django application (e.g., via a custom middleware or directly in the view serving the SPA shell) will generate a unique, cryptographically random nonce (a single-use token, e.g., a base64 encoded string of 16+ random bytes).
    2.  **Nonce Injection (Backend to Frontend Template):** This generated nonce will be passed to the HTML template and dynamically inserted as an attribute into all legitimate inline `<script>` tags (e.g., `<script nonce="${request_specific_nonce}">/* initial state or bootstrap code */</script>`) and any unavoidable inline `<style>` tags (`<style nonce="${request_specific_nonce}">/* critical inline styles */</style>`) that are rendered in the server-side HTML template.
    3.  **CSP Header Configuration (Backend):** The CSP header sent with the HTTP response from the server will include this same nonce value in the relevant directives, typically `script-src` and `style-src` (e.g., `script-src 'self' 'nonce-YOUR_GENERATED_NONCE_PER_REQUEST' https://trusted.cdn.com; style-src 'self' 'nonce-YOUR_GENERATED_NONCE_PER_REQUEST' https://fonts.googleapis.com;`).
    This ensures that only inline scripts and styles that have the correct, matching nonce (i.e., those intentionally placed by the server for that specific request) will be processed and executed by the browser. Any malicious inline scripts injected through XSS without the correct nonce will be blocked by the browser.
*   **Example CSP Directives (These will be refined during development and thoroughly tested to ensure functionality while maintaining maximum security. The exact directives will depend on specific third-party services used, e.g., Sentry, analytics if any):**
    *   `default-src 'self';` (Restricts most content types to be loaded only from the application's own origin by default).
    *   `script-src 'self' 'nonce-PER_REQUEST_NONCE' https://embed.diagrams.net https://*.sentry.io;` (Allows scripts from the application's own origin, inline scripts possessing the correct nonce for the current request, scripts from the `embed.diagrams.net` origin for the Draw.io editor, and scripts from Sentry for error reporting).
    *   `style-src 'self' 'nonce-PER_REQUEST_NONCE' https://fonts.googleapis.com;` (Allows stylesheets from self, inline styles with the correct nonce, and stylesheets from Google Fonts if used).
    *   `font-src 'self' https://fonts.gstatic.com;` (Allows fonts to be loaded from self and Google Fonts' CDN).
    *   `img-src 'self' data: https: blob:;` (Allows images from self, `data:` URIs for embedded images (e.g., from Tiptap image uploads or local previews), any HTTPS source, and `blob:` URIs which might be used by frontend libraries for client-side file previews).
    *   `frame-src 'self' https://embed.diagrams.net;` (Restricts where iframes can be embedded from, allowing only self and the Draw.io editor iframe).
    *   `connect-src 'self' https://*.sentry.io ${CC_OLLAMA_API_BASE_URL_IF_DIFFERENT_ORIGIN};` (Allows XHR/fetch/WebSocket connections to self, Sentry for error reporting, and the configured LLM API endpoint if it's hosted on a different origin than the main application).
    *   `object-src 'none';` (Disallows the use of `<object>`, `<embed>`, and `<applet>` tags to prevent vulnerabilities associated with legacy browser plugins).
    *   `base-uri 'self';` (Restricts the URLs which can be used in a document's `<base>` element, mitigating certain types of XSS attacks that rely on manipulating the base URI).
    *   `form-action 'self';` (Restricts the URLs to which forms can submit data, helping to prevent data exfiltration via malicious forms).
    *   `frame-ancestors 'none';` (Prevents the application from being embedded in iframes on other sites, which is a strong defense against clickjacking attacks).
    *   `report-uri /api/csp-reports/;` (Optional but recommended: An API endpoint on the application server to which browsers can send reports of CSP violations. This is very useful for monitoring the effectiveness of the CSP, identifying unexpected blocked resources during development or after deployment, and detecting potential attacks. Requires a backend handler to receive and process these JSON reports).
*   **Draw.io Iframe Sandboxing:** The `<iframe>` used to embed the `draw.io` editor will additionally use the `sandbox` attribute for further isolation of the external editor content from the main application page. The exact sandbox directives will be chosen to provide the minimum necessary permissions for the draw.io editor to function while restricting its capabilities as much as possible (e.g., `sandbox="allow-forms allow-scripts allow-same-origin allow-popups allow-downloads"` - these permissions will be reviewed carefully for minimality).

**B. Input Validation & LLM Prompt Injection Defense:**
*   **API Input Validation (DRF Serializers & Pydantic):** All data received by API endpoints (request bodies, query parameters, path parameters) will be rigorously validated on the backend using Django REST Framework serializers. These serializers will enforce:
    *   Correct data types (string, integer, boolean, object, array, date-time, etc.).
    *   Presence of required fields.
    *   Maximum lengths for strings and maximum values/sizes for numbers or collections.
    *   Format constraints (e.g., for email addresses, UUIDs, date-time strings matching ISO 8601).
    *   Valid choices for fields that act like enumerations (e.g., `Attachment.scan_status`).
    *   For more complex nested data structures or for validation logic that needs to be shared across different parts of the application (e.g., validating configuration objects for LLM interactions), Pydantic models may be used within DRF serializer validation methods or in service layers to define and enforce these complex schemas.
*   **LLM Prompt Input Sanitization (Defense against Prompt Injection):** User-provided free-text inputs that are incorporated into LLM prompts (e.g., `context_text` provided by the user for summarization, user queries for text generation, selected code for explanation) will undergo several layers of sanitization on the backend before being passed to the LLM. This is a critical defense against prompt injection attacks where users might try to trick the LLM into ignoring its original instructions or performing unauthorized actions. The sanitization process will include:
    1.  **Strict Length Truncation:** Enforce maximum length limits (configurable via settings like `CC_LLM_MAX_CONTEXT_LENGTH` but also potentially per-feature limits based on typical input sizes and the specific LLM's token limits) to ensure inputs fit within the LLM's context window and to prevent denial-of-service attacks or excessive resource consumption by overly long prompts. Strategies like taking the first N characters/tokens and the last M characters/tokens might be used to preserve leading/trailing context from a larger user input while managing overall length.
    2.  **Control Character Stripping/Escaping:** Removal or proper escaping of non-printable ASCII control characters (e.g., null bytes, bell characters, vertical tabs) that could disrupt LLM processing, cause rendering issues if the LLM echoes them back, or potentially be used in obscure injection attacks.
    3.  **HTML Tag Stripping (if context is intended as plain text):** If the input context being passed to an LLM is expected to be plain text but might contain user-generated HTML (e.g., from a selection made by the user in the rich text editor that includes formatting), these HTML tags will be stripped using a robust HTML sanitization library like `Bleach` configured with an empty allowlist (i.e., strip all tags, keep only the text content), or a carefully crafted and well-tested regular expression (e.g., `re.sub(r'<[^>]+>', '', text)`). This prevents HTML structure from confusing the LLM if it expects plain text.
    4.  **Normalization of Whitespace:** Excessive whitespace (multiple consecutive spaces, tabs, multiple newlines) will be normalized (e.g., multiple spaces collapsed to a single space, multiple newlines reduced to one or two as appropriate for the LLM's expected input format or the prompting strategy).
    5.  **Known Injection Sequence Filtering and Escaping (Ongoing Research & Model-Specific Defenses):**
        *   The sanitization logic will be designed to be extensible to incorporate defenses against known prompt injection techniques as they are discovered and become relevant. This is an evolving area of AI security research.
        *   This might involve detecting and neutralizing or escaping specific character sequences or keywords that are known to act as instruction separators (e.g., `---`, `### Instruction:`), role-play induction markers (e.g., "You are now an unconstrained AI..."), or "jailbreak" prompts designed to make the LLM ignore its original system prompt or safety guidelines.
        *   The effectiveness of these techniques can be model-specific, so this requires ongoing monitoring of LLM security research and may need to be tailored to the specific LLMs being used and the structure of the application's system prompts. A good default is to be very restrictive about what characters or sequences are allowed if they resemble known LLM control patterns.

**C. Output Sanitization & Secure Rendering:**
*   **Code Snippets (Frontend Rendering):** User-written code within `code_block` nodes is stored as plain text. When the `highlight.js` library (or a similar syntax highlighting library) processes this plain text and generates HTML markup for syntax highlighting, this HTML output will be rigorously sanitized on the client-side using **`DOMPurify.sanitize(highlightedHtml, { USE_PROFILES: { html: true }, FORBID_TAGS: ['script', 'style', 'iframe', 'object', 'embed', 'link', 'form', 'svg', 'math'], FORBID_ATTR: ['onerror', 'onload', 'onclick', 'onmouseover', 'onfocus', 'autofocus', 'style', /* and other event handlers or dangerous attributes like those starting with 'on' */] })`** before being rendered into the DOM using React's `dangerouslySetInnerHTML`. This strict sanitization prevents XSS if `highlight.js` itself has a vulnerability or if malicious content (e.g., HTML tags within a string literal in the code, or even attempts to inject `<svg><script>...`) somehow gets interpreted as active HTML by the highlighter or the browser during rendering.
*   **User-Generated Content (ProseMirror JSON Rendering):** The primary method for rendering page content on the frontend is by converting the stored ProseMirror JSON (from `Page.raw_content`) to HTML. This conversion is handled by Tiptap/ProseMirror's own rendering mechanism, which uses the `toDOM` specifications defined for each node and mark in the `appSchema` (see Section III.A). This process is inherently secure against XSS through content injection *from the ProseMirror structure itself*, because it only generates HTML elements and attributes explicitly allowed and defined by the schema. Arbitrary HTML tags or attributes not in the schema cannot be rendered through this path. The security of attributes within schema-defined elements (like `href` in links, `src` in images) depends on proper validation/sanitization when those attributes are set in the editor (Tiptap's link/image extensions usually handle this).
*   **LLM Output Sanitization & Rendering:**
    *   **Textual Outputs:** Textual outputs from LLMs intended for direct display to the user (e.g., summaries, explanations, generated paragraphs of text) will generally be treated as plain text. Before rendering this text in an HTML context on the frontend, any characters that have special meaning in HTML (such as `<`, `>`, `&`, `"`, `'`) will be properly HTML-escaped (e.g., using a utility function similar to Python's `html.escape()` or relying on React's default JSX escaping behavior for text content) to prevent XSS vulnerabilities if the LLM output accidentally contains HTML-like structures or malicious script tags.
    *   **Specific Formats (Mermaid, Markdown):**
        *   **Mermaid Syntax:** LLM-generated Mermaid syntax will first be validated (as per V.B.2). If valid, the raw syntax string is passed to the client-side `Mermaid.js` library for rendering. `Mermaid.js` itself should be kept updated and configured securely (e.g., `securityLevel: 'strict'` or higher if available, ensuring it doesn't evaluate arbitrary JavaScript embedded within Mermaid comments or labels).
        *   **Markdown:** If an LLM generates Markdown, this Markdown will be rendered to HTML using a robust Markdown-to-HTML converter (e.g., `markdown-it` or `Remark` with `rehype-react`). This converter **must** be configured with a strong HTML sanitization step (e.g., `markdown-it-sanitizer`, or by piping its HTML output through `DOMPurify` with an appropriate allowlist for common Markdown-generated tags like `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>`, `<li>`, `<a>` (with `href` validation), `<img>` (with `src` validation), `<code>`, `<pre>`, `<blockquote>`, `<table>`, `<thead>`, `<tbody>`, `<tr>`, `<th>`, `<td>`, etc., and disallowing `<script>`, `<iframe>`, event handlers, etc.).
    *   **Controlled HTML Snippets (Sandboxed iFrames):** In very specific, controlled scenarios where an LLM might be guided by highly constrained prompts and post-processing to generate extremely simple and predictable HTML snippets (e.g., for creating custom formatted information boxes with only `<p>`, `<strong>`, `<em>` tags, or simple unordered lists, where the overall structure and allowed elements are predefined by application logic), this HTML output will be rendered within a sandboxed `<iframe>` on the frontend using the `srcdoc` attribute.
        *   The `sandbox` attribute of the iframe will be configured as restrictively as possible (e.g., `sandbox="allow-same-origin"` if the content needs no scripts or forms to function within its sandboxed context; more permissive directives like `allow-scripts` or `allow-forms` would only be added if absolutely essential for the intended display and the risks are fully understood and mitigated, but `allow-top-navigation` or `allow-popups-to-escape-sandbox` should generally be avoided).
        *   The HTML content provided to `srcdoc` must either be:
            *   Generated by trusted backend logic that tightly constrains the LLM's output to a very small, known-safe subset of HTML tags and attributes, possibly by using an LLM to fill in only textual parts of a predefined HTML template controlled by the application.
            *   Or, if based on more free-form LLM output (which is generally discouraged for direct HTML rendering), be heavily and rigorously sanitized using `DOMPurify` with an extremely strict allowlist before being assigned to `srcdoc`.
        *   Rendering arbitrary LLM-generated HTML/JavaScript directly in the main page context (outside a secure sandbox) is strictly out of scope due to significant and unacceptable security risks.

**D. API Rate Limiting:**
*   The `django-ratelimit` library will be configured to apply rate limits to sensitive, resource-intensive, or potentially abusable API endpoints. This helps protect against denial-of-service (DoS) attacks, brute-force attempts on authentication endpoints, and excessive resource consumption by legitimate or malicious clients.
*   **Configuration:** Rate limits will be defined in Django settings (e.g., in `settings.py` or a dedicated configuration file) and can be applied globally to all API views, or on a per-view/per-viewset basis for more granular control. Rates will be configurable via environment variables to allow administrators to tune them for their specific deployment needs and expected load.
*   **Scoped Rates:** Rate limits can be scoped based on various criteria:
    *   Per IP address (typically for unauthenticated endpoints or as a general fallback).
    *   Per authenticated user (for endpoints requiring login).
    *   A combination (e.g., a global limit per IP, and a stricter limit per user once authenticated).
*   **Example Rate Limit Configurations (these are illustrative and will be tuned):**
    *   Authentication endpoints (`/api/auth/login/`, `/api/auth/register/`): e.g., 5-10 requests per minute per IP address to prevent brute-force attacks on user accounts.
    *   Password reset request endpoint (`/api/auth/password/reset/`): e.g., 3-5 requests per hour per IP address or per email address to prevent abuse.
    *   LLM interaction endpoints (`/api/llm/...`): e.g., 20-30 requests per hour per authenticated user (overall for all LLM features combined), with potentially stricter individual limits on very computationally expensive LLM tasks if identified.
    *   Page creation/update endpoints (`POST /api/pages/`, `PUT /api/pages/{id}/`): e.g., 30-60 requests per minute per authenticated user to prevent rapid, high-volume content changes that could strain the system.
    *   Confluence import initiation endpoint (`POST /api/io/import/confluence/`): e.g., 1-2 import attempts per hour per authenticated user to prevent overloading the import processing queue.
*   **Response on Rate Limit Exceeded:** When a client exceeds a configured rate limit, the API will return an HTTP 429 "Too Many Requests" response. This response should ideally include a `Retry-After` header indicating to the client (in seconds, or as an HTTP date) when they can try making the request again.

**E. Other Key Security Measures:**
*   **Regular Dependency Updates & Vulnerability Management:** A proactive process will be established for regularly scanning and updating all backend (Python/Django and their dependencies using tools like `pip-audit`, `safety`, or integrated Snyk/Dependabot checks in CI) and frontend (NPM/Yarn packages using `npm audit`, `yarn audit`, or Snyk/Dependabot) dependencies. This is crucial for patching known vulnerabilities in third-party libraries. This should be part of a routine maintenance schedule (e.g., weekly or bi-weekly scans, monthly update cycles for non-critical patches).
*   **HTTPS Enforcement in Production:** HTTPS will be strictly enforced for all communication with the application in production environments. This is typically configured at the reverse proxy level (e.g., Nginx will handle SSL/TLS termination, obtain/renew certificates via Let's Encrypt or other CAs, and redirect all HTTP requests to HTTPS). HTTP Strict Transport Security (HSTS) headers (`Strict-Transport-Security: max-age=31536000; includeSubDomains; preload`) will be used to instruct browsers to only communicate with the server over HTTPS for a long duration, protecting against protocol downgrade attacks and cookie hijacking.
*   **Standard Django Security Middleware:** All relevant standard Django security middleware components will be enabled and correctly configured in `settings.py` to provide baseline web application security:
    *   `django.middleware.security.SecurityMiddleware`: For HSTS, secure session cookies (if Django sessions are used directly), X-Content-Type-Options: nosniff header, X-XSS-Protection header (though CSP is a stronger and more modern defense against XSS), Referrer-Policy header.
    *   `django.contrib.sessions.middleware.SessionMiddleware`: If using Django's database or cache-backed sessions. Session cookies will be configured with `SESSION_COOKIE_SECURE = True` (ensures cookies are only sent over HTTPS in production), `SESSION_COOKIE_HTTPONLY = True` (prevents client-side JavaScript from accessing session cookies), and `SESSION_COOKIE_SAMESITE = 'Lax'` or `'Strict'` (mitigates CSRF by controlling when cookies are sent with cross-site requests).
    *   `django.middleware.csrf.CsrfViewMiddleware`: For Cross-Site Request Forgery (CSRF) protection. This is crucial for any views that modify data and are accessed via traditional web forms or state-changing GET requests (though less critical for DRF APIs using token authentication if sessions are not used for API auth). Ensure AJAX requests from the frontend include the CSRF token if needed.
    *   `django.middleware.clickjacking.XFrameOptionsMiddleware`: To prevent clickjacking attacks by setting the `X-Frame-Options` header (e.g., to `DENY` or `SAMEORIGIN`). This complements the `frame-ancestors` directive in CSP, providing defense for older browsers that might not fully support CSP Level 3.
*   **Secret Management:** All sensitive credentials, API keys, and configuration secrets (Django `SECRET_KEY`, database passwords, Redis passwords, S3 access keys and secret keys, Sentry DSN, LLM API keys if any external LLMs were ever used) will be managed exclusively via **environment variables**. They will **never be hardcoded** into the codebase or committed to version control. A `.env.example` file will list all required environment variables with placeholders. In local development, `python-dotenv` will load these from a `.env` file (which is in `.gitignore`). In production, environment variables will be set securely via the deployment platform (e.g., Docker Compose environment files, Kubernetes Secrets, PaaS configuration variables).
*   **Object-Level Permissions (`django-guardian`):** As detailed in Section I.C, `django-guardian` will be used to enforce fine-grained permissions, ensuring that users can only access, view, edit, or delete the specific resources (spaces, pages, attachments) for which they have been granted explicit authorization through defined roles. This is a cornerstone of data security and segregation within the application, preventing unauthorized data access.
*   **Secure JWT Handling (Rotation & Blacklisting):** As detailed in Section I.C, `djangorestframework-simplejwt` will be configured for refresh token rotation (issuing a new refresh token and invalidating the old one upon use) and blacklisting of used refresh tokens. This enhances the security of JWT-based authentication by reducing the window of opportunity for compromised refresh tokens to be exploited. Access tokens will have short expiry times (e.g., 15 minutes) to limit their validity if intercepted.

---

**VII. Error Handling & Logging Strategy**

**A. Standardized API Error Responses with Documentation URLs & Enum Error Codes:**
*   A custom Django REST Framework exception handler will be implemented. This global handler will catch all exceptions raised within API views  both standard DRF exceptions (like `serializers.ValidationError`, `NotAuthenticated`, `PermissionDenied`, `NotFound`) and custom application-specific exceptions that inherit from a defined `APIBaseException`  and transform them into a consistent, machine-readable JSON error response format. This ensures that clients (like the frontend application) can reliably parse and handle errors.
*   A base Python exception class, `APIBaseException`, will be defined in a shared backend module (e.g., `core/exceptions.py`). More specific application exceptions (e.g., `ResourceNotFoundAPIException`, `ValidationAPIException`, `PermissionDeniedAPIException`, `LLMServiceUnavailableAPIException`, `ImportTaskFailedAPIException`, `DiagramSyntaxAPIException`) will inherit from this base class, allowing them to set specific default status codes, error codes, and messages.
    ```python
    # Located in a shared backend 'core/exceptions.py' module
    import enum
    from rest_framework import status # For standard HTTP status codes

    class ErrorCode(str, enum.Enum): # Using str enum for easy serialization and readability
        # General Errors (E0000 - E0099)
        INTERNAL_SERVER_ERROR = "E0001_INTERNAL_SERVER_ERROR"
        UNKNOWN_ERROR = "E0002_UNKNOWN_ERROR"
        SERVICE_UNAVAILABLE = "E0003_SERVICE_UNAVAILABLE" # For general service issues
        # Authentication Errors (E0100 - E0199)
        AUTH_CREDENTIALS_REQUIRED = "E0100_CREDENTIALS_REQUIRED"
        AUTH_FAILED_INVALID_CREDENTIALS = "E0101_INVALID_CREDENTIALS"
        AUTH_INVALID_TOKEN = "E0102_INVALID_TOKEN"
        AUTH_TOKEN_EXPIRED = "E0103_TOKEN_EXPIRED"
        AUTH_USER_INACTIVE = "E0104_USER_INACTIVE"
        AUTH_USER_NOT_FOUND = "E0105_USER_NOT_FOUND"
        # Authorization Errors (E0200 - E0299)
        PERMISSION_DENIED = "E0201_PERMISSION_DENIED"
        # Validation & Input Errors (E0300 - E0399)
        VALIDATION_ERROR = "E0301_VALIDATION_ERROR" # Generic, details will contain field errors
        INVALID_REQUEST_PAYLOAD = "E0302_INVALID_REQUEST_PAYLOAD"
        MISSING_REQUIRED_FIELD = "E0303_MISSING_REQUIRED_FIELD"
        INVALID_QUERY_PARAMETER = "E0304_INVALID_QUERY_PARAMETER"
        # Resource Errors (E0400 - E0499)
        RESOURCE_NOT_FOUND = "E0404_RESOURCE_NOT_FOUND" # Generic not found
        PAGE_NOT_FOUND = "E0404_PAGE_NOT_FOUND" # More specific example
        SPACE_NOT_FOUND = "E0404_SPACE_NOT_FOUND"
        ATTACHMENT_NOT_FOUND = "E0404_ATTACHMENT_NOT_FOUND"
        # Rate Limiting Errors (E0429)
        RATE_LIMIT_EXCEEDED = "E0429_RATE_LIMIT_EXCEEDED"
        # LLM Service & AI Errors (E0500 - E0599)
        LLM_SERVICE_UNAVAILABLE = "E0501_LLM_SERVICE_UNAVAILABLE" # e.g., llama.cpp server down or Ollama unreachable
        LLM_REQUEST_FAILED = "E0502_LLM_REQUEST_FAILED" # Generic failure during LLM interaction
        LLM_OUTPUT_VALIDATION_FAILED = "E0503_LLM_OUTPUT_VALIDATION_FAILED" # LLM output bad format
        LLM_MODEL_NOT_FOUND = "E0504_LLM_MODEL_NOT_FOUND" # Specified model alias/path invalid
        LLM_CIRCUIT_BREAKER_OPEN = "E0505_LLM_CIRCUIT_BREAKER_OPEN"
        # Import/Export Errors (E0600 - E0699)
        IMPORT_TASK_FAILED = "E0601_IMPORT_TASK_FAILED"
        IMPORT_FILE_INVALID = "E0602_IMPORT_FILE_INVALID" # e.g., bad ZIP, wrong format
        # Diagram & Content Errors (E0700 - E0799)
        DIAGRAM_VALIDATION_FAILED = "E0701_DIAGRAM_VALIDATION_FAILED" # e.g., invalid Mermaid syntax
        PROSEMIRROR_SCHEMA_VALIDATION_FAILED = "E0702_PROSEMIRROR_SCHEMA_VALIDATION_FAILED"
        # Attachment Errors (E0800 - E0899)
        ATTACHMENT_SCAN_FAILED = "E0801_ATTACHMENT_SCAN_FAILED"
        ATTACHMENT_INFECTED = "E0802_ATTACHMENT_INFECTED"
        # Add more specific error codes as needed for different scenarios.

    class APIBaseException(Exception):
        status_code: int = status.HTTP_500_INTERNAL_SERVER_ERROR
        error_code: ErrorCode = ErrorCode.INTERNAL_SERVER_ERROR
        message: str = "An unexpected internal server error occurred."
        doc_url_base: str = "https://docs.your-confluence-clone.com/errors" # Placeholder base URL for error documentation

        def __init__(self, message: str = None, error_code: ErrorCode = None, status_code: int = None, details: dict = None, doc_url_fragment: str = None):
            super().__init__(message or self.message) # Set exception message for logging
            if message is not None: self.message = message # Override default message if provided
            if error_code is not None: self.error_code = error_code
            if status_code is not None: self.status_code = status_code
            self.details = details or {} # For field-specific validation errors or additional context

            # Construct doc_url using the error_code's value
            effective_error_code_value = self.error_code.value if isinstance(self.error_code, ErrorCode) else str(self.error_code)
            self.doc_url_fragment = doc_url_fragment or f"#{effective_error_code_value}"

        def to_response_dict(self) -> dict:
            response_data = {
                "error_code": self.error_code.value if isinstance(self.error_code, ErrorCode) else str(self.error_code),
                "message": self.message,
            }
            if self.details: # Only include details if present
                response_data["details"] = self.details
            if self.doc_url_base and self.doc_url_fragment: # Only include doc_url if base and fragment are present
                response_data["doc_url"] = f"{self.doc_url_base}{self.doc_url_fragment}"
            return response_data
    ```
*   The JSON structure for error responses returned by the API will be consistently:
    `{ "error_code": "E0XXX_SPECIFIC_ERROR_NAME", "message": "User-friendly (but also informative for developers) error message.", "details": { /* Optional: A dictionary containing field-specific validation errors (e.g., from DRF serializers: {"fieldName": ["Error message 1 for this field.", "Error message 2 for this field."]}) or other machine-readable contextual information about the error. */ }, "doc_url": "https://your-docs-site.com/errors#E0XXX_SPECIFIC_ERROR_NAME" /* Optional: A direct link to a page in the application's documentation that explains this specific error code, its common causes, and potential solutions or troubleshooting steps for the user or administrator. */ }`
*   A central registry or enumeration (like the `ErrorCode` Python Enum example above) of all application-specific `error_code` values will be maintained within the backend codebase. This list, along with detailed explanations for each code, common causes, and recommended actions, will be part of the project's developer documentation and potentially exposed in a user-accessible knowledge base linked via the `doc_url`.
*   The frontend React application will implement a **global error handling strategy**:
    *   **React Error Boundaries:** These components will be used to wrap major sections of the UI (or the entire application) to catch JavaScript rendering errors that occur within their component subtree. When an error is caught, the Error Boundary can display a graceful fallback UI (e.g., "Oops, something went wrong. Please try refreshing the page.") instead of a blank page or a crashed application. The error can be logged to Sentry from the Error Boundary.
    *   **API Call Error Handling (Interceptors):** Interceptors for the HTTP client library (e.g., Axios interceptors or a custom wrapper around `fetch`) will be used to globally handle HTTP error responses from the backend API. These interceptors will:
        *   Parse the standardized JSON error response from the backend.
        *   Log relevant information about the error (e.g., to the console in development, or to Sentry).
        *   Trigger user-friendly error messages or notifications (e.g., using a toast notification library like `react-toastify` or an in-app alert component). These messages should be user-centric, explaining what went wrong in simple terms if possible, and might reference the `error_code` or provide a link to the `doc_url` for more detailed troubleshooting by the user or an administrator.
        *   For specific errors (like 401 Unauthorized), they might trigger actions like redirecting the user to the login page.

**B. Logging & Error Tracking (with Sentry Integration):**
*   **Logging Framework:** Django's standard logging framework (`logging` Python module) will be used. It will be configured comprehensively in `settings.py` to define loggers (for different application modules), handlers (e.g., console output, file output, Sentry handler), formatters (for structured logging), and filters (if needed to suppress noise).
*   **Log Format (Structured JSON):** Logs will be output in a **structured JSON format**. This is crucial for easier parsing, searching, filtering, and analysis by centralized log management systems (e.g., ELK stack - Elasticsearch, Logstash, Kibana; Splunk; Grafana Loki; or cloud-based logging services like AWS CloudWatch Logs or Google Cloud Logging).
    *   A custom JSON log formatter will be implemented (e.g., by subclassing `logging.Formatter`), or a well-maintained third-party Python library like `python-json-logger` will be used to achieve this.
*   **Log Content (Key Information in Each Entry):** Each structured log entry will include, at a minimum:
    *   `timestamp`: ISO 8601 formatted timestamp with millisecond precision (e.g., `2023-10-27T10:30:00.123Z`).
    *   `level`: The log level string (e.g., "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL").
    *   `logger_name`: The name of the logger that emitted the message (typically the Python module path, e.g., `app.views.pages`, `app.services.llm`).
    *   `message`: The main human-readable log message string.
    *   **Contextual Information (added as key-value pairs in the JSON log, where applicable):**
        *   `request_id`: A unique identifier generated for each incoming HTTP request (e.g., via a custom Django middleware). This ID should be included in all logs related to that specific request, allowing for easy tracing of a request's lifecycle through different parts of the application.
        *   `user_id`: The ID of the authenticated user associated with the request or event. For PII (Personally Identifiable Information) compliance, this might be an internal database ID or a hashed/anonymized identifier in production logs, especially for log levels of INFO or below. Full user PII should generally not be logged unless essential for specific debug scenarios and with appropriate data handling policies.
        *   `source_ip`: The source IP address of the incoming request (particularly important for security-related logs like authentication failures or potential abuse).
        *   `http_method`, `http_path`, `http_status_code` (for logs related to API requests).
        *   `duration_ms` (for API request logs or logs timing long-running operations).
        *   Relevant business object IDs (e.g., `page_id`, `space_key`, `attachment_id`, `task_id` for Celery tasks).
        *   For ERROR and CRITICAL level logs capturing exceptions:
            *   `exception_type`: The class name of the exception (e.g., `ValueError`, `KeyError`, `APIBaseException`).
            *   `exception_message`: The message of the exception.
            *   `stack_trace`: The full Python stack trace (can be multi-line or formatted for JSON).
*   **Key Areas for Comprehensive Logging (with appropriate log levels):**
    *   **API Requests & Responses (INFO/DEBUG):** Log incoming requests (method, path, source IP, user agent) and outgoing responses (status code, response time). Sensitive data in request/response bodies (e.g., passwords during login, PII in page content) must be redacted or logged only at DEBUG level in development environments and never in production INFO logs.
    *   **Uncaught Exceptions & Critical Errors (ERROR/CRITICAL):** Log with full tracebacks and all available context to facilitate debugging.
    *   **Authentication Events (INFO/WARNING):** Successful logins, failed login attempts (log source IP and username for failed attempts to detect brute-forcing), token generation/refresh events, logout events.
    *   **Authorization Failures (WARNING):** Log permission denied events, including the user, the resource they attempted to access, and the attempted action.
    *   **Confluence Import Process (INFO/DEBUG/ERROR):** Log key milestones of the import process (task started, metadata parsed, number of pages/attachments found, progress updates for large imports), any errors encountered per page or attachment (with specific identifiers), and summary statistics upon completion or failure.
    *   **LLM Interactions (INFO/DEBUG/ERROR):** Log sanitized prompts (or identifiers for the prompt templates being used), the LLM provider/model being invoked, the status of the response from the LLM service, processing time for the LLM call, and any errors returned by the LLM service or during output parsing. Full LLM responses might be too verbose for INFO level; consider logging summaries or response IDs at INFO, with full (sanitized) responses available at DEBUG level if needed for troubleshooting.
    *   **Celery Background Tasks (INFO/DEBUG/ERROR):** Log task initiation (with parameters), successful completion, failures with tracebacks, and retry attempts.
    *   **Database Query Performance (DEBUG, development only):** Optionally, in development environments, log slow database queries or all queries for debugging (e.g., using Django Debug Toolbar or specific Django settings like `LOGGING` for `django.db.backends`). In production, this level of database logging is typically handled by dedicated APM (Application Performance Monitoring) tools or database-level logging.
    *   **Security Events (WARNING/ERROR):** Log potential security events like CSRF failures, CSP violations (if `report-uri` is used), suspicious request patterns detected by rate limiters.
*   **Error Tracking (Sentry Integration):**
    *   The **Sentry SDK** (or a fully self-hostable alternative like **GlitchTip** if self-hosting all components is a strict project requirement) will be integrated in **Phase 1** of development for both the backend (Django Python SDK) and frontend (React JavaScript SDK).
    *   Sentry will be configured to automatically capture and report unhandled exceptions (and optionally, handled exceptions logged at ERROR level) from both backend and frontend code to a central Sentry instance (either the Sentry.io cloud service or a self-hosted Sentry/GlitchTip installation).
    *   The Sentry integration will provide rich context for errors, greatly aiding in debugging and issue resolution. This context typically includes:
        *   Full stack traces (these will be de-minified for frontend JavaScript errors using uploaded source maps from the Vite build process).
        *   Request data associated with the error (URL, method, headers, sanitized payload if configured).
        *   User information (ID, email, IP address - subject to PII scrubbing rules configured in Sentry).
        *   Browser and Operating System details for frontend errors.
        *   Application release version (configured during deployment, allowing errors to be tracked by specific software versions).
        *   Custom tags (e.g., `space_key`, `feature_area`) and breadcrumbs (a trail of user actions or application events leading up to an error) can be added programmatically to provide even more context.
    *   **Data Scrubbing for Sentry:** Appropriate data scrubbing rules will be configured in the Sentry SDKs or on the Sentry server-side to prevent sensitive Personally Identifiable Information (PII) (e.g., passwords from request bodies, full names or email addresses if they appear in error messages or context, API keys, session IDs) from being sent to or stored by Sentry.
    *   **Source Maps for Frontend:** For frontend JavaScript errors, source maps generated during the Vite production build process will be uploaded to Sentry. This allows Sentry to de-minify (un-mangle) the minified/uglified JavaScript stack traces, making them human-readable and pointing to the original source code, which is essential for effective debugging.

**C. Monitoring (Basic Requirements for Self-Hosting Administrators) (Continued):**
*   The application will expose a basic, unauthenticated **health check endpoint** (e.g., `/api/health/`). This endpoint will perform quick, lightweight checks on the status of critical dependencies, primarily:
    *   Connectivity to the PostgreSQL database (e.g., attempt a simple query like `SELECT 1`).
    *   Connectivity to the Redis server (e.g., attempt a PING command).
    It will return an HTTP 200 OK status if all checks pass, or an HTTP 503 Service Unavailable status (with a JSON body indicating which check failed) if any critical dependency is unhealthy. This endpoint can be used by external monitoring systems, load balancers for health probes, or container orchestration platforms (like Kubernetes) for liveness and readiness probes.
*   The self-hosting documentation will include comprehensive guidance for administrators on **key metrics to monitor** to ensure a healthy, performant, and reliable deployment. While the application itself will not include built-in advanced monitoring dashboards, it will be designed to be compatible with common monitoring tools by providing structured logs (which can be ingested by log analysis platforms) and the health check endpoint. Recommended metrics for administrators to track include:
    *   **Application Metrics:**
        *   **API Error Rates:** Percentage of HTTP 5xx (server-side errors) and HTTP 4xx (client-side errors, which might indicate issues with client integrations, user input, or security probes) responses over defined time windows (e.g., per minute, per hour). Alerts should be configured for significant spikes in 5xx errors.
        *   **API Request Latency:** Average, 95th percentile (p95), and 99th percentile (p99) response times for key API endpoints, particularly those critical to user experience (e.g., page load, page save, search).
        *   **Request Throughput:** Number of requests per second/minute for the application as a whole and for key services (e.g., API, LLM interactions).
        *   **Celery Task Queue Length & Worker Status:** Number of tasks waiting in Celery queues, number of active/idle Celery workers, and task failure rates. This is crucial for monitoring the health of asynchronous processes like Confluence imports and virus scanning.
        *   **User Activity:** Number of active user sessions or authenticated users (if trackable through logs or session store metrics).
    *   **System Resource Metrics (for each service container/VM - Web Server/Django App, Celery Workers, Database, Redis, LLM Service):**
        *   **CPU Utilization:** Average and per-core CPU utilization. Sustained high CPU usage (e.g., >80-90%) can indicate performance bottlenecks or insufficient resources.
        *   **Memory Usage:** Total system memory, free/available memory, and swap usage. High swap usage is a strong indicator of RAM pressure and can severely degrade performance.
        *   **Disk Space:** Available disk space on volumes used for database storage, attachment storage (if local), application logs, and LLM model files. Alerts should be configured for low disk space conditions.
        *   **Disk I/O:** Read/write operations per second, disk queue length. High disk I/O can be a bottleneck for database-intensive applications.
        *   **Network I/O:** Bytes in/out, packets in/out, network errors or dropped packets.
    *   **PostgreSQL Database Metrics:**
        *   **Connections:** Number of active database connections, percentage of maximum connections used (ensure the connection pool is adequately sized).
        *   **Query Throughput:** Transactions per second (TPS), queries per second (QPS).
        *   **Slow Queries:** Identification and logging of slow database queries (e.g., using PostgreSQL's `log_min_duration_statement` setting).
        *   **Replication Lag:** If read replicas are used in an advanced setup, monitor the replication lag between the primary and replica instances.
        *   **Cache Hit Rates:** Monitor PostgreSQL's internal buffer cache hit rates (aim for >99% for frequently accessed data).
        *   **Index Usage:** Periodically check for unused indexes or queries performing full table scans where indexes should be used.
    *   **Redis Metrics:**
        *   **Memory Usage:** Total memory used by Redis, especially if a `maxmemory` limit is set. Monitor eviction rates if memory limits are reached.
        *   **Cache Hit/Miss Ratio:** A low hit ratio for data expected to be cached might indicate ineffective caching strategies or too-small cache sizes.
        *   **Number of Connected Clients.**
        *   **Command Latency:** Average latency for common Redis commands (GET, SET, etc.).
        *   **Persistence Status:** Status of AOF/RDB persistence (last successful save time, any errors during persistence operations like BGSAVE or AOF rewrite).
    *   **LLM Service Metrics (if running as a separate service like Ollama, or for the `llama-cpp-python` process itself):**
        *   **Request Queue Length:** If the LLM service uses an internal request queue, monitor its length.
        *   **Inference Time:** Time taken per token generated or per full request completion.
        *   **GPU Utilization and VRAM Usage:** If GPU offloading is configured and used, monitor GPU utilization percentage and the amount of VRAM consumed by the LLM models.
        *   **CPU Utilization:** CPU load generated by the LLM inference process (especially important for CPU-only inference).
        *   **Error Rates:** Number of errors reported by the LLM service itself (e.g., model loading failures, out-of-memory errors during inference).
*   The documentation will suggest that for more advanced and automated monitoring, administrators can integrate the application and its underlying services with common open-source monitoring stacks like **Prometheus** (using various exporters for Django, PostgreSQL, Redis, system metrics, Celery) and **Grafana** (for creating dashboards and visualizations). Commercial APM (Application Performance Monitoring) tools could also be used.

---

**VIII. Configuration Management**

*   All application settings and configurations will be managed centrally through Django's `settings.py` file, which serves as the single source of truth for defining how the application behaves in different environments.
*   To adhere to the **12-factor app methodology** (which promotes building robust, scalable, and maintainable applications that are environment-agnostic), values for these settings will be sourced primarily from **environment variables**. This approach allows for easy and secure configuration across different deployment environments (local development, testing/staging, production) without requiring any code modifications.
*   A **`.env.example`** file will be provided in the project's root directory. This file will serve as a template, meticulously listing all required and optional environment variables that the application uses. Each variable will be accompanied by:
    *   A placeholder value (e.g., `your_database_password_here`).
    *   Descriptive comments explaining its purpose, expected format (e.g., boolean, integer, string, URL), any default value used by the application if the variable is not set, and example valid values.
    *   Users will be instructed to copy this file to `.env` (which will be included in the project's `.gitignore` file to prevent accidental commits of sensitive credentials or environment-specific configurations) and customize it with their specific deployment values.
*   **Application-Specific Settings Structure (Example conventions within `settings.py`):**
    *   Application-specific settings (i.e., those not standard to Django or common third-party apps) will be clearly prefixed (e.g., `CC_` for "Confluence Clone") for better organization, to avoid naming conflicts with settings from Django itself or other installed apps, and to make them easily identifiable as custom configurations for this project.
    *   Helper functions or libraries like `python-dotenv` (to automatically load variables from `.env` files during local development) and `dj-database-url` (to parse full database connection URLs from a single environment variable like `DATABASE_URL`) will be used to simplify configuration management in `settings.py`.
    ```python
    # Example snippets from Django settings.py, demonstrating environment variable usage for configuration

    import os
    from pathlib import Path
    from dotenv import load_dotenv # For loading .env file in development
    import dj_database_url     # For parsing DATABASE_URL environment variable
    import ast                 # For safely evaluating string representations of lists/dicts from env vars if needed
    import json                # For parsing JSON strings from env vars

    # Define BASE_DIR for the project, typically where manage.py resides
    BASE_DIR = Path(__file__).resolve().parent.parent

    # Load environment variables from .env file if it exists (primarily for local development)
    # In production, environment variables are typically set directly in the environment or via platform config.
    dotenv_path = os.path.join(BASE_DIR, '.env')
    if os.path.exists(dotenv_path):
        load_dotenv(dotenv_path)

    # --- Standard Django Settings from Environment Variables ---
    # It is CRITICAL that SECRET_KEY is a strong, unique, random string in production.
    # The fallback here is ONLY for making development startup easier without an .env file initially.
    SECRET_KEY = os.getenv("DJANGO_SECRET_KEY", "fallback_insecure_secret_key_for_dev_only_CHANGE_THIS_IN_PRODUCTION")
    # A check should be added to ensure this is changed in production if DEBUG is False.
    if SECRET_KEY == "fallback_insecure_secret_key_for_dev_only_CHANGE_THIS_IN_PRODUCTION" and not (os.getenv("DJANGO_DEBUG", "False").lower() == "true"):
        raise ValueError("CRITICAL: DJANGO_SECRET_KEY is not set or is using the insecure default in a non-DEBUG environment!")

    DEBUG = os.getenv("DJANGO_DEBUG", "False").lower() == "true" # Default to False for production

    ALLOWED_HOSTS_STRING = os.getenv("DJANGO_ALLOWED_HOSTS", "localhost,127.0.0.1")
    ALLOWED_HOSTS = [host.strip() for host in ALLOWED_HOSTS_STRING.split(',') if host.strip()]
    # If behind a proxy, might need to configure SECURE_PROXY_SSL_HEADER for request.is_secure()

    # Database Configuration (using dj_database_url for flexibility to support various DBs via one URL)
    DEFAULT_SQLITE_PATH = f"sqlite:///{os.path.join(BASE_DIR, 'db.sqlite3')}" # Default for simple local dev if no DATABASE_URL
    DATABASE_URL = os.getenv("DATABASE_URL", DEFAULT_SQLITE_PATH)
    DATABASES = {'default': dj_database_url.parse(DATABASE_URL)}

    # Example: Add connection options specifically for PostgreSQL if it's the detected engine
    if DATABASES['default']['ENGINE'] == 'django.db.backends.postgresql':
        DATABASES['default']['OPTIONS'] = {
            'connect_timeout': int(os.getenv("CC_DB_CONNECT_TIMEOUT", "5")), # seconds
            # 'sslmode': os.getenv("CC_DB_SSLMODE", "prefer"), # Example, adjust as needed for DB security
        }
        # Ensure pg_trgm extension is available. This might be checked in a startup script or migration.

    # --- Confluence Clone Specific Settings (prefixed with CC_ for clarity) ---

    # LLM Configuration
    CC_LLM_PROVIDER = os.getenv("CC_LLM_PROVIDER", "llama_cpp").lower() # Options: "llama_cpp", "ollama_http"
    # For llama_cpp provider:
    CC_LLM_MODEL_PATH = os.getenv("CC_LLM_MODEL_PATH") # Full path to the GGUF model file. Required if CC_LLM_PROVIDER is "llama_cpp".
    CC_LLM_N_GPU_LAYERS = int(os.getenv("CC_LLM_N_GPU_LAYERS", "0")) # Number of model layers to offload to GPU (0 for CPU only).
    # For Ollama HTTP provider:
    CC_OLLAMA_API_BASE_URL = os.getenv("CC_OLLAMA_API_BASE_URL", "http://localhost:11434") # Base URL of the Ollama server.
    CC_OLLAMA_REQUEST_TIMEOUT = int(os.getenv("CC_OLLAMA_REQUEST_TIMEOUT", "120")) # Timeout in seconds for requests to Ollama.

    # General LLM parameters:
    CC_LLM_MAX_CONTEXT_LENGTH = int(os.getenv("CC_LLM_MAX_CONTEXT_LENGTH", "4096")) # Max context length (tokens) for LLM.
    CC_LLM_TEMPERATURE = float(os.getenv("CC_LLM_TEMPERATURE", "0.7")) # Default LLM sampling temperature.
    CC_LLM_DEFAULT_MODEL_ALIAS = os.getenv("CC_LLM_DEFAULT_MODEL_ALIAS", "mistral-7b") # Logical alias for default text generation model.
    CC_LLM_DEFAULT_CODE_MODEL_ALIAS = os.getenv("CC_LLM_DEFAULT_CODE_MODEL_ALIAS", "codellama-7b") # Logical alias for default code-related model.
    # Example of how a more complex model mapping might be stored if models are not just single files/names:
    # This allows different providers or parameters per alias.
    # CC_LLM_MODEL_MAPPING_JSON = os.getenv("CC_LLM_MODEL_MAPPING_JSON", """
    # {
    #   "mistral-7b": {"provider": "llama_cpp", "path_env_var": "CC_LLM_MISTRAL7B_PATH", "quantization": "Q4_K_M"},
    #   "codellama-7b": {"provider": "ollama_http", "model_name_on_ollama": "codellama:7b-instruct", "quantization": "Q5_K_M"}
    # }
    # """)
    # try:
    #     CC_LLM_MODEL_MAPPING = json.loads(CC_LLM_MODEL_MAPPING_JSON)
    # except json.JSONDecodeError:
    #     CC_LLM_MODEL_MAPPING = {} # Default to empty if JSON is invalid or not provided

    # Import Configuration
    CC_IMPORT_MAX_FILE_SIZE_MB = int(os.getenv("CC_IMPORT_MAX_FILE_SIZE_MB", "200")) # Max size of Confluence export ZIP file in MB.
    CC_IMPORT_MAX_PAGES_PER_SPACE = int(os.getenv("CC_IMPORT_MAX_PAGES_PER_SPACE", "5000")) # Guideline for number of pages per import operation.
    # Documentation should advise that imports with many/large attachments or very complex pages may need more resources/time.
    CC_IMPORT_TIMEOUT_SECONDS = int(os.getenv("CC_IMPORT_TIMEOUT_SECONDS", "7200")) # Timeout for the entire import Celery task (e.g., 2 hours).

    # Cache Configuration (Redis)
    CC_REDIS_URL = os.getenv("CC_REDIS_URL", "redis://localhost:6379/0") # Default Redis connection URL.
    CACHES = {
        "default": {
            "BACKEND": "django_redis.cache.RedisCache",
            "LOCATION": CC_REDIS_URL,
            "OPTIONS": {
                "CLIENT_CLASS": "django_redis.client.DefaultClient",
                # Example: Add connection pool settings for production, configurable via env vars
                # "CONNECTION_POOL_KWARGS": {
                #     "max_connections": int(os.getenv("CC_REDIS_MAX_CONNECTIONS", "50")),
                #     "timeout": int(os.getenv("CC_REDIS_CONNECT_TIMEOUT", "20")), # Socket connect timeout
                # },
            }
        }
    }
    # Configure Django sessions to use Redis cache if desired for scalability (instead of default database-backed sessions)
    # SESSION_ENGINE = "django.contrib.sessions.backends.cache"
    # SESSION_CACHE_ALIAS = "default"

    # Attachment Storage (django-storages) Configuration
    CC_STORAGE_BACKEND = os.getenv("CC_STORAGE_BACKEND", "local").lower() # Options: "local", "s3"
    # S3/MinIO Specific Settings (these are only actively used if CC_STORAGE_BACKEND is "s3")
    CC_AWS_ACCESS_KEY_ID = os.getenv('CC_AWS_ACCESS_KEY_ID')
    CC_AWS_SECRET_ACCESS_KEY = os.getenv('CC_AWS_SECRET_ACCESS_KEY')
    CC_AWS_STORAGE_BUCKET_NAME = os.getenv('CC_AWS_STORAGE_BUCKET_NAME')
    CC_AWS_S3_REGION_NAME = os.getenv('CC_AWS_S3_REGION_NAME') # Optional, e.g., 'us-east-1'. Not always needed for MinIO.
    CC_AWS_S3_ENDPOINT_URL = os.getenv('CC_AWS_S3_ENDPOINT_URL') # Crucial for MinIO or other S3-compatible services (e.g., 'http://minio_service_name:9000' in Docker Compose).
    CC_AWS_S3_USE_SSL = os.getenv('CC_AWS_S3_USE_SSL', 'True').lower() == 'true' # Whether to use SSL for S3 endpoint.
    CC_AWS_S3_VERIFY = os.getenv('CC_AWS_S3_VERIFY', 'True').lower() == 'true' # SSL certificate verification for S3 endpoint. Set to 'False' for self-signed certs in dev.

    if CC_STORAGE_BACKEND == "s3":
        if not (CC_AWS_ACCESS_KEY_ID and CC_AWS_SECRET_ACCESS_KEY and CC_AWS_STORAGE_BUCKET_NAME):
            # If S3 is chosen but credentials/bucket are missing, this is a configuration error.
            # Could raise an ImproperlyConfigured exception or log a critical warning.
            # For now, will fall back to local but this should be handled robustly.
            print("WARNING: CC_STORAGE_BACKEND='s3' but S3 credentials/bucket not fully configured. Falling back to local storage.")
            DEFAULT_FILE_STORAGE = 'django.core.files.storage.FileSystemStorage'
            MEDIA_URL = '/media/'
            MEDIA_ROOT = os.path.join(BASE_DIR, 'mediafiles_data_fallback')
        else:
            DEFAULT_FILE_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'
            AWS_ACCESS_KEY_ID = CC_AWS_ACCESS_KEY_ID
            AWS_SECRET_ACCESS_KEY = CC_AWS_SECRET_ACCESS_KEY
            AWS_STORAGE_BUCKET_NAME = CC_AWS_STORAGE_BUCKET_NAME
            if CC_AWS_S3_REGION_NAME: AWS_S3_REGION_NAME = CC_AWS_S3_REGION_NAME
            if CC_AWS_S3_ENDPOINT_URL: AWS_S3_ENDPOINT_URL = CC_AWS_S3_ENDPOINT_URL
            AWS_S3_USE_SSL = CC_AWS_S3_USE_SSL
            AWS_S3_VERIFY = CC_AWS_S3_VERIFY if CC_AWS_S3_ENDPOINT_URL else None # Boto3 default behavior if no custom endpoint URL
            AWS_S3_FILE_OVERWRITE = False # Recommended: Set to False to prevent accidental data overwrites in S3.
            AWS_DEFAULT_ACL = None # Or 'private' / 'public-read' depending on bucket policy and needs. `None` usually means private.
            # Other S3 specific settings like AWS_S3_SIGNATURE_VERSION ('s3v4'), AWS_S3_ADDRESSING_STYLE can be added if needed for specific S3 providers.
    else: # Default to local filesystem storage if CC_STORAGE_BACKEND is "local" or S3 config is incomplete
        DEFAULT_FILE_STORAGE = 'django.core.files.storage.FileSystemStorage'
        MEDIA_URL = '/media/' # URL prefix for serving media files locally during development (requires URL pattern)
        MEDIA_ROOT = os.path.join(BASE_DIR, 'mediafiles_data') # Local directory where media files will be stored
        # Ensure MEDIA_ROOT is writable by the Django application process in production.

    # Virus Scanning Configuration
    CC_ENABLE_VIRUS_SCANNING = os.getenv("CC_ENABLE_VIRUS_SCANNING", "False").lower() == "true" # Enable/disable AV scanning globally
    CC_CLAMAV_HOST = os.getenv("CC_CLAMAV_HOST", "clamav") # Hostname of the clamd daemon (e.g., Docker service name)
    CC_CLAMAV_PORT = int(os.getenv("CC_CLAMAV_PORT", "3310")) # Port for the clamd daemon
    # Alternatively, if using clamdscan CLI directly: CC_CLAMDSCAN_PATH = os.getenv("CC_CLAMDSCAN_PATH", "/usr/bin/clamdscan")

    # Sentry DSN for Error Tracking
    CC_SENTRY_DSN = os.getenv('CC_SENTRY_DSN')
    if CC_SENTRY_DSN and not DEBUG: # Typically enable Sentry only in non-DEBUG (production-like) environments
        import sentry_sdk
        from sentry_sdk.integrations.django import DjangoIntegration
        from sentry_sdk.integrations.celery import CeleryIntegration
        sentry_sdk.init(
            dsn=CC_SENTRY_DSN,
            integrations=[
                DjangoIntegration(),
                CeleryIntegration(), # If using Celery
            ],
            traces_sample_rate=float(os.getenv('CC_SENTRY_TRACES_SAMPLE_RATE', '0.1')), # For Sentry Performance Monitoring (0.0 to 1.0)
            send_default_pii=False, # Do not send PII (like user IPs or usernames in event context) to Sentry by default
            environment=os.getenv('CC_ENVIRONMENT_NAME', 'production'), # e.g., production, staging, development
            release=os.getenv('APP_RELEASE_VERSION', 'unknown') # Helps track errors by app version (e.g., git commit SHA or semantic version)
        )
    ```
*   The self-hosting documentation will provide a comprehensive list of all supported environment variables. This list will clearly explain the purpose of each variable, its acceptable values or format, any default behavior if the variable is not explicitly set, and provide example configurations for common deployment scenarios (e.g., local development using Docker Compose, a production deployment using S3 with MinIO for storage, a production deployment with AWS S3). This detailed configuration guidance is crucial for enabling users to successfully self-host the application.

---

**IX. Backup and Restore Strategy**

*   A comprehensive and well-documented backup and restore strategy is essential for data safety and operational resilience in self-hosted deployments. The documentation provided to users will detail the following components and procedures:

    **A. Components to Backup:**
    1.  **PostgreSQL Database:** This is the most critical component as it contains all core application data, including users, spaces, page content (ProseMirror JSON), page versions, tags, attachment metadata, permissions, and application settings stored in the database.
    2.  **Attachment Storage:** The actual binary files (images, documents, videos, etc.) uploaded by users. The location of these files depends on the `django-storages` backend configuration:
        *   If using local filesystem storage: The directory specified by `MEDIA_ROOT`.
        *   If using S3-compatible object storage: The entire S3 bucket (or relevant prefixes within it).
    3.  **Redis Data:** Redis is used for caching, as a Celery message broker, and potentially for Django sessions.
        *   Since AOF (Append Only File) persistence is enabled for Redis for better durability, backing up the AOF and RDB files is recommended. This ensures that if Redis restarts, it can recover its state, which might include important Celery task queue information or cached data that is expensive to regenerate. For purely transient caches, backup might be less critical but still good for faster recovery.
    4.  **LLM Model Files (If Locally Managed by User in a Custom Path):** If users download and manage large GGUF model files themselves in a specific directory on their server that is referenced by application settings (e.g., `CC_LLM_MODEL_PATH`), they are responsible for backing up these model files if re-downloading them would be time-consuming or problematic. The application itself does not modify these model files; it only reads them.
    5.  **Configuration Files:**
        *   The primary configuration file is the **`.env` file**, which contains sensitive credentials (database passwords, `SECRET_KEY`, S3 keys) and all custom application settings. This file **must** be backed up securely and kept separate from public code repositories.
        *   Any custom Nginx or other web server configuration files related to the application deployment.
        *   Custom scripts used for deployment, management, or cron jobs associated with the application.
    6.  **Docker Volumes:** If data is stored in named Docker volumes (e.g., for PostgreSQL data, Redis data, or the local `mediafiles_data` directory if `django-storages` is configured for local filesystem and mounted via a volume), these volumes must be explicitly included in the backup strategy.

    **B. Backup Procedures:**
    1.  **PostgreSQL Database:**
        *   **Method:** Regular, automated logical backups using the `pg_dump` utility. This utility creates a SQL script or, preferably, a custom-format archive (`-Fc` flag) that can be used to recreate the database structure and data.
        *   **Frequency:** Recommended: At least nightly for most deployments. For highly active instances with critical data, more frequent backups (e.g., every few hours) or setting up Point-In-Time Recovery (PITR) might be necessary.
        *   **Command Example (Custom Format Archive):**
            `pg_dump -U <db_username> -h <db_hostname_or_service_name> -Fc -Z 9 -f /path/to/secure_backups/yourdbname_$(date +%Y%m%d_%H%M%S).dump.gz <database_name>`
            (Replace placeholders with actual values. `-Fc` for custom format, `-Z 9` for gzip compression. Ensure the backup user has necessary permissions.)
        *   **Retention Policy:** Implement a clear retention policy for backup files (e.g., keep daily backups for 7-14 days, weekly backups for 4-8 weeks, monthly backups for 6-12 months, and potentially yearly backups for longer archival if required by compliance).
        *   **Security of Backups:** Backup files must be stored securely:
            *   Encrypted at rest (e.g., using GPG to encrypt the dump file, or by storing on an encrypted backup storage medium/service).
            *   Stored in an access-controlled location, physically or logically separate from the primary server (ideally off-site or in a different availability zone/region for robust disaster recovery).
        *   **Advanced (Point-In-Time Recovery - PITR):** The documentation should briefly mention Point-In-Time Recovery setup using PostgreSQL's Write-Ahead Logging (WAL) archiving as an advanced option for deployments requiring minimal data loss between full backups (i.e., ability to restore to any specific point in time). This involves continuously archiving WAL segments to a separate storage location.
    2.  **Attachment Storage:**
        *   **Local Filesystem (`MEDIA_ROOT` or Docker Volume):**
            *   Regularly back up the entire `MEDIA_ROOT` directory (or the Docker volume that mounts it).
            *   Tools: `rsync` to a separate backup disk or server (can be done incrementally), filesystem-level snapshots (if available on the host OS or virtualization platform), or dedicated backup software (e.g., Bacula, Duplicati, Restic, BorgBackup).
            *   Frequency should generally align with the database backup frequency to maintain consistency between database records (which reference attachments) and the actual file storage.
        *   **S3-Compatible Object Storage (MinIO, AWS S3, etc.):**
            *   Leverage the built-in features and best practices of the S3 provider:
                *   **Bucket Versioning:** Enable versioning on the S3 bucket. This keeps all versions of an object (including all writes and even deletes), which can protect against accidental deletions or overwrites and allow for easy rollback of individual files.
                *   **Replication (for DR):** For AWS S3, configure Cross-Region Replication (CRR) or Same-Region Replication (SRR) to automatically replicate objects to a bucket in a different AWS region or account for disaster recovery or data redundancy. For MinIO, use `mc mirror` to replicate the bucket to another MinIO instance or S3-compatible target.
                *   **Lifecycle Policies:** Use S3 lifecycle policies to manage object versions automatically (e.g., transition older, non-current versions to cheaper storage classes like Glacier, and eventually delete very old versions if they are no longer needed for compliance or recovery).
                *   **S3 Bucket Backups/Snapshots:** Some cloud providers offer services or features to take point-in-time snapshots or backups of S3 buckets to other storage or archival tiers.
    3.  **Redis Data (with AOF Persistence):**
        *   **Configuration:** Ensure Redis is configured for AOF (Append Only File) persistence in its `redis.conf` file: `appendonly yes`. This provides better durability against data loss compared to RDB snapshots alone, as AOF logs every write operation. RDB snapshots (e.g., configured with `save 60 1000` to save if 1000 keys changed in 60 seconds) can still be useful for faster startup or as periodic full backups.
        *   **Backup Procedure:** Regularly back up both the AOF file (default filename: `appendonly.aof`) and the RDB snapshot file (default filename: `dump.rdb`) from the Redis data directory (path specified by the `dir` directive in `redis.conf`).
        *   These files should be copied while Redis is running. AOF is designed for this (though ensure consistency if copying a live, frequently changing AOF). For RDB, it's safest to trigger a `BGSAVE` command via `redis-cli` to ensure an up-to-date snapshot is written to disk, and then copy the `dump.rdb` file.
        *   Frequency should align with the criticality of the data stored in Redis (e.g., if Celery task queues are vital and stored in Redis, backups should be frequent).
    4.  **Configuration Files & Docker Volumes:**
        *   The `.env` file (containing sensitive credentials and all custom configurations) and any custom deployment scripts or web server configuration files (e.g., Nginx `sites-available` if customized) **must** be backed up securely. Consider storing them in a private, encrypted version control repository (like a private GitHub repo with secrets management) or a secure password manager.
        *   If using named Docker volumes for persistent data (e.g., for PostgreSQL data, Redis data, or the local `mediafiles_data` directory if `django-storages` is configured for local filesystem and mounted via a volume), the backup strategy must explicitly include backing up these volumes. This can be done using:
            *   Docker volume backup utilities or scripts (e.g., a common pattern is to run a temporary container that mounts the volume to be backed up and another volume for the backup destination, then uses `tar` to create an archive: `docker run --rm -v volume_name_to_backup:/data -v /path_to_host_backups:/backup ubuntu tar cvf /backup/volume_name_to_backup_$(date +%Y%m%d).tar /data`).
            *   By stopping the containers that use the volume and then copying the volume's data directory directly from the Docker host filesystem (the path to volume data can usually be found via `docker volume inspect volume_name_to_backup`). This requires downtime for the affected containers.

    **C. Restore Procedures Documentation:**
    *   A clear, step-by-step, and **ordered** procedure for restoring the entire application to a consistent state from backups must be meticulously documented. This procedure is critical for effective disaster recovery. The typical steps involve:
        1.  **Prepare Environment:** Set up a new (or clean) server/environment if restoring from a complete disaster. Install Docker, Docker Compose, and any other necessary system dependencies.
        2.  **Restore PostgreSQL Database:**
            *   Create a new, empty PostgreSQL database and user with appropriate permissions on the target PostgreSQL server.
            *   Use `pg_restore` (if the backup is in custom format `.dump` or an archive format) or `psql` (if the backup is a plain SQL script `.sql`) to import the database backup into the new PostgreSQL instance. Example for custom format: `pg_restore -U <db_username> -h <db_hostname> -d <new_database_name> /path/to/backups/yourdbname_backup.dump`.
        3.  **Restore Attachment Files:**
            *   **Local Filesystem:** Copy the backed-up `MEDIA_ROOT` directory (or the contents of the backed-up Docker volume for media) to the appropriate location on the new server. Ensure that file ownership and permissions are correctly set for the user that the Django application process will run as.
            *   **S3 Object Storage:** Restore the S3 bucket from its backup (if using S3 backup services) or ensure the new application instance is configured to point to the correct, restored S3 bucket (and that data integrity within the bucket has been verified). If using bucket versioning, restoring might involve copying specific versions of objects or undeleting them.
        4.  **Restore Redis Data:**
            *   Stop the Redis server on the new instance (if it's running).
            *   Replace the `appendonly.aof` and/or `dump.rdb` files in its configured data directory with the backed-up versions of these files.
            *   Restart the Redis server. Redis will then load its state from the AOF file (if enabled and present, as it provides more recent data) or from the RDB snapshot.
        5.  **Restore Configuration:**
            *   Place the backed-up `.env` file (ensuring it contains correct database URLs, S3 credentials if applicable, Redis URL, etc., for the new/restored environment) in the application's root directory or configure environment variables directly in the deployment environment.
            *   Restore any custom Nginx or other web server configuration files to their correct locations.
        6.  **Deploy Application Code:** Deploy the same version of the application code (e.g., the Docker images) that was running when the backup was taken, or a version known to be compatible with the restored database schema. If restoring an older database, deploying newer code might require running database migrations first.
        7.  **Run Database Migrations (If Necessary):** After restoring the database and deploying the application code, run Django database migrations (`python manage.py migrate`) to ensure the database schema is fully up-to-date and consistent with the deployed code version. Be cautious if restoring an older database to a significantly newer code version, as complex migrations might be involved.
        8.  **Start Application Services:** Start all application services in the correct order (e.g., PostgreSQL and Redis first, then Celery workers and beat (if used), then the Django/Gunicorn application server, then Nginx reverse proxy).
        9.  **Thorough Testing Post-Restore:** After restoration, thoroughly test all aspects of the application to ensure data integrity, full functionality, correct user access, and acceptable performance. Check application logs for any errors or warnings during startup or initial use.
    *   **Crucial Note for Documentation (Disaster Recovery Testing):** The self-hosting documentation must strongly emphasize the **critical importance of periodically (e.g., quarterly or semi-annually, or after any significant infrastructure change) testing the entire restore procedure** from a set of backups in a non-production, isolated environment. Untested backups provide a false sense of security. A tested Disaster Recovery (DR) plan is essential for business continuity and minimizing downtime in the event of an actual disaster. This testing should also aim to validate the Recovery Point Objective (RPO - how much data can be affordably lost, which dictates backup frequency) and the Recovery Time Objective (RTO - how long it takes to restore service after a disaster).
    *   **[Post-MVP/Community Contribution] Automated DR Validation:** For larger or more critical deployments, the documentation might suggest that scripts or CI/CD pipeline steps could be developed to automate parts of the Disaster Recovery validation process. This could involve automatically spinning up a temporary environment, attempting to restore a recent backup, and running a suite of basic health checks or E2E tests against the restored instance to confirm its viability.

---

**X. Internationalization (i18n) & Localization (l10n)**

*   Support for multiple languages for the application's user interface (UI strings, labels, messages) and for managing page content itself in different languages (localization) is **considered out of scope for the initial MVP (Minimum Viable Product)** and immediate post-MVP releases.
*   The primary development focus for the MVP will be a fully functional application with an English-language user interface and content management capabilities.
*   **Architectural Considerations to Facilitate Future i18n/l10n (Best Practices to be followed during MVP development):**
    *   **UTF-8 Encoding:** The application will use UTF-8 encoding by default for all text data. This includes:
        *   Database storage (PostgreSQL defaults to UTF-8 for new databases, which should be ensured during setup).
        *   File handling (e.g., when reading/writing Markdown files for import/export).
        *   API request/response bodies (e.g., API responses will specify `Content-Type: application/json; charset=utf-8`).
        *   HTML rendering by the frontend (e.g., `<meta charset="UTF-8">` in the main HTML shell).
        UTF-8 is a variable-width character encoding that can represent most characters from all major global languages, making it a suitable foundation for future internationalization.
    *   **UI String Management (Frontend - Preparation for i18n):** While not implementing a full i18n framework like `react-i18next` during the MVP, frontend development (React components) will strive to **avoid hardcoding UI display strings directly within component render methods wherever practical.** Instead, UI strings will be managed as:
        *   Constants in dedicated modules (e.g., `src/constants/messages.ts`).
        *   Or within simple translation-like object structures (e.g., `const T = { page_title_greeting: "Hello World" };` used as `T.page_title_greeting`).
        This practice will make future integration with a dedicated frontend i18n library significantly easier if this feature is prioritized later, as it centralizes strings and makes them easier to extract for translation.
    *   **Backend i18n (Django - Awareness):** Django has robust built-in support for internationalization via its `django.utils.translation` module, which primarily uses the `gettext` system ( `.po` and `.mo` files). If backend-generated messages (e.g., email notifications, some API error messages not handled by DRF's default localization, or Django model field `verbose_name` and `help_text` attributes) need to be translated in the future, this framework would be the standard choice. For MVP, these will be in English.
    *   **Date/Time Formatting:** Dates and times displayed in the UI will be handled with consideration for potential future localization. This means:
        *   Storing all timestamps in UTC (Coordinated Universal Time) on the backend database.
        *   Using a robust client-side date/time formatting library (e.g., `date-fns` or `Day.js`) that supports locale-specific formatting. For MVP, it will default to a common English locale (e.g., `en-US` or `en-GB`), but the use of such a library makes it easier to switch locales later.
*   **Future Prioritization:** If there is significant user demand or a clear business case for supporting multiple languages (i18n for the UI, l10n for allowing content to be managed in different languages, potentially with translation workflows) post-launch, it will be evaluated and potentially planned as a major feature set for a subsequent development phase. This would involve significant effort in:
    *   Extracting all UI strings for translation.
    *   Setting up translation workflows and managing translation files.
    *   Adapting date, number, and currency formats for different locales.
    *   Potentially designing schema changes for storing and managing localized page content if that level of l10n is required.

---

**XI. Multi-Tenancy Considerations**

*   The application is designed and planned as a **single-tenant, self-hosted solution**. This means that each deployed instance of the application is intended for use by a single, distinct organization, team, or individual. All data within a particular deployment (including users, spaces, pages, attachments, and configurations) belongs to that single tenant and is not shared with, nor isolated from (in the context of other tenants on the same instance, as there are none), other deployments.
*   **True multi-tenancy**, which would be required for a SaaS (Software as a Service) offering where a single, shared application instance and potentially a shared database serve multiple distinct customer organizations (tenants) with strict data isolation between tenants and potentially per-tenant customizations (like custom domains or branding), introduces significant architectural, security, and operational complexities. Key challenges and considerations in a multi-tenant architecture include:
    *   **Data Isolation Strategy:** Implementing a robust and secure method for isolating each tenant's data within the shared database (if a shared DB approach is taken). Common approaches include:
        *   *Discriminator Column with Row-Level Security:* Adding a `tenant_id` foreign key to all relevant database tables and ensuring that all database queries (reads and writes) are strictly scoped by the current tenant's ID. This is often combined with database-level Row-Level Security (RLS) policies in databases like PostgreSQL for an added layer of enforcement. This is the most common approach for Django applications.
        *   *Schema-per-Tenant (PostgreSQL):* Using separate database schemas within a single PostgreSQL database instance for each tenant. Each tenant's data resides in its own schema, providing strong isolation. Django database routers would be needed to direct queries to the correct schema.
        *   *Database-per-Tenant:* Using entirely separate database instances for each tenant. This provides the strongest isolation but is generally more complex and costly to manage and scale.
    *   **User Authentication & Authorization Across Tenants:** Managing users and their permissions in a way that is aware of tenant boundaries (e.g., a user might belong to multiple tenants with different roles in each, or user identity might be global but their access scoped per tenant). URLs and API calls would need to resolve to the correct tenant context.
    *   **Request Routing & Tenant Identification:** Identifying the correct tenant for each incoming HTTP request. This can be done based on:
        *   Subdomain (e.g., `tenantA.yourapp.com`, `tenantB.yourapp.com`).
        *   Custom domain mapped by the tenant.
        *   A path component in the URL (e.g., `/tenantA/somepage`, though less common for SaaS).
        *   A tenant identifier passed in an HTTP header (less common for primary identification).
    *   **Customization & Branding:** Potentially allowing tenants to customize aspects of their application experience, such as theme colors, logos, or specific feature flags.
    *   **Resource Management, Quotas, and Billing:** If it's a commercial SaaS offering, accurately tracking and managing resource usage (storage space, number of users, API call volume, LLM usage) on a per-tenant basis and implementing billing based on usage or subscription plans.
    *   **Deployment & Scalability:** Managing the deployment, scaling, and maintenance of a multi-tenant application, including onboarding new tenants, handling database migrations across many tenants, and ensuring performance does not degrade as the number of tenants grows.
*   Given these extensive complexities, implementing true multi-tenancy is **explicitly out of scope for the initial MVP** and any near-term subsequent releases. It would require a fundamental re-architecture of many core parts of the application (data models, querying logic, authentication, deployment infrastructure) and would be considered a separate, major project if a compelling business case for a SaaS offering or a multi-organization shared instance were to emerge in the future. The current focus is on providing a high-quality, feature-rich, and secure application for single-tenant self-hosting scenarios.

---

**XII. Technology Stack & Key Dependencies (Consolidated & Categorized)**

*   **Backend (Python / Django Ecosystem):**
    *   **Core Framework:** Python (latest stable 3.x, e.g., 3.10+, 3.11+), Django (latest stable LTS, e.g., 4.2.x or 5.0.x if LTS by project start).
    *   **API Development:** Django REST Framework (DRF).
    *   **Authentication & Authorization:**
        *   `djangorestframework-simplejwt` (For JWT-based authentication, configured for token rotation and blacklisting).
        *   `django-guardian` (For object-level permissions / Role-Based Access Control - RBAC).
        *   `django-allauth` (Consideration for comprehensive user account management: registration, email verification, password reset; social auth part is **[Post-MVP Consideration]**).
    *   **Task Queuing:** Celery (Distributed task queue).
    *   **Database & Cache Interaction:**
        *   `psycopg2-binary` (PostgreSQL adapter for Python).
        *   `django-redis` (Django cache backend integration for Redis).
        *   `dj-database-url` (For parsing `DATABASE_URL` environment variable).
    *   **File Handling & Storage:**
        *   `django-storages` (Provides a unified interface for various file storage backends: local filesystem, S3/MinIO, etc.).
        *   `boto3` (AWS SDK for Python, used by `django-storages` for S3 backends).
    *   **Content Processing & Import:**
        *   `beautifulsoup4` (For HTML parsing during Confluence import).
        *   `turndown` (Python port or equivalent, for HTML to Markdown conversion as a fallback during import).
        *   `Markdown` (Python Markdown library, potentially for server-side Markdown processing or AST manipulation if needed during import/export).
        *   `python-magic` (Optional, for more reliable MIME type detection of uploaded files if Django's default is insufficient).
    *   **LLM Integration:**
        *   `llama-cpp-python` (Primary library for direct interaction with GGUF quantized LLM models).
        *   `langchain` (Framework for building LLM-powered applications: prompt templating, chains, output parsers, LLM wrappers).
    *   **Resilience & Security:**
        *   `pybreaker` (Circuit breaker pattern implementation, especially for LLM calls).
        *   `bleach` (HTML sanitization library for use on the backend, e.g., for sanitizing content before passing to LLMs if it's not plain text).
        *   `django-ratelimit` (For API request rate limiting).
    *   **Web Server (Production):** `gunicorn` or `uwsgi` (WSGI HTTP Server).
    *   **Environment Management (Development):** `python-dotenv` (For loading environment variables from `.env` files).
    *   **Error Tracking:** Sentry SDK (`sentry-sdk[django,celery]`).
    *   **Virus Scanning Interface (ClamAV):** `python-clamd` (If interfacing with a `clamd` daemon via its socket) or direct subprocess calls to `clamscan`/`clamdscan` command-line tools.
*   **Frontend (React Ecosystem with TypeScript):**
    *   **Core:** React (v18+), TypeScript.
    *   **Build Tool & Dev Server:** Vite.
    *   **Rich Text Editor:** Tiptap (v2) & underlying ProseMirror libraries.
        *   Key Tiptap packages: `@tiptap/react`, `@tiptap/pm` (core ProseMirror modules), `@tiptap/starter-kit` (bundle of common editor extensions).
        *   Specialized Tiptap extensions to be used: `@tiptap/extension-link`, `@tiptap/extension-table`, `@tiptap/extension-table-row`, `@tiptap/extension-table-cell`, `@tiptap/extension-table-header`, `@tiptap/extension-task-list`, `@tiptap/extension-task-item`, `@tiptap/extension-image`, `@tiptap/extension-placeholder`, `@tiptap/extension-focus`, `@tiptap/extension-character-count`.
        *   `@tiptap/extension-markdown` (For Markdown import/export integration with Tiptap).
        *   Custom Tiptap extensions will be developed by the project for Mermaid diagrams, Draw.io diagrams, enhanced Code Blocks, and FallbackMacroPlaceholders, potentially using React Node Views for complex UI within these nodes.
    *   **State Management:** Zustand (for global application state). React Context API (for more localized state sharing).
    *   **Routing:** React Router (`react-router-dom` v6+).
    *   **Styling:** Tailwind CSS (Utility-first CSS framework). PostCSS (used by Tailwind).
    *   **UI Primitives (Optional but Recommended):** Headless UI libraries like Radix UI or Headless UI by Tailwind Labs (to provide unstyled, accessible base components that are then styled with Tailwind CSS).
    *   **HTTP Client:** `axios` (or native `fetch` API wrapped with custom logic for interceptors, error handling, and adding auth headers).
    *   **Diagram Rendering & Interaction:**
        *   `Mermaid.js` (Client-side library for rendering Mermaid diagrams from syntax).
        *   `diagrams.net-embed` (or a similar library/approach for embedding the Draw.io editor in an iframe).
    *   **Syntax Highlighting:** `highlight.js` (Client-side syntax highlighting for code blocks within the editor and on rendered pages).
    *   **HTML Sanitization (Frontend):** `DOMPurify` (Client-side HTML sanitization, critically important for sanitizing the HTML output from `highlight.js` before rendering).
    *   **Diffing:**
        *   `prosemirror-changeset` (or a similar ProseMirror-native diffing library, this is the preferred option for semantic comparison of page versions).
        *   `diff-match-patch` (Client-side library for text/HTML diffing, as a fallback for page version comparison if semantic ProseMirror diffing is too complex for MVP).
    *   **Date/Time Utilities:** A library like `date-fns` or `Day.js` for robust formatting and manipulation of dates/times in the UI, supporting potential future internationalization.
    *   **UUID Generation:** `uuid` (for client-side generation of unique identifiers, e.g., for code blocks).
    *   **Error Tracking:** Sentry SDK (`@sentry/react` with `@sentry/tracing` for performance).
*   **Database:** PostgreSQL (latest stable version, e.g., 14+, 15+, or 16+ at project start), with the **`pg_trgm` extension enabled** in the database instance for fuzzy search capabilities.
*   **Cache & Message Broker:** Redis (latest stable version, e.g., 6+ or 7+), configured with **AOF (Append Only File) persistence enabled** for data durability.
*   **Deployment & Infrastructure (Self-Hosted Context):**
    *   Docker & Docker Compose (for local development environment setup and for providing containerized production deployment artifacts).
    *   Nginx (or a similar robust web server/reverse proxy like Traefik or Caddy) for serving static frontend files, acting as a reverse proxy for the Django/Gunicorn application server, handling HTTPS termination (SSL/TLS), and potentially injecting CSP headers in production.
    *   ClamAV (antivirus engine, typically run in a separate Docker container, accessible via `clamd` socket or HTTP if using a wrapper).
    *   Ollama (Optional, alternative way for users to serve local LLMs via an HTTP API if they prefer this over the direct `llama-cpp-python` integration managed by the main application).
*   **Development Tools & CI/CD Pipeline:**
    *   **Version Control:** Git (hosted on GitHub, GitLab, or similar).
    *   **Code Linting & Formatting:**
        *   Frontend (JS/TS): ESLint, Prettier.
        *   Backend (Python): Ruff (preferred for its speed and comprehensive checks, covering Flake8, Black, isort, and more), or alternatively Flake8 + Black + isort.
    *   **Testing Frameworks:**
        *   Backend: `pytest` and `pytest-django`.
        *   Frontend: Jest & React Testing Library (`@testing-library/react`).
        *   End-to-End (E2E): Playwright (preferred for its robustness and features) or Cypress.
    *   **Accessibility Testing:** `axe-core` (automated accessibility scanning engine, typically integrated with E2E tests via libraries like `axe-playwright` or `cypress-axe`).
    *   **Performance Auditing:** Lighthouse (can be run via CLI, browser DevTools, or integrated into CI pipelines for automated performance checks of key pages).
    *   **Continuous Integration/Continuous Deployment (CI/CD):** A platform like GitHub Actions, GitLab CI, Jenkins, or CircleCI will be used to automate:
        *   Running linters and code style checks on every commit/PR.
        *   Executing unit and integration tests.
        *   Running E2E tests (including accessibility scans).
        *   Building Docker images for the application services.
        *   Deploying to staging and production environments (manual trigger for production, or automated on merge to main/release branch).

---

**XIII. API Endpoints (Summary & Key Paths - Full Details in OpenAPI 3.0 Specification)**

*   The definitive and comprehensive source for all API endpoint detailsincluding precise paths, HTTP methods, request/response schemas (with JSON Schema definitions for ProseMirror `raw_content` and other complex objects), parameters (path, query, header), authentication requirements, and expected status codes with their error response schemaswill be the **OpenAPI 3.0 specification**. This specification will be auto-generated from the Django REST Framework code (serializers, viewsets, docstrings) using the `drf-spectacular` package. It will be version-controlled with the codebase and made available via a dedicated API endpoint (e.g., `/api/schema/openapi.yaml` or `/api/schema/swagger-ui/`) for developers and API consumers.
*   **Pagination Standard:** For API endpoints that return lists of resources (e.g., pages, spaces, versions), **offset-based pagination** will be used by default for the MVP. This involves `limit` (number of items per page, e.g., `?limit=25`) and `offset` (starting index of items, e.g., `&offset=50`) query parameters. Django REST Framework's `LimitOffsetPagination` class will be the standard. API responses for paginated lists will include `count` (total number of items), `next` (URL for the next page or `null`), `previous` (URL for the previous page or `null`), and `results` (array of resource items for the current page).
*   **List of Key API Endpoints (This is an illustrative summary to guide initial scaffolding by an AI; the OpenAPI spec will be the source of truth):**
    *   **Authentication & User Management (`/api/auth/`):**
        *   `POST /register/`: User self-registration (expects username, email, password; returns user info & tokens upon success with email verification step).
        *   `POST /login/`: User login (expects username/email, password; returns JWT access and refresh tokens).
        *   `POST /token/refresh/`: Obtain a new JWT access token using a valid refresh token.
        *   `POST /logout/`: User logout (if using token blacklisting for refresh tokens, this endpoint would blacklist the current refresh token).
        *   `GET /user/`: Get details (profile) of the currently authenticated user.
        *   `POST /password/reset/`: Request a password reset email (expects email).
        *   `POST /password/reset/confirm/`: Confirm password reset using a token from email (expects token, new password).
        *   `POST /email/verify/request/`: Request a new email verification link (if initial one expired or not received).
        *   `POST /email/verify/confirm/:key/`: Confirm email address using a key from verification email.
    *   **Spaces (`/api/spaces/`):**
        *   `GET /`: List all spaces accessible to the current user (paginated, filterable by e.g., owner, name).
        *   `POST /`: Create a new space (expects `key`, `name`, optional `description`).
        *   `GET /{space_key}/`: Retrieve details of a specific space by its unique key.
        *   `PUT /{space_key}/`: Update details (name, description) of a specific space (requires appropriate permissions).
        *   `DELETE /{space_key}/`: Soft-delete a space (requires appropriate permissions).
    *   **Pages & Versions (`/api/pages/`, related to spaces):**
        *   `GET /spaces/{space_key}/pages/`: List pages within a specific space (paginated, filterable by title, tags; sortable by title, updated_at; can support query params for hierarchy root or depth).
        *   `POST /pages/`: Create a new page (expects `space_id` or `space_key`, `title`, `raw_content` (ProseMirror JSON), optional `parent_page_id`, optional `tags`).
        *   `GET /pages/{page_id}/`: Retrieve details of a specific page, including its `raw_content` and current `version`.
        *   `PUT /pages/{page_id}/`: Update an existing page (expects `title`, `raw_content`, optional `commit_message`, optional `tags`). This action will create a new `PageVersion`.
        *   `DELETE /pages/{page_id}/`: Soft-delete a page.
        *   `GET /pages/{page_id}/children/`: List direct child pages of a given page (paginated).
        *   `GET /pages/{page_id}/versions/`: List all historical versions of a specific page (paginated, showing version number, author, timestamp, commit message).
        *   `GET /pages/{page_id}/versions/{version_number}/`: Retrieve the content (`raw_content` and `schema_version`) of a specific historical version of a page.
        *   `POST /pages/{page_id}/revert/{version_number}/`: Revert the page's current content to that of a specific historical version (this creates a new current version reflecting the reverted state).
    *   **Attachments (`/api/attachments/`):**
        *   `GET /?page_id={page_id}`: List metadata of attachments for a specific page (paginated).
        *   `POST /presigned-upload-url/`: (Used when S3-compatible storage is configured) Generate a pre-signed URL for direct client-side file upload to S3/MinIO. Request body: `{ page_id: int, filename: str, content_type: str, size: int }`. Response: `{ upload_url: str, fields: dict }` (for POST) or `{ upload_url: str }` (for PUT).
        *   `POST /complete-upload/`: (Used with pre-signed URLs) Notify the backend after the client has successfully uploaded a file to S3, so the backend can create the final `Attachment` record and trigger virus scan. Request body: `{ page_id: int, filename: str, mime_type: str, size_bytes: int, upload_key_in_s3: str }`.
        *   `POST /`: (Used for uploads directly through the Django server, e.g., for local filesystem storage or if direct S3 upload is not the chosen path for some reason) Upload a new attachment file (using `multipart/form-data`, requires `page_id` in form data). This endpoint will save the file and then trigger the asynchronous virus scanning Celery task.
        *   `GET /{attachment_id}/`: Retrieve metadata for a specific attachment (including its `scan_status`).
        *   `GET /{attachment_id}/download/`: Download the attachment file. This endpoint will enforce "zero-trust" download headers (`Content-Type: application/octet-stream`, `Content-Disposition`) and will check `scan_status` before allowing the download (blocking if infected, warning if pending/error).
        *   `DELETE /{attachment_id}/`: Delete an attachment (this might be a soft delete or hard delete depending on policy).
    *   **Import/Export & Utilities (`/api/io/`):**
        *   `POST /import/confluence/`: Upload a Confluence Space HTML Export ZIP file for processing. This will save the file and initiate an asynchronous Celery background task for the import. Response: `{ task_id: "celery_task_uuid" }`.
        *   `GET /import/confluence/status/{task_id}/`: Check the status (e.g., PENDING, STARTED, PROGRESS (with percentage), SUCCESS, FAILURE with error details) of an ongoing Confluence import Celery task.
        *   `GET /pages/{page_id}/export/markdown/`: Export the content of a specific page as a downloadable Markdown (`.md`) file.
        *   `POST /pages/import/markdown/`: Import a Markdown file to create a new page (request body: `{ space_id_or_key: str|int, parent_page_id: Optional[int], markdown_content: str, title: Optional[str] }`).
        *   `POST /diagrams/validate/mermaid/`: Validate provided Mermaid syntax (request body: `{ mermaid_code: str }`, response: `{ "is_valid": true/false, "error_message": Optional[str] }`).
    *   **LLM Services (`/api/llm/`):** (All endpoints in this group will be rate-limited and require authentication)
        *   `POST /generate-text/`: Generate text based on a prompt and provided context. This endpoint **must** support streaming HTTP responses for real-time feedback. (Request body: `{ prompt: str, context_text: Optional[str], page_id: Optional[int] (for page context), model_alias: Optional[str] (to select which configured LLM to use, defaults to `CC_LLM_DEFAULT_MODEL_ALIAS`) }`).
        *   `POST /explain-code/`: Request an explanation for a provided code snippet. (Request body: `{ code_block_uuid_in_page: Optional[str] (if referencing an existing snippet on a page), page_id: Optional[int] (for context if using UUID), language: str (e.g., "python", "javascript"), code_content: str (sanitized code string), model_alias: Optional[str] (defaults to `CC_LLM_DEFAULT_CODE_MODEL_ALIAS`) }`).
        *   `POST /generate-diagram-code/`: Generate diagram code (e.g., Mermaid syntax) from a textual description. (Request body: `{ prompt: str, diagram_type: "mermaid" (currently only mermaid supported), model_alias: Optional[str] }`). Response will include the generated (and validated) code.
    *   **Search (`/api/search/`):**
        *   `GET /?q={query_string}&space_key={optional_space_key}&tag_name={optional_tag_name}&page={page_number}&limit={page_size}`: Perform full-text search across pages with support for pagination and filtering by space or tags.
    *   **Tags (`/api/tags/`):**
        *   `GET /`: List all available tags (potentially with usage counts, paginated, filterable by name).
        *   `POST /`: Create a new tag (permissions may be restricted to administrators or Space Administrators).
        *   `GET /?page_id={page_id}`: List tags currently associated with a specific page.
        *   `POST /pages/{page_id}/tags/`: Add an existing tag to a page (request body: `{ tag_id_or_name: str|int }`).
        *   `DELETE /pages/{page_id}/tags/{tag_id_or_name}/`: Remove a tag from its association with a page.
    *   **Permissions (`/api/permissions/` - actual paths might follow `django-guardian` patterns or be custom viewsets for managing object permissions):**
        *   Endpoints for listing current permissions and granting/revoking roles for users or groups on specific `Space` objects (e.g., `GET /spaces/{space_key}/permissions/`, `POST /spaces/{space_key}/permissions/assign_role/`). These are typically restricted to Space Admins for their own space or superusers globally.
    *   **System & Health:**
        *   `GET /api/health/`: Basic application health check endpoint (checks DB, Redis connectivity).
        *   `GET /api/schema/openapi.yaml` (or `/api/schema/` which might serve Swagger UI/Redoc): Serves the OpenAPI 3.0 specification document.

---

**XIV. Phases & Timeline (~9.5-11 Months Total Estimated Duration)**

*   *Note: These timelines are estimates and can vary based on actual team velocity, unforeseen complexities, and the depth of implementation for each feature. Regular review and adjustment of the plan and timeline will be necessary throughout the project lifecycle.*

**Phase 1: Core Engine, Data Models, Authentication, Basic Editor & Foundational Setup (8 Weeks)**
*   **Goal:** Establish a functional foundational system for the application. This includes setting up the core backend and frontend projects, implementing basic user authentication and authorization structures, defining the core data models for spaces and pages (using ProseMirror JSON for content), integrating a very basic Tiptap editor for creating and viewing simple rich text, configuring `django-storages` for attachment handling (local backend initially), setting up Celery with Redis for future asynchronous tasks, implementing initial API specification generation, and establishing basic error tracking and logging.
*   **Key Deliverables & Tasks:**
    1.  **Project Initialization & Setup:**
        *   Initialize Django backend project with suggested app structure (Section I.A).
        *   Initialize React frontend project using Vite and TypeScript.
        *   Set up local development environment with PostgreSQL database and Redis instance.
        *   Initialize Git version control repository with appropriate branching strategy (e.g., Gitflow).
    2.  **Data Models & Initial Migrations:**
        *   Implement Django ORM models for `User` (using standard Django User), `Space`, `Page` (including `schema_version` field), `PageVersion` (including `schema_version` field), `Attachment` (including `scan_status` field), `Tag`, and `FallbackMacro`.
        *   Create and apply initial database migrations for these models.
        *   (Soft delete fields like `is_deleted`, `deleted_at` will be defined in the models during this phase, but the full logic and UI for soft delete management will be deferred to Phase 3).
    3.  **Core APIs (CRUD Basics):**
        *   Develop basic CRUD (Create, List, Retrieve) API endpoints using Django REST Framework for:
            *   Users: Self-service registration (with email verification stubs/planning), login, retrieving current user details.
            *   Spaces: Create, list (all accessible), retrieve basic details of a single space.
            *   Pages: Create a new page (associating with a space, saving basic `raw_content`), retrieve basic details of a single page, initial update logic for `raw_content`.
    4.  **Authentication & Initial Authorization Setup:**
        *   Implement JWT-based authentication using `djangorestframework-simplejwt`. Configure JWT settings for security (e.g., `ROTATE_REFRESH_TOKENS = True`, `BLACKLIST_AFTER_ROTATION = True`).
        *   Set up `django-guardian` and define initial application roles (e.g., `SpaceAdmin`, `Editor`, `Viewer`). Implement basic permission checks for the core CRUD API actions created above (e.g., ensuring only authenticated users can create spaces/pages, and basic ownership checks if applicable). Defer the UI for detailed permission management to a later phase.
        *   Implement self-service user registration (with planning for email verification flow) and password reset functionality (with planning for email sending flow) using Django's built-in authentication system or by planning the integration of `django-allauth`.
    5.  **Basic Tiptap Editor Integration (Frontend):**
        *   Integrate the Tiptap editor into the React frontend.
        *   Define an initial, minimal version of the `appSchema` (ProseMirror schema) supporting essential nodes like paragraphs, headings (H1-H3), and basic marks like bold and italic, plus basic unordered/ordered lists.
        *   Enable users to create a new page, type simple rich text using these basic features, save the page content (which will be ProseMirror JSON stored in the `raw_content` field), and view this rendered simple rich text.
    6.  **`django-storages` & Celery Setup:**
        *   Configure `django-storages` with a local filesystem backend for development purposes. Implement a placeholder API endpoint stub for attachment uploads (actual file handling and UI deferred).
        *   Set up Celery with Redis as the message broker. Define a simple test Celery task to ensure the infrastructure is working correctly (actual import/scan tasks deferred).
    7.  **Frontend Foundations:**
        *   Set up Zustand for global state management (e.g., to hold authentication state, current user details).
        *   Implement React Router with basic routes for login/registration, a placeholder dashboard or space listing page, and a placeholder page viewing component.
        *   Create initial high-level React layout components (`AppLayout`, `Header`, `Sidebar`) and a basic style guide (colors, typography).
    8.  **Docker Environment:** Create an initial `docker-compose.yml` file for the local development environment. This should include services for the Django web application, React frontend development server (via Vite), PostgreSQL database, and Redis. (Illustrative `docker-compose.yml` structure provided in Section XII).
    9.  **API Specification (Initial Structure with `drf-spectacular`):**
        *   Integrate `drf-spectacular` into the Django project.
        *   Begin structuring the OpenAPI 3.0 specification by adding docstrings and type hints to the initial API views and serializers.
        *   Ensure the initial authentication, space, and page endpoints are documented with example request/response schemas.
    10. **Error Handling & Logging (Initial Setup):**
        *   Implement the basic standardized API error response structure (using the base `APIBaseException` and a custom DRF exception handler). Define an initial set of `ErrorCode` enums for common errors encountered in this phase.
        *   Configure initial Django logging to the console, preferably in a structured JSON format for easier parsing.
        *   Integrate Sentry SDK (or a self-hosted alternative like GlitchTip) for both backend (Django) and frontend (React) basic error tracking. Configure it to report unhandled exceptions.

**Phase 2: Confluence Import, Advanced Editor Nodes & ProseMirror Schema Definition (11 Weeks)**
*   **Goal:** Implement the core Confluence HTML import functionality (including page hierarchy, conversion of common macros, and attachment handling). Significantly enhance the Tiptap editor with all defined custom nodes (code blocks, Mermaid diagrams, Draw.io diagrams) and marks. Fully define, implement, and test the application's ProseMirror schema and its versioning mechanism. Implement the "raw Markdown" editing mode. Implement the full attachment virus scanning pipeline and client-side direct-to-S3 uploads using pre-signed URLs.
*   **Key Deliverables & Tasks:**
    1.  **ProseMirror Schema (`appSchema`) Finalization & Implementation:**
        *   Finalize and implement the comprehensive `appSchema` (as detailed in Section III.A) in the frontend codebase. This includes all standard nodes (image, lists, tables, horizontal rule, etc.), all defined custom nodes (code_block, mermaid_diagram, drawio_diagram, fallback_macro_placeholder), and all marks (link, bold, italic, code, strike, underline).
    2.  **Server-Side Schema Validation & Content Versioning:**
        *   Implement robust logic on the backend (e.g., in the `Page` model's `save` method or within DRF serializer validation) to validate the structure of incoming `raw_content` (ProseMirror JSON) against the current `appSchema` version before saving new page versions to the database.
        *   Ensure the `schema_version` field in `Page` and `PageVersion` models is correctly populated and managed. Plan the detailed strategy for handling content migrations if the `appSchema` evolves in the future (e.g., scripts to transform old content, editor logic to handle older versions).
    3.  **Confluence Importer - Core Logic & Parsing:**
        *   Develop HTML parsing capabilities using `BeautifulSoup4` as the primary tool.
        *   During this phase, if testing with very large real-world Confluence export HTML files reveals significant memory exhaustion issues with BeautifulSoup, investigate and prototype using a SAX parser (e.g., Python's `xml.sax` if applicable) or an incremental HTML parser (`html.parser` with custom event handlers) as an advanced optimization for handling such extreme cases.
        *   Implement the primary HTML-to-ProseMirror JSON conversion logic, ensuring comprehensive mapping of common Confluence elements (headings, paragraphs, all list types, tables with nested content, links, images, formatting) to the `appSchema`.
        *   Implement the fallback HTML -> Markdown (`Turndown`) -> ProseMirror JSON (`Remark`/`prosemirror-markdown`) conversion strategy for identified complex or non-standard HTML elements that are difficult to map directly.
    4.  **Confluence Importer - Specific Features Implementation:**
        *   Implement accurate page hierarchy reconstruction using Confluence export metadata (page IDs, parent IDs). Conduct thorough tests for various hierarchy scenarios (deeply nested structures, pages with many siblings, re-parented pages if discernible from export).
        *   Develop specific converters for common Confluence macros: `{code}` (to Tiptap `code_block`), `[draw.io]` (to Tiptap `drawio_diagram` node with embedded XML), and `[Mermaid]` (to Tiptap `mermaid_diagram` node with embedded syntax). Ensure macro parameters (like code language) are preserved.
        *   Implement the `FallbackMacro` database model and the logic to store data for unsupported Confluence macros, ensuring these are linked to `fallback_macro_placeholder` nodes in the imported page content.
        *   Integrate attachment extraction from the Confluence export and storage via `django-storages`. Ensure attachments are correctly linked or embedded within the imported ProseMirror JSON content.
    5.  **Tiptap Editor - Custom Nodes Full Implementation (with Mobile Touch):**
        *   Fully develop custom Tiptap extensions and/or React Node Views for:
            *   `code_block`: Include client-side UUID generation (using `uuid` library), a language selection dropdown, and integrate `highlight.js` (with `DOMPurify` sanitization of its HTML output if it generates HTML) for syntax highlighting directly within the editor.
            *   `mermaid_diagram`: Provide an input area for Mermaid syntax (e.g., in a modal or an inline editable field within the Node View), and render a preview using `Mermaid.js` within the Node View.
            *   `drawio_diagram`: The Node View should display a placeholder or a static preview. Clicking this placeholder (or an edit button on it) should launch the Draw.io editor in a modal. XML data must be correctly passed to and retrieved from the Draw.io editor.
        *   Implement mobile-friendly touch event handling (e.g., using `touchend` listeners) for these custom node views to ensure good usability and interaction on touch devices (e.g., tapping a diagram node should open its editor).
    6.  **Tiptap Editor - Markdown Mode (Full Implementation):**
        *   Implement the "Raw Markdown Mode" UI toggle in the editor.
        *   Implement robust on-demand conversion logic for both directions (ProseMirror JSON <-> Markdown string) using Tiptap's Markdown extension or `prosemirror-markdown` with custom serializers/parsers aligned with `appSchema`.
    7.  **Attachment Management (Full CRUD & Security Features):**
        *   Develop full API and UI functionality for managing page attachments: uploading new files, listing existing attachments for a page (with metadata like name, size, type, scan status), and deleting attachments.
        *   If an S3-compatible storage backend is configured (`CC_STORAGE_BACKEND="s3"`), implement client-side direct-to-S3 uploads using pre-signed URLs generated by the backend. The client uploads to S3, then notifies the backend to finalize the `Attachment` record.
        *   For all attachment downloads, implement the "Zero-Trust Attachment Download" headers (`Content-Type: application/octet-stream`, `Content-Disposition: attachment`) on the backend API endpoint.
        *   Implement the full asynchronous attachment virus scanning pipeline: the Celery task for scanning, integration with ClamAV (running in a separate Docker container), and updating the `Attachment.scan_status` field. Ensure the UI reflects the scan status and blocks downloads of infected files.

**Phase 3: Interactive Diagrams, Versioning UI & Soft Deletes Implementation (8 Weeks)**
*   **Goal:** Make diagramming features fully interactive and polished within the Tiptap editor. Implement a user-friendly and robust page versioning system with effective diffing capabilities (prioritizing semantic ProseMirror diffs). Fully implement the soft delete functionality for pages and spaces, including UI for management.
*   **Key Deliverables & Tasks:**
    1.  **Interactive Mermaid Diagrams in Editor (Polished):**
        *   Implement a debounced live preview mechanism for the Mermaid Tiptap node. The preview within the Node View should update smoothly and efficiently as the user types or modifies Mermaid syntax.
        *   Develop and fully integrate the `POST /api/diagrams/validate/mermaid/` backend endpoint. The editor should use this to provide real-time (or on-demand, e.g., via a "Validate Syntax" button) validation feedback for the Mermaid syntax directly within the editor interface, highlighting errors if any.
    2.  **Interactive Draw.io Diagrams in Editor (Polished):**
        *   Fully integrate the `draw.io` editor (from `diagrams.net-embed` or a similar library) within a sandboxed `<iframe>`. This iframe will be launched as a modal window when a user interacts with a Draw.io Tiptap node (e.g., creates a new one or clicks to edit an existing one).
        *   Implement robust logic to pass the current diagram's XML data (from the Tiptap node's `xml` attribute) to the iframe editor upon opening it.
        *   Implement reliable logic to receive the updated XML data from the Draw.io editor iframe (e.g., via `window.postMessage` API communication or a callback mechanism provided by the embedding library if available) when the user saves or applies changes within the draw.io UI. This updated XML is then saved back to the Tiptap node's `xml` attribute, triggering an editor update.
        *   Ensure strict CSP and `sandbox` attributes are correctly applied and tested for the Draw.io editor iframe to maximize security.
    3.  **Page Versioning UI & Diffing Implementation:**
        *   Develop the user interface to list all historical versions of a page. This list should clearly display the version number, the author of that version, the timestamp of its creation, and any commit message provided by the user when that version was saved.
        *   Allow users to easily select any past version from this list and view its fully rendered content as it appeared at that point in time (read-only view).
        *   Implement visual diffing between two selected page versions (e.g., comparing the current version with the immediately preceding one, or comparing any two arbitrary historical versions).
            *   **Primary approach:** Utilize a ProseMirror-native diffing library (e.g., **`prosemirror-changeset`** or a similar stable and well-maintained tool compatible with the version of ProseMirror used by Tiptap). Such libraries can compute a semantic diff of the two ProseMirror JSON document structures, identifying structural changes (like paragraph splits/merges, list item movements, table modifications), text content changes, and attribute changes (like heading level changes, link URL modifications) more accurately than a plain text or simple HTML diff. This semantic diff can then be rendered into a user-friendly visual comparison (e.g., using different background colors for added/deleted/modified blocks or inline content, or a side-by-side view with clear change markers).
            *   **Fallback approach (if semantic diffing proves too complex for MVP):** If a suitable ProseMirror-native diffing library is not readily available, is immature, or its integration proves too time-consuming for the MVP, the fallback strategy will be to render the HTML output of both selected page versions (by converting their respective ProseMirror JSON `raw_content` to HTML using Tiptap/ProseMirror's rendering logic on the client-side) and then use a client-side library like `diff-match-patch` (or a React wrapper around it) to compute and display the differences between these two rendered HTML strings. While less semantically precise, this still provides a useful visual comparison, especially for textual changes.
        *   Implement functionality for users to revert a page's current content to that of a selected previous historical version. This "revert" action will itself create a new current `PageVersion` reflecting the state of the chosen older version (and noting the revert action in its commit message), rather than overwriting the existing current version or deleting intermediate versions. This ensures that the full, unbroken, auditable history of the page is always preserved.
    4.  **Soft Deletes Implementation (Full Logic & UI):**
        *   Implement the complete backend logic for soft deleting `Page` and `Space` objects (setting the `is_deleted=True` flag and populating the `deleted_at` timestamp field upon a "delete" action through the API).
        *   Ensure all standard API queries that list or retrieve pages and spaces (e.g., listing pages in a space, search results, navigation trees) correctly and consistently filter out soft-deleted items by default, unless an explicit parameter is provided to include them (e.g., for an admin view). This will be handled by custom default managers on the Django models.
        *   Develop UI functionality (e.g., in an administration panel accessible to superusers, or a "Trash" / "Recycle Bin" feature within space settings accessible to Space Admins) for users with the necessary privileges to:
            *   View a list of soft-deleted items (pages and/or spaces) within their scope of authority.
            *   Permanently delete selected soft-deleted items from the database (a "hard delete" operation, which will be clearly marked as irreversible and may require a confirmation step).
            *   Restore selected soft-deleted items, which would involve setting their `is_deleted` flag back to `False` and clearing the `deleted_at` timestamp. Restored items would then reappear in normal application listings and search results.

**Phase 4: LLM AI Integration & Advanced Security Hardening (9 Weeks)**
*   **Goal:** Integrate core AI-assisted authoring features using local CPU-friendly LLMs. Implement robust security measures for all LLM interactions (input sanitization, output validation, circuit breakers, rate limiting). Further harden overall application security with features like Content Security Policy (CSP) Nonces. Explore and implement performance optimizations for diagram rendering, such as using Web Workers.
*   **Key Deliverables & Tasks:**
    1.  **LLM Backend Setup & Interaction Logic (Full Implementation):**
        *   Fully integrate `llama-cpp-python` for direct interaction with GGUF quantized models (e.g., Mistral, CodeLlama). Configure model paths (`CC_LLM_MODEL_PATH`, model mapping from aliases to paths/quantization levels), and GPU offloading parameters (`CC_LLM_N_GPU_LAYERS`).
        *   Ensure the fallback support for interacting with an Ollama HTTP API (if `CC_LLM_PROVIDER` is set to `ollama_http`) is functional and tested.
        *   Develop and refine LangChain components (prompt templates optimized for chosen models and specific tasks, chains for orchestrating sequences of operations, custom output parsers for extracting structured data from LLM responses) for all defined AI features.
        *   Implement a circuit breaker pattern (using `pybreaker` or a similar library) for all calls to LLM services (both `llama-cpp-python` and Ollama API calls) to enhance application resilience against LLM service unavailability or persistent errors.
    2.  **LLM API Endpoints with Streaming & Advanced Security:**
        *   Implement the backend API endpoints for all defined LLM features:
            *   "Generate Text" (`POST /api/llm/generate-text/`): This endpoint **must** support streaming HTTP responses (e.g., using Django's `StreamingHttpResponse` or Django Channels) to provide real-time feedback to the user as text is generated.
            *   "Explain this Code" (`POST /api/llm/explain-code/`).
            *   "AI-assisted Mermaid Diagram Generation" (`POST /api/llm/generate-diagram-code/`).
        *   Implement rigorous input sanitization (including advanced prompt injection defense techniques as detailed in Section VI.B) for all data passed from users or application context to LLMs via these API endpoints.
        *   Implement robust validation for LLM outputs:
            *   Generated Mermaid syntax from the "AI-assisted Mermaid Diagram Generation" feature will be validated using the internal validation logic (shared with the `/api/diagrams/validate/mermaid/` endpoint). Only valid syntax will be returned or saved.
            *   Other structured outputs (if any) will be parsed and their schema validated.
    3.  **LLM Frontend Integration (UI & Streaming):**
        *   Develop user interface elements within the Tiptap editor (e.g., context menus, toolbar buttons, potentially slash commands) or page view to trigger the defined AI features.
        *   Implement frontend logic to robustly consume streaming HTTP responses for text generation features, displaying text to the user token by token (or in small chunks) as it's generated for an improved, interactive user experience.
    4.  **LLM Security, Performance & Caching Strategies:**
        *   Implement and configure `django-ratelimit` for all LLM API endpoints with appropriate, configurable rates (e.g., per user per hour) to prevent abuse and manage resource consumption.
        *   Implement Redis caching for appropriate LLM responses (e.g., explanations for identical code snippets, validated diagram generations from identical prompts) using semantic cache keys (hashes of key prompt components and context elements) to maximize cache effectiveness.
        *   Benchmark LLM performance (latency, resource usage, tokens/second) on representative low-end hardware (e.g., Raspberry Pi 4/5 equivalent with sufficient RAM for target models like 7B Q4_K_M) and document findings. This includes establishing realistic performance expectations and minimum/recommended hardware specifications for users intending to self-host and utilize these features.
        *   **Controlled HTML Output from LLM (if pursued):** If any feature involves LLMs generating simple, controlled HTML snippets, implement their rendering within a sandboxed `<iframe>` using the `srcdoc` attribute, as detailed in Section VI.C. Ensure the HTML content for `srcdoc` is either generated by trusted backend logic that tightly constrains the LLM's output or is heavily sanitized with a very strict allowlist if based on more free-form LLM output.
    5.  **Advanced Security Hardening (CSP Nonces Implementation):**
        *   Implement the full Content Security Policy (CSP) with Nonces for all inline scripts and styles. This requires backend logic (e.g., a Django middleware) to generate a unique nonce for each HTTP request, inject this nonce into relevant `<script>` and `<style>` tags in server-rendered HTML templates (primarily the main SPA shell), and include the nonce in the CSP header sent with the response.
    6.  **Diagram Rendering Performance Optimizations (Web Workers for Mermaid):**
        *   For Mermaid diagram previews within the Tiptap editor, if initial testing during Phase 3 or early Phase 4 identifies significant performance issues (editor lag, UI unresponsiveness) when rendering very large or computationally complex Mermaid diagrams, explore and implement using a **Web Worker** to offload the `Mermaid.js` rendering task from the main UI thread.
        *   This Web Worker implementation should include logic for passing the Mermaid syntax to the worker, receiving the rendered SVG (or an error message) back from the worker, and updating the preview in the Node View. It must also include robust cancellation logic for ongoing render tasks in the worker if the diagram syntax is changed again by the user before the current render completes, to prevent stale previews and wasted computation.

**Phase 5: Search Implementation, Self-Hosting Polish, Advanced Permissions UI & General Caching (7 Weeks)**
*   **Goal:** Implement robust full-text search functionality with fuzzy matching and result highlighting. Refine the application for easy and secure self-hosting by users. Develop a user interface for managing detailed Role-Based Access Control (RBAC) via `django-guardian`. Implement general application caching strategies for improved overall performance.
*   **Key Deliverables & Tasks:**
    1.  **Search Implementation (PostgreSQL FTS - Full Featured):**
        *   Configure and utilize PostgreSQL Full-Text Search features comprehensively: ensure the `SearchVectorField` on the `Page` model is populated correctly and updated on page saves. Ensure appropriate GIN indexing is in place.
        *   Ensure the `pg_trgm` PostgreSQL extension is enabled and correctly integrated (e.g., using `TrigramSimilarity` or the `%` operator in Django ORM queries) to provide effective fuzzy search capabilities for handling misspellings or variations in search terms.
        *   Implement server-side generation of search result snippets with highlighted matching terms using PostgreSQL's `ts_headline` function (via Django ORM's `SearchHeadline` annotation).
        *   Develop the full frontend search UI: a global search bar (likely in `AppHeader`), a dedicated search results page (`/search?q=...`) with clear display of results (title, highlighted snippet, metadata like author/date/space), robust pagination for search results, and filtering options (e.g., filter by a specific `Space`, by one or more `Tags`).
    2.  **Self-Hosting Polish & Comprehensive Documentation:**
        *   Finalize the `docker-compose.yml` file for production-like deployments. This should include clear instructions and examples for configuring all services (Django web app, Nginx reverse proxy, PostgreSQL, Redis) and provide detailed guidance for setting up local LLM services if users opt to use Ollama as a separate container (including necessary network configurations between containers). Include best practices for volume mounts for persistent data.
        *   Write comprehensive installation, configuration (detailing all environment variables, their purpose, default values, and example configurations), and deployment guides for self-hosting users on various common platforms (e.g., a generic Linux server using Docker and Docker Compose).
        *   Document all environment variables meticulously in the `.env.example` file and in the main administrator documentation.
    3.  **Advanced Permissions UI (`django-guardian` Management):**
        *   Develop a user interface, likely within an administration section of the application (accessible to superusers) or integrated into space settings pages (accessible to Space Admins for their own spaces), for managing `django-guardian` object-level permissions.
        *   This UI should allow authorized users to:
            *   View current permissions on a `Space` (and potentially on individual `Page` objects if page-level overrides are heavily used, though space-level is primary for MVP).
            *   Assign users or Django groups to the defined roles (e.g., `SpaceEditor`, `SpaceViewer`) for specific `Space` objects.
            *   Revoke roles from users or groups for specific `Space` objects.
    4.  **General Application Caching Implementation (Redis):**
        *   Implement Redis caching for frequently accessed and computationally expensive API responses or data segments to improve overall application performance and reduce database load. Examples of data to cache:
            *   Results of common or popular search queries.
            *   Rendered content (or parts of it, if feasible) of very popular, infrequently changing pages (this needs careful consideration of cache invalidation).
            *   Aggregated data for dashboards or space overviews (e.g., page counts, list of recently updated pages, space statistics).
        *   Implement appropriate cache keys and cache invalidation strategies (e.g., time-based expiry, event-based invalidation when underlying data changes).
        *   Review and optimize general application performance. Continue to identify and address any remaining database query bottlenecks using tools like `django-debug-toolbar` (in dev) and `EXPLAIN ANALYZE`.
    5.  **Tagging/Labeling (Full UI Implementation):** Implement the full user interface for managing tags:
        *   UI for creating new tags (if allowed globally or by administrators/Space Admins).
        *   UI for assigning existing tags to pages (e.g., via an autocomplete input field in the page editor interface or page settings panel).
        *   UI for removing tags from pages.
        *   UI elements for browsing content by selecting a specific tag (e.g., a page listing all pages with a given tag).
        *   Integration of tag filtering into the main search results page.

**Phase 6: Comprehensive Testing, Documentation Finalization, Deployment Strategy & Release Preparation (9 Weeks)**
*   **Goal:** Ensure the application is exhaustively tested across all functional and non-functional aspects (performance, security, accessibility, usability), all user, administrator, and developer documentation is complete, accurate, and user-friendly, and the application is fully prepared for an initial production release or wider user adoption.
*   **Key Deliverables & Tasks:**
    1.  **Testing Strategy Execution (Comprehensive & Rigorous):**
        *   **Unit Tests:**
            *   Backend (Python/Django): Write and execute unit tests using `pytest` and `pytest-django` for all critical business logic, models, services, API view logic, utility functions, and helper classes. Aim for a code coverage target of **>85%** for these critical modules.
            *   Frontend (React/TypeScript): Write and execute unit tests using Jest & React Testing Library (`@testing-library/react`) for complex UI components (especially those with significant logic or state), utility functions, and Zustand stores/actions. Aim for a code coverage target of **>70%** for critical UI logic.
        *   **Integration Tests:**
            *   Backend: Develop and run integration tests for API endpoints, focusing on testing full request/response cycles, authentication flows (JWT handling), authorization logic (with `django-guardian` permission checks for different user roles), data integrity with database interactions, and interactions between different service layers (e.g., page service calling attachment service).
            *   Frontend: Test interactions between major React components, integration with Zustand state management, and correct consumption and handling of API service layer responses.
        *   **End-to-End (E2E) Tests:** Create, execute, and maintain a comprehensive E2E test suite using **Playwright** (preferred for its robustness and features like auto-waits and multi-browser support) or Cypress. This suite should cover at least **20-25 core user workflows**, including but not limited to:
            *   User registration (including email verification flow if implemented) and login/logout.
            *   Space creation, viewing, and basic settings modification.
            *   Page creation, editing (with various content types like tables, code blocks, Mermaid diagrams, Draw.io diagrams), and viewing.
            *   Performing a search with various criteria and verifying results and highlighting.
            *   Page versioning: creating multiple versions, viewing history, diffing two versions, and successfully restoring a previous page version.
            *   Basic permissions checks (e.g., an editor can edit a page in their space, a viewer cannot; an admin can delete a space).
            *   Attachment upload (both direct and S3 pre-signed URL if applicable), download, and checking virus scan status display.
            *   Usage of key LLM features (e.g., trigger text generation, explain a code block, generate a Mermaid diagram) and verify basic output or streaming behavior.
            *   Confluence import of a small, representative sample dataset to verify core import functionality.
        *   **Performance Benchmarks & Load Testing:**
            *   Conduct performance tests against the defined validation metrics (see Section XVI) for page load times (LCP, TTI), API response latencies (average, p95, p99 for key endpoints), and LLM task completion times. Use tools like browser developer tools, WebPageTest, or custom scripting.
            *   Perform load testing using tools like `k6` or `Locust`. Simulate a target of at least **100-200 concurrent users** (for MVP; 500+ is a stretch goal for later, enterprise-grade testing) performing a realistic mix of read (viewing pages, searching) and write (creating/editing pages, adding attachments) operations for a sustained period (e.g., 30-60 minutes). Monitor system resources (CPU, memory, DB load, Redis performance, API error rates, latencies) during load tests to identify bottlenecks and assess stability under pressure.
        *   **Usability Testing:** Conduct further internal usability testing sessions with team members acting as users. If possible, recruit a small group of external beta testers (representative of the target user base) to use the application and provide feedback on the overall user experience, ease of use, clarity of the UI, intuitiveness of features, and any pain points. Incorporate critical feedback.
        *   **Accessibility Audit & Automated Scans:**
            *   Implement automated accessibility scans in the CI/CD pipeline using **`axe-core`** integrated with E2E tests (e.g., via `axe-playwright` or `cypress-axe`). Aim for zero critical or serious WCAG 2.1 AA violations on all critical user paths (login, page view, page edit, search, space navigation).
            *   Supplement automated scans with **manual accessibility reviews** of key application workflows. This should include testing with keyboard-only navigation, using common screen readers (e.g., NVDA on Windows, VoiceOver on macOS/iOS, TalkBack on Android) for core tasks, checking color contrast with tools, and ensuring logical heading structure and form accessibility. Strive for demonstrating strong effort towards WCAG 2.1 Level AA compliance on all critical paths.
        *   **Security Testing (Final Review & Hardening):**
            *   Conduct a final comprehensive review of all security implementations: CSP (including nonces and `report-uri` if configured), input validation (API request bodies, query params, LLM prompts), output sanitization (`DOMPurify` for frontend HTML, backend sanitization of LLM outputs), permissions (`django-guardian` policies), authentication mechanisms (JWT rotation/blacklisting), API rate limiting, attachment handling security (virus scan, zero-trust download headers).
            *   Review Sentry (or alternative error tracker) for any recurring security-related errors or warnings that might indicate vulnerabilities.
            *   Consider engaging a third-party security consultant for a basic penetration test or vulnerability assessment if budget and time allow before a significant public release, or plan for this as a high-priority post-MVP activity.
        *   **Cross-Browser/Cross-Device Testing:** Verify core functionality, layout consistency, and display on the latest stable versions of major web browsers (Google Chrome, Mozilla Firefox, Apple Safari, Microsoft Edge) and on representative mobile devices (iOS Safari on iPhone/iPad, Android Chrome on various Android phones/tablets) or using browser developer tool emulators.
    2.  **Documentation Finalization (All Categories - User, Administrator/Self-Hosting, Developer):**
        *   **User Documentation:** Review, update, and finalize all user-facing guides to ensure they are clear, concise, accurate, comprehensive, and easy to understand for non-technical users. This includes: "Getting Started Guide," "Using the Editor (Rich Text Features, Markdown Mode, Working with Mermaid Diagrams, Using the Draw.io Integration, Formatting Code Blocks, Using Custom Nodes)," "AI-Assisted Authoring Features (How to use Text Generation, Code Explanation, Diagram Generation; Tips for writing effective prompts)," "Managing Pages and Spaces (Creating Pages/Spaces, Organizing Content with Hierarchy, Deleting/Restoring Content)," "Effective Searching and Filtering Content," "User Profile & Account Settings (Password changes, email management)."
        *   **Administrator/Self-Hosting Documentation:** Review, update, and finalize all documentation crucial for users setting up, configuring, maintaining, and troubleshooting their own instance of the application. This includes:
            *   "Complete Installation and Setup Guide (Covering Docker Compose deployment, manual installation steps if any are supported, system dependencies)."
            *   "Detailed Configuration Options (Explanation of all environment variables and their impact on application behavior)."
            *   "Setting up and Configuring Local LLM Services (Step-by-step guide for `llama-cpp-python` including model downloading and placement; guide for Ollama setup if users choose that route)."
            *   "Comprehensive Backup and Restore Procedures (Detailed steps for PostgreSQL, Attachments on local FS/S3, Redis AOF/RDB, Configuration files, Docker volumes). Emphasize the importance of regular DR testing."
            *   "Troubleshooting Common Issues & FAQs (Based on issues found during testing and anticipated user problems)."
            *   "Application Upgrading Guide (Clear steps for updating to new versions of the application, including database migration handling and any breaking changes)."
            *   A list of standardized error codes (from the `ErrorCode` enum) with their explanations and links to relevant sections of the `doc_url` site.
            *   Guidance on achieving near-zero-downtime upgrades (e.g., concepts of Blue-Green deployment, database migration considerations for compatibility).
        *   **Developer Documentation (For internal team or future open-source contributors):**
            *   Finalize and publish the API documentation (via the generated OpenAPI specification, accessible through Swagger UI/Redoc). Include supplementary guides on authentication mechanisms, API rate limits, and conventions.
            *   Provide a comprehensive architecture overview document (including high-level diagrams of key components like Frontend SPA, Django Backend API, PostgreSQL DB, Redis, Celery, LLM Service, Object Storage, and their primary interactions - e.g., using C4 model Level 1 & Level 2 diagrams).
            *   Document the frontend component library/structure (if a significant custom library is built).
            *   Provide clear guidelines for setting up a local development environment.
            *   Document coding standards and conventions (Python using Ruff/Black, TypeScript using ESLint/Prettier).
            *   Define the Git branching strategy (e.g., Gitflow) and pull request process.
            *   Outline instructions on how to contribute to the project (issue reporting, feature requests, code contributions).
        *   "Importing Confluence Data: Detailed Guide (List of officially supported Confluence versions for import, detailed explanation of import capabilities for different Confluence content types and macros, known limitations and common workarounds, best practices for users preparing their Confluence exports for optimal import results, troubleshooting common import errors)."
    3.  **Performance Optimization & Final Bug Fixing:** Address any remaining significant performance bottlenecks or critical/major bugs identified during the comprehensive testing phase. Perform final optimizations on database queries, frontend rendering paths, and server-side logic as needed based on profiling data and load test results.
    4.  **Deployment Preparation & Release Management:**
        *   Prepare the codebase for an initial official release (e.g., tagging as v1.0.0 in Git). Create a detailed changelog summarizing all new features, significant improvements, and bug fixes included in this release since the last major development milestone or beta.
        *   Finalize all build scripts for both frontend (Vite production build) and backend applications (any pre-deployment checks or static file collection steps).
        *   Build and thoroughly test the final Docker images for all application services (web server, Celery workers, Nginx if used).
        *   If the project is intended to be open source, prepare the GitHub repository (or other VCS platform) for public release. This includes:
            *   A comprehensive `README.md` file at the project root (providing a project overview, key features, quick start guide for installation/running, links to the full documentation site, license information, and contribution pointers).
            *   A clear `LICENSE` file (e.g., MIT, Apache 2.0, AGPLv3 - the choice of license depends on the project's goals and community engagement strategy).
            *   Contribution guidelines (`CONTRIBUTING.md`) detailing how others can contribute to the project (code style, pull request process, issue reporting standards, development setup).
            *   A Code of Conduct (`CODE_OF_CONDUCT.md`) to foster a welcoming and inclusive community.
        *   Draft initial release notes for the first public release, highlighting key features, benefits for users, known issues or limitations in v1.0.0, and directions for getting support or providing feedback.

---

**XV. Risk Mitigation (Consolidated & Refined)**

*   **Confluence Import Complexity & Fidelity:**
    *   **Mitigation:** Prioritize robust parsing of common Confluence HTML structures for direct HTML-to-ProseMirror conversion. Utilize the `FallbackMacro` system to ensure no content is lost for unhandled/custom macros, preserving original content for manual review or future converter development. Clearly document importer limitations and supported Confluence versions. Rigorously test page hierarchy reconstruction using diverse Confluence export structures. For extremely large individual HTML files (a rare edge case), the plan includes investigating SAX/incremental parsing as an advanced optimization if memory issues arise with BeautifulSoup during import processing. Plan for ongoing iterative improvements to the importer post-launch based on user feedback and analysis of problematic exports.
*   **Diagram Data Integrity, Performance & Usability:**
    *   **Mitigation:** Embed diagram source data (Mermaid syntax, Draw.io XML) directly within ProseMirror JSON nodes; this ensures data is versioned consistently with the page content. Implement client-side suggestions or soft limits for diagram node data size to guide users and prevent extreme editor performance issues. Use debounced live previews for Mermaid diagrams in the editor to balance responsiveness with performance. Ensure the draw.io editor iframe is performant and securely sandboxed with a strict Content Security Policy. Implement touch-friendly interactions for diagram nodes on mobile devices. For performance with very large diagrams, explore using Web Workers for client-side Mermaid rendering and lazy-loading strategies (or static image previews with on-demand full load) for large Draw.io diagrams in page view mode.
*   **ProseMirror Schema Evolution & Content Integrity:**
    *   **Mitigation:** Implement schema versioning from the outset by including a `schema_version` field on both `Page` and `PageVersion` models. For any future breaking changes to the `appSchema` (the editor's ProseMirror schema), a documented migration path must be defined and implemented. This includes developing scripts (e.g., Django management commands) capable of transforming existing stored content from older schema versions to the new version, and ensuring the editor can gracefully handle (and ideally, guide the migration of or auto-migrate on save) content created with older schema versions.
*   **ProseMirror JSON <-> Markdown Conversion Fidelity:**
    *   **Mitigation:** Adopt an on-demand conversion strategy for the "raw Markdown mode" and for Markdown file import/export. This approach, where conversion happens only when the user explicitly switches editing modes or initiates an import/export operation, minimizes the risks and complexities associated with attempting live, bi-directional synchronization between WYSIWYG and Markdown views. Thoroughly test conversions with a wide range of common GitHub Flavored Markdown (GFM) features using robust libraries like `prosemirror-markdown` or Tiptap's built-in Markdown extension, configured to align with the `appSchema`.
*   **Security Vulnerabilities (XSS, Injection, Data Exposure, etc. - Multi-Layered Defense):**
    *   **Mitigation:**
        *   **Permissions & Access Control:** Employ `django-guardian` for robust, fine-grained object-level permissions, ensuring users can only access and modify data they are authorized for.
        *   **Content Security Policy (CSP):** Enforce a strict Content Security Policy (CSP) with Nonces for inline scripts and styles to mitigate XSS.
        *   **Output Sanitization (Frontend):** Sanitize HTML output from client-side libraries like `highlight.js` using `DOMPurify` before rendering. Rely on ProseMirror's schema-driven rendering for the main page content, which prevents injection of arbitrary HTML not defined in the schema.
        *   **Input Validation (API):** Rigorously validate all API inputs (request bodies, query parameters, path parameters) on the backend using Django REST Framework serializers and potentially Pydantic for complex data structures.
        *   **LLM Security:** Implement multi-layered sanitization for all inputs to LLMs (including length limits, control character stripping, HTML tag removal for plain text contexts, and research/filtering of known prompt injection patterns). Validate and sanitize LLM outputs before rendering or processing (e.g., programmatically validate generated Mermaid syntax, render any controlled LLM-generated HTML snippets within sandboxed iframes with strict `srcdoc` sanitization).
        *   **Rate Limiting:** Implement API rate limiting (`django-ratelimit`) for sensitive and resource-intensive endpoints, especially authentication and LLM APIs, to prevent abuse and DoS.
        *   **Data Safety & Integrity:** Utilize soft deletes for pages and spaces to prevent accidental data loss, with a clear process for restoration or permanent deletion by authorized users. Implement schema versioning for ProseMirror content.
        *   **General Web Security Best Practices:** Adhere to Django security best practices: enforce HTTPS in all production deployments, enable all relevant Django security middleware (CSRF protection, HSTS, XSS headers, clickjacking protection), manage all secrets and sensitive configurations exclusively via environment variables, keep all software dependencies (backend and frontend) regularly updated and patched for known vulnerabilities using automated scanning tools.
        *   **JWT Security:** Implement secure JWT handling, including short-lived access tokens, refresh token rotation, and blacklisting of used refresh tokens.
        *   **Attachment Security:** Implement "Zero-Trust Attachment Downloads" (forcing download with a generic MIME type like `application/octet-stream` and `Content-Disposition: attachment`) and implement an asynchronous virus scanning pipeline for all uploaded files using an engine like ClamAV.
*   **LLM Hallucination, Performance, Setup Complexity, and Safety:**
    *   **Mitigation:**
        *   **Model Choice & Interaction Method:** Prioritize CPU-friendly LLM inference using GGUF quantized models via `llama-cpp-python` for easier self-hosting by users with diverse hardware. Provide clear guidance on selecting appropriate quantization levels (e.g., Q4_K_M, Q5_K_M) based on resource availability and desired quality/performance trade-off.
        *   **Prompt Engineering & Orchestration:** Utilize LangChain for advanced prompt engineering techniques (e.g., providing clear system prompts, using few-shot examples if beneficial for certain tasks, designing prompts for structured output where applicable), effective context window management, and robust output parsing to improve the accuracy and reliability of LLM responses and reduce the likelihood of hallucinations.
        *   **User Experience (UX) for LLM Features:** Implement streaming HTTP responses for all LLM text generation features to significantly improve perceived performance and provide a more interactive user experience. Clearly label all AI-generated content within the UI and always provide users with the ability to easily edit, accept, or reject AI suggestions or generated content.
        *   **Performance & Resource Management for LLMs:** Cache LLM responses in Redis using semantic cache keys to reduce redundant computations for similar requests. Implement a circuit breaker pattern for LLM service calls to enhance application resilience against temporary LLM unavailability or persistent errors.
        *   **Documentation & Guidance for Self-Hosters:** Provide comprehensive documentation on hardware requirements (CPU, RAM, disk space for models, optional GPU with VRAM estimates for offloading), detailed setup procedures for local LLM services (including guidance on downloading GGUF models and configuring `llama-cpp-python` or Ollama), and realistic expectations for performance on representative hardware configurations.
*   **Application Performance (Large Pages, High User Load, Search Functionality):**
    *   **Mitigation:**
        *   Optimize client-side rendering, particularly for potentially complex elements like diagrams (e.g., using Web Workers for Mermaid rendering, lazy-loading for large Draw.io diagrams in view mode).
        *   Optimize Tiptap editor configuration for performance, especially when dealing with very large documents, if any issues are identified during testing.
        *   Utilize PostgreSQL Full-Text Search efficiently with the `pg_trgm` extension for fuzzy matching and ensure appropriate GIN indexing for the `SearchVectorField` to optimize search query performance.
        *   Implement Redis caching broadly and strategically: for frequently accessed API responses, popular search queries and their results, potentially for pre-rendered components or aggregated data for dashboards if they are computationally intensive to generate.
        *   Rigorously optimize database queries throughout the development lifecycle. Use Django ORM features like `select_related` and `prefetch_related` to avoid N+1 query problems. Analyze query plans for slow queries using PostgreSQL's `EXPLAIN ANALYZE` command and add custom database indexes where necessary.
        *   Implement pagination for all potentially long lists displayed in the UI (e.g., lists of pages within a space, search results, page version history, lists of tags) to ensure fast initial load times and manageable data transfer.
        *   Conduct load testing during Phase 6 to proactively identify and address performance bottlenecks under simulated user load before a production release.
*   **Complexity of Direct HTML-to-ProseMirror Conversion (During Confluence Import):**
    *   **Mitigation:** Acknowledge this as a significant technical challenge due to the potential variability and complexity of Confluence's HTML output (especially with user-generated content and macros). Allocate sufficient development and testing time for the importer. Start by targeting the most common and well-structured Confluence HTML patterns for direct conversion. If Confluence's HTML output proves extremely varied, inconsistent across different Confluence versions, or heavily reliant on complex custom user macros that are difficult to parse, be prepared to lean more heavily on the HTML -> Markdown -> ProseMirror JSON fallback pathway for those particularly problematic elements as an initial, more pragmatic solution. The primary goal is robust handling of the majority of typical content, ensuring data is not lost, rather than aiming for perfect pixel-for-pixel conversion of every conceivable edge case from Confluence's proprietary format. Maintainability and robustness of the importer are key concerns.
*   **Scope Creep & Timeline Management:**
    *   **Mitigation:** Adhere strictly to the defined features and tasks outlined within each phase of this project plan. Maintain a clear distinction between features essential for the MVP (Minimum Viable Product) and those designated as post-MVP enhancements (clearly marked with **[Post-MVP Consideration]** or **[Out of Scope for MVP]** in this document). Use a project management tool (e.g., Jira, Trello, GitHub Issues with milestones and proper labeling) to track progress, identify and manage issues, and monitor dependencies. Conduct regular (e.g., weekly or bi-weekly) progress review meetings with the development team, comparing actual progress against the planned timeline. Any proposed changes to the scope (new features, significant alterations to existing features) must go through a formal change request process. This process should evaluate the impact of the proposed change on the project timeline, required resources, existing priorities, and overall project goals before a decision is made to incorporate it. Be prepared to make tough prioritization decisions if delays occur in one area, to protect the MVP timeline for core, essential features.

---

**XVI. Validation Metrics (Targets for MVP Release)**

*   **Confluence Import:**
    *   **Page Content & Structure Fidelity:** Successfully import >90% of pages from a moderately complex Confluence space export (e.g., a representative test suite of 100-500 pages containing diverse content types like text, tables, lists, headings, and embedded images) with key content and basic inline formatting (bold, italic, links) preserved with high fidelity when rendered in the application.
    *   **Page Hierarchy Reconstruction Accuracy:** Achieve >99% accuracy in reconstructing the original parent-child page relationships from the Confluence export metadata for all pages in the test suite.
    *   **Macro Conversion Success Rate:** Common Confluence macros (`{code}`, `[draw.io]`, `[Mermaid]`) are correctly converted to their native application equivalents (Tiptap nodes with appropriate data) in >95% of encountered instances within the test suite. Unsupported macros are correctly identified and result in `FallbackMacro` database records and corresponding `fallback_macro_placeholder` nodes in the page content.
    *   **Attachment Handling Fidelity:** >95% of common image attachments (JPEG, PNG, GIF) are imported correctly, linked to the appropriate page, and render with their original dimensions (within a small tolerance, e.g., +/- 5px, to account for web display differences and responsive scaling). Non-image attachments (PDFs, DOCX, spreadsheets, etc.) are correctly linked and downloadable. The attachment corruption rate (for successfully extracted and downloadable files, verified by file hash comparison or manual check if necessary) must be <0.1%.
*   **Diagram Rendering & Interaction:**
    *   **Mermaid Diagram Rendering Accuracy:** 100% rendering accuracy for all common and supported Mermaid syntax elements (including flowcharts, sequence diagrams, class diagrams, state diagrams, Gantt charts) as defined by the specific `Mermaid.js` library version used in the project. Rendered diagrams in the application should be visually identical to those rendered by the `Mermaid.js` library standalone when given the same syntax.
    *   **Mermaid Editor Responsiveness:** The live preview feature for Mermaid diagrams within the Tiptap editor should update within 2 seconds of a syntax change by the user for moderately complex diagrams (e.g., diagrams with 50-100 lines of Mermaid code). The `/api/diagrams/validate/mermaid/` API endpoint should correctly identify valid or invalid syntax with a low latency (e.g., average response time <500ms).
    *   **Draw.io Integration:** Users can seamlessly open the Draw.io editor iframe modal from a Draw.io node in Tiptap. Saved XML data from the Draw.io editor accurately reflects the diagram changes made by the user. The diagram renders correctly (e.g., as an SVG or via the Draw.io viewer component) in the page view mode.
*   **Markdown Conversion & Editing:**
    *   Bi-directional conversion (ProseMirror JSON <-> GitHub Flavored Markdown) for all common GFM features (headings, lists, bold, italic, links, inline code, code blocks, tables, blockquotes, horizontal rules) results in <1% semantic data loss or unintended formatting changes. This will be verified by round-trip conversion tests (JSON -> MD -> JSON, and MD -> JSON -> MD) and visual comparison. The "Raw Markdown" editing mode functions as expected for both viewing the Markdown representation and applying changes back to the ProseMirror document.
*   **Code Snippet Functionality:**
    *   Syntax highlighting support for at least 50 common programming and markup languages via `highlight.js` (or the chosen syntax highlighting library) is accurate and visually clear, using appropriate color schemes.
    *   Rendering of highlighted code snippets is secure (passes `DOMPurify` sanitization tests against known XSS vectors if `highlight.js` output is HTML).
    *   Client-side generated UUIDs are correctly and uniquely assigned to all `code_block` nodes in the ProseMirror JSON and are usable by LLM features (e.g., "Explain this Code" can target a specific snippet).
*   **AI-Assisted Features (Targets with specified test conditions & hardware context):**
    *   **Code Explanation Relevance & Accuracy:** For a predefined set of common Python and JavaScript code snippets (ranging from approx. 20-100 lines each, with varying levels of complexity), explanations generated by the configured CodeLlama 7B GGUF (e.g., Q4_K_M quantization) model are rated as "relevant and accurate" or "mostly relevant and accurate" by human evaluators in >80% of the test cases.
    *   **Text Generation Coherence & Appropriateness (Streaming):** For a set of typical prompts (e.g., "write a summary of this 500-word text provided as context," "draft an introduction for a new page about topic X based on these keywords: ..."), text generated by the configured Mistral 7B GGUF (e.g., Q4_K_M) model is rated as coherent, contextually appropriate, and grammatically correct in >80% of test cases. The streaming response for text generation should begin providing tokens to the UI within <2-3 seconds on average after the request is initiated.
    *   **Mermaid Diagram Generation Accuracy:** For a set of natural language requests describing common diagram types (e.g., "Create a sequence diagram for a user login process," "Generate a flowchart for a basic customer order processing workflow"), the LLM-generated Mermaid code is syntactically valid (passes API validation via `/api/diagrams/validate/mermaid/`) and accurately represents the user's expressed intent (as judged by human evaluators comparing the request to the rendered diagram) in >70% of test cases.
    *   **LLM Responsiveness (Hardware Dependent Benchmark - e.g., Raspberry Pi 4-level CPU, as a low-end target):**
        *   For the "Explain this Code" task (using a ~50 line Python snippet, CodeLlama 7B GGUF Q4_K_M quantization), when running on a system with a 4-core CPU (e.g., Raspberry Pi 4 with at least 4GB of RAM dedicated/available for the LLM process) and no GPU offloading, the median API response time (from the moment the request is initiated by the client to the full explanation being received by the client) should be targeted at **<10-15 seconds**. (This is an aspirational target for low-end hardware and will be confirmed or adjusted based on actual benchmarking results obtained during Phase 4).
        *   **Token Generation Speed (Streaming):** Aim for a perceived token generation speed of **>30-50 tokens/second** by the user on the target low-end hardware (as specified above) for interactive text generation tasks using 7B GGUF Q4_K_M models. This measures the rate at which new tokens appear in the UI during streaming.
    *   **VRAM Usage (GPU Offloading Guidance):** For 7B GGUF models (e.g., Q4_K_M quantization), the documentation will provide guidance that significant layer offloading to a GPU should be achievable and provide performance benefits if the user has a GPU with common consumer VRAM capacities (e.g., fitting a useful number of layers within <4GB to <6GB VRAM, with more layers or even full offload possible with 8GB+ VRAM). Actual VRAM needs per model/quantization will be documented.
*   **Application Performance & Scalability (General):**
    *   **Page Load Time (Authenticated User, Typical Content Page):** Average page load time (measured by Largest Contentful Paint - LCP, or Time to Interactive - TTI) for a typical content page (containing mixed text, a few images, and 1-2 embedded diagrams or code snippets) should be **<2 seconds** on a standard broadband internet connection.
    *   **API Response Time (Synchronous, Non-LLM Endpoints, p95):** The 95th percentile latency for most synchronous, non-LLM API endpoints (e.g., fetching page data, listing spaces, saving a simple page edit) should be **<500ms** under moderate load conditions.
    *   **Search Results Page Load (Typical Query):** For a typical search query (1-3 keywords) on an indexed dataset of up to 10,000 pages, the search results page should load (Time to Interactive - TTI) in **<3 seconds** on average.
    *   **Load Test Performance:** The application must remain stable and responsive (all key performance metrics like API latency and error rates should stay within acceptable degradation thresholds, e.g., no more than 20-30% degradation from baseline performance under no load) when subjected to a simulated load of **100-200 concurrent users** performing a realistic mix of read (viewing pages, searching) and write (creating/editing pages, adding attachments) operations for a sustained period (e.g., 30 minutes to 1 hour).
    *   **Lighthouse Performance Scores (Lab Tests):** For key pages (e.g., application homepage/dashboard after login, a typical page view, editor loading), aim for a Lighthouse Performance Score (as measured in lab tests using Chrome DevTools Lighthouse audit or CI-integrated Lighthouse runs) of **>80-90 for desktop environments, and >70-80 for mobile environments**. Particular focus will be on optimizing Core Web Vitals: Largest Contentful Paint (LCP), Interaction to Next Paint (INP) (or its proxy First Input Delay - FID), and Cumulative Layout Shift (CLS).
*   **Stability & Reliability:**
    *   During the final (Phase 6) comprehensive testing period, there should be **fewer than 5 critical or major bugs** (defined as showstoppers preventing core functionality, causing data loss or corruption, or having a severe usability impact) identified and remaining unresolved per week leading up to release.
    *   The application should demonstrate stability by running without crashing or exhibiting major performance degradation under the simulated load test conditions for extended operational periods (e.g., successfully complete an 8-hour stability test under moderate simulated load).
*   **Test Coverage (Code):**
    *   Backend Unit Test Coverage (Python/Django code, measured by line coverage using `pytest-cov`): Target **>85%** for critical business logic, models, services, and utility functions.
    *   Frontend Unit/Integration Test Coverage (React components, Zustand stores, utility functions, measured by line/branch coverage using Jest/RTL coverage reports): Target **>70%** for critical UI logic and complex interactive components.
*   **Accessibility (WCAG Compliance):**
    *   Automated accessibility scanning tools (e.g., `axe-core` integrated with E2E tests via `axe-playwright` or `cypress-axe`) must pass with **zero critical or serious WCAG 2.1 Level AA violations** on all critical user paths (including login, page view, page editing basics, search functionality, and primary space navigation).
    *   Manual review by developers (and ideally by an accessibility expert if resources permit, or through community feedback post-MVP) confirms that major accessibility requirements (such as keyboard-only navigation, screen reader compatibility for core tasks, sufficient color contrast, logical heading structure, and form accessibility) are substantially met for these critical paths, demonstrating a strong and ongoing effort towards achieving WCAG 2.1 Level AA compliance.

---
